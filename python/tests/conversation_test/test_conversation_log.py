# -*- coding: utf-8 -*-
"""
LLM ëŒ€í™” ìˆ˜í–‰ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ (Adaptive Mode)
í…ŒìŠ¤í„° LLMì´ Rayì˜ ì‘ë‹µì— ë”°ë¼ í›„ì† ì§ˆë¬¸ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤:
    python test_conversation_log.py

  íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤:
    python test_conversation_log.py --ids 1-1,2-1

  ëŒ€í™”í˜• ëª¨ë“œ:
    python test_conversation_log.py --interactive

  ëª¨ë¸ ë³€ê²½:
    python test_conversation_log.py --model gpt-5-mini
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("PowerShell: $env:OPENAI_API_KEY = 'your-key'")
    sys.exit(1)

from openai import OpenAI

# ìƒìœ„ ë””ë ‰í† ë¦¬(python/) ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from config import RESPONSES_PRESETS, RESPONSES_MODEL
from llm.prompts import SYSTEM_PROMPT_V0_1
from engine.session import ConversationManager
from test_scenarios import TEST_SCENARIOS

# â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEFAULT_PROMPT_NAME = "SYSTEM_PROMPT_V0_1"
DEFAULT_SYSTEM_PROMPT = SYSTEM_PROMPT_V0_1
DEFAULT_MODEL = RESPONSES_MODEL
TESTER_MODEL = "gpt-4.1-mini"  # í…ŒìŠ¤í„° LLM ëª¨ë¸

TOOLS = [
    {
        "type": "web_search",
        "user_location": {"type": "approximate", "country": "KR"},
    },
]


# â”€â”€â”€ í…ŒìŠ¤í„° LLM (Adaptive Questioning) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TESTER_SYSTEM_PROMPT = """\
ë„ˆëŠ” AI ëŒ€í™” ë¡œë´‡ì˜ ì˜í™” ì§€ì‹ê³¼ ëŒ€í™” ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í‰ê°€ìì´ë‹¤.
ëŒ€í™” ìƒëŒ€(Ray)ì˜ ì‘ë‹µì„ ë¶„ì„í•˜ê³ , ë‹¤ìŒì— í•  ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•œë‹¤.

## í•µì‹¬ ì›ì¹™
1. ì ˆëŒ€ë¡œ ë‹µì„ ë¯¸ë¦¬ ì•Œë ¤ì£¼ì§€ ë§ˆë¼. "ì¹¸ ì˜í™”ì œì—ì„œ ìƒ ë°›ì•˜ì§€?" ê°™ì€ ì§ˆë¬¸ì€ ê¸ˆì§€. ëŒ€ì‹  "ê·¸ ì˜í™” ìƒ ë°›ì€ ì  ìˆì–´?"ì²˜ëŸ¼ ë¬¼ì–´ë¼.
2. Rayê°€ ì •í™•í•˜ê²Œ ë‹µí•˜ë©´ ë” ê¹Šì€ í›„ì† ì§ˆë¬¸ì„ í•´ë¼.
3. Rayê°€ ë¶€ì •í™•í•˜ê±°ë‚˜ ëª¨í˜¸í•˜ê²Œ ë‹µí•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¸í•´ë¼.
4. Rayê°€ ëª¨ë¥¸ë‹¤ê³  í•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë¥¸ ê´€ë ¨ ì£¼ì œë¡œ ë„˜ì–´ê°€ë¼.
5. ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ í•œêµ­ì–´ë¡œ ì§ˆë¬¸í•´ë¼. ì‹œí—˜ê´€ì²˜ëŸ¼ ë”±ë”±í•˜ê²Œ í•˜ì§€ ë§ˆë¼.
6. ì§ˆë¬¸ë§Œ ì¶œë ¥í•´ë¼. ì„¤ëª…ì´ë‚˜ ë¶„ì„ì€ í•˜ì§€ ë§ˆë¼.
"""


def generate_next_question(client: OpenAI, scenario: dict, conversation_so_far: list) -> str:
    """í…ŒìŠ¤í„° LLMì„ ì‚¬ìš©í•´ ë‹¤ìŒ ì§ˆë¬¸ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""

    # ëŒ€í™” ì´ë ¥ì„ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
    conv_text = ""
    for turn in conversation_so_far:
        conv_text += f"User: {turn['user']}\n"
        conv_text += f"Ray: {turn['ray']}\n\n"

    knowledge_str = "\n".join(f"- {k}" for k in scenario.get("knowledge_to_verify", []))

    user_prompt = f"""## í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
ì´ë¦„: {scenario['name']}
ëª©í‘œ: {scenario['objective']}

## í™•ì¸í•´ì•¼ í•  ì§€ì‹
{knowledge_str}

## ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”
{conv_text}

## ì§€ì‹œ
ìœ„ ëŒ€í™”ë¥¼ ë³´ê³ , ë‹¤ìŒì— í•  ìì—°ìŠ¤ëŸ¬ìš´ í›„ì† ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ìƒì„±í•´ë¼.
ë‹µì„ ë¯¸ë¦¬ ì•Œë ¤ì£¼ì§€ ë§ê³ , Rayê°€ ìŠ¤ìŠ¤ë¡œ ì•„ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¬¼ì–´ë¼."""

    try:
        response = client.responses.create(
            model=TESTER_MODEL,
            input=[
                {"role": "system", "content": TESTER_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
        )
        for item in response.output:
            if item.type == "message":
                return item.content[0].text.strip()
    except Exception as e:
        return f"[TESTER_ERROR] {e}"

    return "ê·¸ë ‡êµ¬ë‚˜. ì¢€ ë” ìì„¸íˆ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´?"


# â”€â”€â”€ Ray ëŒ€í™” í˜¸ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chat_turn(client: OpenAI, cm: ConversationManager, user_input: str, model: str) -> str:
    """í•œ í„´ì˜ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    cm.add_message({"role": "user", "content": user_input, "type": "message"})
    current_log = cm.get_current_log()

    params = {
        **RESPONSES_PRESETS.get(model, {}),
        "input": current_log,
        "tools": TOOLS,
    }

    try:
        response = client.responses.create(**params)
    except Exception as e:
        return f"[API_ERROR] {e}"

    final_text = ""
    for item in response.output:
        if item.type == "message":
            final_text = item.content[0].text.strip()
            break

    if final_text:
        cm.add_message({"role": "assistant", "content": final_text, "type": "message"})

    return final_text


# â”€â”€â”€ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_scenario(client: OpenAI, scenario: dict, prompt_name: str, system_prompt: str, model: str) -> dict:
    """Adaptive ëª¨ë“œë¡œ í•˜ë‚˜ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    sid = scenario["id"]
    name = scenario["name"]
    num_turns = scenario.get("num_turns", 5)

    print(f"\n{'â•' * 60}")
    print(f"  [{sid}] {name}")
    print(f"  ì¹´í…Œê³ ë¦¬: {scenario['category']}")
    print(f"  í„´ ìˆ˜: {num_turns}")
    print(f"{'â•' * 60}")

    cm = ConversationManager(openai_api_key=OPENAI_API_KEY)
    cm.start_new_session(system_prompt=system_prompt)

    conversation = []

    for turn_num in range(1, num_turns + 1):
        # ì²« í„´ì€ ì‹œë‚˜ë¦¬ì˜¤ì˜ first_message, ì´í›„ëŠ” í…ŒìŠ¤í„° LLMì´ ìƒì„±
        if turn_num == 1:
            user_text = scenario["first_message"]
        else:
            print(f"  ğŸ§ª í…ŒìŠ¤í„°ê°€ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± ì¤‘...")
            user_text = generate_next_question(client, scenario, conversation)

        print(f"\n  [{turn_num}/{num_turns}] User: {user_text}")
        start_t = time.time()
        reply = chat_turn(client, cm, user_text, model)
        elapsed = time.time() - start_t
        print(f"           Ray:  {reply}")
        print(f"           ({elapsed:.2f}s)")

        conversation.append({
            "turn": turn_num,
            "user": user_text,
            "ray": reply,
            "response_time_sec": round(elapsed, 2),
        })

    return {
        "scenario_id": sid,
        "scenario_name": name,
        "category": scenario["category"],
        "check_points": scenario["objective"],
        "knowledge_to_verify": scenario.get("knowledge_to_verify", []),
        "conversation": conversation,
    }


def run_interactive(client: OpenAI, prompt_name: str, system_prompt: str, model: str) -> dict:
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    print(f"\n{'â•' * 60}")
    print(f"  ëŒ€í™”í˜• ëª¨ë“œ (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'q')")
    print(f"  ëª¨ë¸: {model}  |  í”„ë¡¬í”„íŠ¸: {prompt_name}")
    print(f"{'â•' * 60}\n")

    cm = ConversationManager(openai_api_key=OPENAI_API_KEY)
    cm.start_new_session(system_prompt=system_prompt)

    conversation = []
    turn_num = 0

    while True:
        try:
            user_input = input("User: ").strip()
            if not user_input:
                continue
            if user_input.lower() in ["quit", "q", "ì¢…ë£Œ"]:
                break

            turn_num += 1
            start_t = time.time()
            reply = chat_turn(client, cm, user_input, model)
            elapsed = time.time() - start_t

            print(f"Ray:  {reply}")
            print(f"({elapsed:.2f}s)\n")

            conversation.append({
                "turn": turn_num,
                "user": user_input,
                "ray": reply,
                "response_time_sec": round(elapsed, 2),
            })
        except KeyboardInterrupt:
            break

    # ëŒ€í™”í˜• ëª¨ë“œì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ì…ë ¥ ë°›ê¸°
    scenario_name = input("\nì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ (ì—”í„°ë¡œ ê±´ë„ˆë›°ê¸°): ").strip()
    if not scenario_name:
        scenario_name = "ëŒ€í™”í˜• ëª¨ë“œ"

    return {
        "scenario_id": "interactive",
        "scenario_name": scenario_name,
        "category": "ìˆ˜ë™ í…ŒìŠ¤íŠ¸",
        "check_points": "ìˆ˜ë™ í™•ì¸",
        "knowledge_to_verify": [],
        "conversation": conversation,
    }


# â”€â”€â”€ ì¶œë ¥ / ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_metadata(prompt_name: str, model: str) -> dict:
    tool_names = [t.get("type") or t.get("name", "unknown") for t in TOOLS]
    return {
        "prompt_name": prompt_name,
        "model": model,
        "model_presets": RESPONSES_PRESETS.get(model, {}),
        "tools": tool_names,
        "timestamp": datetime.now().isoformat(),
    }


def create_run_dir(base_dir: str, model: str) -> str:
    """í…ŒìŠ¤íŠ¸ ëŸ°ë³„ í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì˜ˆ: 20260212_0932_gpt4.1mini/"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    model_short = model.replace("-", "").replace(".", "")
    run_name = f"{timestamp}_{model_short}"
    run_dir = os.path.join(base_dir, run_name)
    os.makedirs(run_dir, exist_ok=True)
    return run_dir


def save_results(results: list, metadata: dict, run_dir: str) -> str:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    output_data = {
        "metadata": metadata,
        "results": results,
    }

    filepath = os.path.join(run_dir, "results.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
    return filepath


# â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(description="LLM ëŒ€í™” ìˆ˜í–‰ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ (Adaptive)")
    parser.add_argument("--ids", type=str, default=None,
                        help="ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ ID (ì½¤ë§ˆ êµ¬ë¶„, ì˜ˆ: 1-1,2-1)")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL,
                        help=f"ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: {DEFAULT_MODEL})")
    parser.add_argument("--interactive", action="store_true",
                        help="ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰")
    parser.add_argument("--prompt", type=str, default=DEFAULT_PROMPT_NAME,
                        help="í”„ë¡¬í”„íŠ¸ ì´ë¦„ (ê¸°ë³¸: SYSTEM_PROMPT_V0_1)")
    args = parser.parse_args()

    model = args.model
    prompt_name = args.prompt
    system_prompt = DEFAULT_SYSTEM_PROMPT

    client = OpenAI(api_key=OPENAI_API_KEY)
    metadata = build_metadata(prompt_name, model)

    # í…ŒìŠ¤íŠ¸ ëŸ° í´ë” ìƒì„±
    base_output = os.path.join(os.path.dirname(__file__), "..", "..", "output", "test_logs")
    run_dir = create_run_dir(base_output, model)

    print(f"{'â•' * 60}")
    print(f"  LLM ëŒ€í™” ìˆ˜í–‰ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ (Adaptive)")
    print(f"  í”„ë¡¬í”„íŠ¸: {prompt_name}")
    print(f"  ëª¨ë¸:     {model}")
    print(f"  ë„êµ¬:     {', '.join(metadata['tools'])}")
    print(f"  ì¶œë ¥:     {run_dir}")
    print(f"{'â•' * 60}")

    results = []

    if args.interactive:
        result = run_interactive(client, prompt_name, system_prompt, model)
        if result["conversation"]:
            results.append(result)
    else:
        if args.ids:
            target_ids = [s.strip() for s in args.ids.split(",")]
            scenarios = [s for s in TEST_SCENARIOS if s["id"] in target_ids]
            if not scenarios:
                print(f"âš ï¸ í•´ë‹¹ IDì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.ids}")
                sys.exit(1)
        else:
            scenarios = TEST_SCENARIOS

        print(f"\n  ì‹¤í–‰ ëŒ€ìƒ: {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ (Adaptive ëª¨ë“œ)")

        for scenario in scenarios:
            result = run_scenario(client, scenario, prompt_name, system_prompt, model)
            results.append(result)

    if results:
        filepath = save_results(results, metadata, run_dir)
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ {len(results)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        print(f"ğŸ“ ê²°ê³¼ í´ë”: {run_dir}")
        print(f"ğŸ“‹ PPT ë³€í™˜:  python format_for_ppt.py \"{filepath}\" --style ppt -o \"{run_dir}\"")


if __name__ == "__main__":
    main()
