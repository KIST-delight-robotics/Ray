import os
import json
import time
import wave
import base64
import asyncio
import logging
from typing import List, Dict, Any

import websockets
from openai import AsyncOpenAI

from config import (
    SLEEP_FILE, AWAKE_FILE, ACTIVE_SESSION_TIMEOUT, START_KEYWORD, END_KEYWORDS,
    TTS_MODEL, VOICE, RESPONSES_MODEL, RESPONSES_PRESETS, AUDIO_CONFIG, ASSETS_DIR
)
from audio_processor import AudioProcessor
from conversation_manager import ConversationManager
from offline_motion import offline_motion_generation

from rpi5_ws2812.ws2812 import Color
from led import led_set_ring, strip
import math

logger = logging.getLogger(__name__)


async def run_thinking_led_breathing():
    """
    LLM ìƒê° ì¤‘ í‘œì‹œë¥¼ ìœ„í•œ ë¹„ë™ê¸° LED ì• ë‹ˆë©”ì´ì…˜ (ìˆ¨ì‰¬ê¸° íš¨ê³¼).
    """
    try:
        t = 0
        while True:
            # íŒŒë€ìƒ‰(Blue) ê³„ì—´ë¡œ ë¶€ë“œëŸ½ê²Œ ìˆ¨ì‰¬ëŠ”(Breathing) íš¨ê³¼
            # sin í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ìµœì†Œ ë°ê¸°(min_brightness) ì´ìƒì—ì„œ ë¶€ë“œëŸ½ê²Œ ì˜¤ë¥´ë‚´ë¦¼
            min_brightness = 20
            max_brightness = 255
            brightness = int(((math.sin(t) + 1) / 2) * (max_brightness - min_brightness) + min_brightness)

            # Blue channelë§Œ ì‚¬ìš© (R, G, B)
            led_set_ring(0, 0, brightness)
            
            t += 0.2
            await asyncio.sleep(0.05) # 50ms ëŒ€ê¸° (ì·¨ì†Œ í¬ì¸íŠ¸)
            
    except asyncio.CancelledError:
        # íƒœìŠ¤í¬ ì·¨ì†Œ ì‹œ LED ë„ê¸°
        led_set_ring(0, 0, 0)
        raise

async def run_thinking_led_spin(r, g, b, speed=4.0, focus=10.0):
    """
    LLM ìƒê° ì¤‘ í‘œì‹œë¥¼ ìœ„í•œ ë¹„ë™ê¸° LED ì• ë‹ˆë©”ì´ì…˜ (ì›í˜• íšŒì „).
    """
    if not strip:
        return
    
    ring_size = 8
    top_offset = 8
    bottom_offset = 16
    start_shift = 4 # 12ë²ˆ ìœ„ì¹˜ë¡œ ì‹œì‘í•˜ê¸° ìœ„í•œ ì˜¤í”„ì…‹

    try:
        while True:
            t = time.time() * speed
            
            for i in range(ring_size):
                # 1. ê°ë„ ê³„ì‚°
                angle = ((i - start_shift) / ring_size) * 2 * math.pi
                
                # 2. ì‚¬ì¸íŒŒ ê³„ì‚° (-1 ~ 1)
                wave = math.sin(t + angle)
                
                # 3. ë°ê¸° ë³€í™˜ (0 ~ 1) ë° ì§‘ì¤‘ë„(Focus) ì ìš©
                brightness = (wave + 1) / 2
                brightness = math.pow(brightness, focus)
                
                # 4. ìƒ‰ìƒ ì ìš©
                cr = int(r * brightness)
                cg = int(g * brightness)
                cb = int(b * brightness)
                
                final_color = Color(cr, cg, cb)
                
                # ìœ„/ì•„ë˜ ë§ ë™ì‹œ ì ìš© (Batch Update)
                strip.set_pixel_color(top_offset + i, final_color)
                strip.set_pixel_color(bottom_offset + i, final_color)
            
            # 5. í•œ ë²ˆì— ì¶œë ¥ (Efficient)
            strip.show()
            
            # 6. ë¹„ë™ê¸° ëŒ€ê¸° (Non-blocking)
            await asyncio.sleep(0.02) # ì•½ 50 FPS
            
    except asyncio.CancelledError:
        # íƒœìŠ¤í¬ ì·¨ì†Œ ì‹œ LED ë„ê¸°
        if strip:
            strip.clear()
            strip.show()
        raise

# ==================================================================================
# TTS ê´€ë ¨
# ==================================================================================

async def save_tts_to_file(response_text: str, client: AsyncOpenAI, filename: str = "output.mp3"):
    """í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ TTS ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        tts_start_time = time.time()
        # íŒŒì¼ ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(os.path.dirname(filename), exist_ok=True)

        async with client.audio.speech.with_streaming_response.create(
            model=TTS_MODEL,
            voice=VOICE,
            input=response_text,
            # instructions="Speak in a positive tone.",
            response_format="wav"
        ) as tts_response:
            
            logging.info(f"ğŸ’¾ TTS íŒŒì¼ ì €ì¥ ì‹œì‘: {filename}")
            
            with open(filename, "wb") as f:
                async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                    if audio_chunk:
                        f.write(audio_chunk)
        
        logging.info(f"âœ… TTS íŒŒì¼ '{filename}' ì €ì¥ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - tts_start_time:.2f}ì´ˆ)")

    except asyncio.CancelledError:
        logging.info("ğŸ›‘ TTS ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # íŒŒì¼ì´ ì“°ë‹¤ ë§Œ ìƒíƒœë¼ë©´ ì‚­ì œí•˜ëŠ” ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if os.path.exists(filename):
            os.remove(filename)
        raise
    except Exception as e:
        logging.error(f"âŒ TTS ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

async def handle_tts_stream(response_stream, client: AsyncOpenAI, websocket, conversation_log: List[Dict[str, Any]], responses_start_time=None):
    """(ì‚¬ìš©ë˜ì§€ ì•ŠìŒ - ì°¸ê³ ìš©) Responses APIì˜ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ TTS ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜ í›„ ì „ì†¡"""
    await websocket.send(json.dumps({"type": "responses_stream_start"}))
    
    full_response_text = ""
    sentence_buffer = ""
    try:
        async for event in response_stream:
            if event.type == "response.output_text.delta":
                text_chunk = event.delta
                sentence_buffer += text_chunk
                full_response_text += text_chunk
                
                if any(p in sentence_buffer for p in ".?!\n"):
                    async with client.audio.speech.with_streaming_response.create(
                        model=TTS_MODEL, voice=VOICE, input=sentence_buffer, response_format="pcm"
                    ) as tts_response:
                        async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                            await websocket.send(json.dumps({
                                "type": "responses_audio_chunk", 
                                "data": base64.b64encode(audio_chunk).decode('utf-8')
                            }))
                    sentence_buffer = ""
            
            if event.type == "response.completed":
                message = f"(ì†Œìš”ì‹œê°„: {time.time() - responses_start_time:.2f}ì´ˆ)" if responses_start_time else ""
                logger.info(f"OpenAI ì‘ë‹µ ì™„ë£Œ: '{full_response_text}' {message}")

        if sentence_buffer.strip():
            async with client.audio.speech.with_streaming_response.create(
                model=TTS_MODEL, voice=VOICE, input=sentence_buffer, response_format="pcm"
            ) as tts_response:
                async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                    await websocket.send(json.dumps({
                        "type": "responses_audio_chunk", 
                        "data": base64.b64encode(audio_chunk).decode('utf-8')
                    }))

    except asyncio.CancelledError:
        logger.info("TTS ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise
    finally:
        await websocket.send(json.dumps({"type": "responses_stream_end"}))
        if full_response_text:
            conversation_log.append({"role": "assistant", "content": full_response_text})

async def handle_tts_oneshot(response_text: str, client: AsyncOpenAI, websocket, tts_start_event: asyncio.Event):
    """ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ í•œ ë²ˆì— TTS ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        tts_streaming_start_time = time.time()

        await websocket.send(json.dumps({"type": "responses_only"}))
        async with client.audio.speech.with_streaming_response.create(
            model=TTS_MODEL, voice=VOICE, input=response_text, response_format="pcm"
        ) as tts_response:
            first_chunk = True
            async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                if first_chunk:
                    first_chunk = False
                    tts_start_event.set()
                    logger.info(f"TTS ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘... (ì†Œìš”ì‹œê°„: {time.time() - tts_streaming_start_time:.2f}ì´ˆ)")
                await websocket.send(json.dumps({
                    "type": "responses_audio_chunk",
                    "data": base64.b64encode(audio_chunk).decode('utf-8')
                }))
        logger.info(f"TTS ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - tts_streaming_start_time:.2f}ì´ˆ)")
    except asyncio.CancelledError:
        logger.info("TTS ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise
    finally:
        await websocket.send(json.dumps({"type": "responses_stream_end"}))
        if not tts_start_event.is_set():
            tts_start_event.set()


# ==================================================================================
# LLM tools
# =================================================================================
import re

# ìŒì•… ì¬ìƒ
with open('assets/songs_db.json', 'r') as f:
    SONG_DB = json.load(f)

def normalize_string(input_str):
    return re.sub(r'\s+', '', input_str).lower()

song_candidates = []
for song in SONG_DB:
    song_processed = song.copy()
    song_processed['norm_title'] = normalize_string(song['title'])
    song_processed['norm_artist'] = normalize_string(song['artist'])
    song_candidates.append(song_processed)

def play_music(song_title: str = "", artist_name: str = ""):
    """
    LLMì´ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜
    ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¡°ê±´ì— ë§ëŠ” ë…¸ë˜ë¥¼ DBì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì¬ìƒ
    """
    target_title = normalize_string(song_title)
    target_artist = normalize_string(artist_name)

    candidates = song_candidates

    if song_title:
        candidates = [s for s in candidates if target_title in s['norm_title']]

    if artist_name:
        candidates = [s for s in candidates if target_artist in s['norm_artist']]

    if candidates:
        selected_song = candidates[0]
        logging.info(f"ì¬ìƒí•  ë…¸ë˜ ì°¾ìŒ: '{selected_song['title']}' by {selected_song['artist']}")
        return selected_song['file_path'], f"Found and playing '{selected_song['title']}' by {selected_song['artist']}."
    else:
        logging.info("ì¬ìƒí•  ë…¸ë˜ë¥¼ ì°¾ì§€ ëª»í•¨.")
        return None, "ë…¸ë˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ==================================================================================
# LLM API Pipeline
# ==================================================================================

async def run_responses_task(openai_client: AsyncOpenAI, manager: ConversationManager):
    """Responses APIë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ ì‘ë‹µê³¼ ìˆ˜í–‰í•  ì•¡ì…˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    logger.info("ğŸ§  Responses Task ì‹œì‘...")
    responses_start_time = time.time()
    current_log = manager.get_current_log()

    response_text = ""
    music_action = None

    try:
        tools = [
            {
                "type": "web_search",
                "user_location": {"type": "approximate", "country": "KR"},
            },
            {
                "type": "function",
                "name": "play_music",
                "description": "ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¡°ê±´ì— ë§ëŠ” ë…¸ë˜ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¬ìƒí•©ë‹ˆë‹¤. ì €ì¥ëœ DBì— ìˆëŠ” ë…¸ë˜ë§Œ ì¬ìƒ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "song_title": {
                            "type": "string",
                            "description": "ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë…¸ë˜ ì œëª© (ì˜ˆ: ë°¤í¸ì§€)"
                        },
                        "artist_name": {
                            "type": "string",
                            "description": "ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê°€ìˆ˜ ì´ë¦„ (ì˜ˆ: ì•„ì´ìœ )"
                        },
                    },
                    "required": ["song_title", "artist_name"] 
                }
            }
        ]

        params = {
            **RESPONSES_PRESETS.get(RESPONSES_MODEL, {}),
            "input": current_log,
            "tools": tools,
        }
        response = await openai_client.responses.create(**params)

        for item in response.output:
            if item.type == "function_call":
                logger.info(f"ğŸ§  Function call: {item.name}")
                if item.name == "play_music":
                    args = json.loads(item.arguments)
                    song_title = args.get("song_title", "")
                    artist_name = args.get("artist_name", "")
                    file_path, message = play_music(song_title, artist_name)

                    if file_path:
                        audio_name = f"{song_title}_{artist_name}"
                        # assets/headMotion í´ë”ì— audio_name.csv íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                        if not os.path.exists(os.path.join(ASSETS_DIR, "headMotion", f"{audio_name}.csv")):
                            await asyncio.to_thread(offline_motion_generation, audio_name)
                        music_action = {"song_title": song_title, "artist_name": artist_name}
                    
                    # í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ ì¶”ê°€ ë° ì¬ìš”ì²­
                    current_log_copy = current_log.copy()
                    current_log_copy.append(item)
                    current_log_copy.append({
                        "type": "function_call_output",
                        "call_id": item.call_id,
                        "output": json.dumps({
                            "status": "success" if file_path else "failure",
                            "message": message
                        })
                    })

                    params = {
                        **RESPONSES_PRESETS.get(RESPONSES_MODEL, {}),
                        "input": current_log_copy,
                        "tools": tools,
                    }
                    response = await openai_client.responses.create(**params)
                    response_text = response.output[0].content[0].text.strip()
                    break

            elif item.type == "message":
                response_text = item.content[0].text.strip()
                break

        logger.info(f"ğŸ§  ë‹µë³€ ìƒì„± ì™„ë£Œ: '{response_text}' (ì†Œìš”ì‹œê°„: {time.time() - responses_start_time:.2f}ì´ˆ)")
        return response_text, music_action

    except Exception as e:
        logger.error(f"ğŸ§  Responses Task ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return None, None

async def run_tts_action_task(websocket, openai_client: AsyncOpenAI, response_text: str, music_action: dict, tts_start_event: asyncio.Event):
    """í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ TTSë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³  ì•¡ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ì·¨ì†Œ ê°€ëŠ¥)"""
    try:
        # TTS ìŠ¤íŠ¸ë¦¬ë°
        if response_text:
            await handle_tts_oneshot(response_text, openai_client, websocket, tts_start_event)
        else:
            # ë§‰íˆì§€ ì•Šë„ë¡ ì´ë²¤íŠ¸ ì„¤ì •
            tts_start_event.set()

        # ìŒì•… ì¬ìƒ ì•¡ì…˜
        if music_action:
            song_title = music_action['song_title']
            artist_name = music_action['artist_name']
            await websocket.send(json.dumps({"type": "play_audio_csv", "audio_name": f"{song_title}_{artist_name}"}))

    except asyncio.CancelledError:
        logger.info("ğŸ”‡ TTS/Action Taskê°€ ìƒˆ ì…ë ¥ì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # TTS ì¤‘ë‹¨ ì‹œê·¸ë„ ì „ì†¡ ë“± ì¶”ê°€ ê°€ëŠ¥
    except Exception as e:
        logger.error(f"TTS/Action Task ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        if not tts_start_event.is_set():
            tts_start_event.set()

async def unified_active_pipeline(websocket, openai_client: AsyncOpenAI, manager: ConversationManager):
    """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ Responses APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ê³  TTS ë° ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸"""
    logger.info("ğŸ¤– Unified Active Pipeline ì‹œì‘...")

    current_tts_task = None
    stt_result_queue = asyncio.Queue()
    main_loop = asyncio.get_running_loop()

    try:
        with AudioProcessor(stt_result_queue, main_loop, websocket, config=AUDIO_CONFIG) as audio_processor:
            while True:
                try:
                    # 1. ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° (TTSê°€ ì‹¤í–‰ ì¤‘ì´ì–´ë„ ë“£ê³  ìˆìŒ)
                    user_text = await asyncio.wait_for(stt_result_queue.get(), timeout=ACTIVE_SESSION_TIMEOUT)

                    # 2. ì…ë ¥ ê°ì§€ ì‹œ ë§í•˜ê³  ìˆë˜ TTS ì·¨ì†Œ
                    if current_tts_task and not current_tts_task.done():
                        logger.info(f"ì‚¬ìš©ì ì¸í„°ëŸ½ì…˜ ê°ì§€: '{user_text}'. ì´ì „ ë°œí™”(TTS)ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        current_tts_task.cancel()
                        await asyncio.gather(current_tts_task, return_exceptions=True)
                        current_tts_task = None

                    if any(kw in user_text for kw in END_KEYWORDS):
                        await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(SLEEP_FILE)}))
                        logger.info(f"ì¢…ë£Œ í‚¤ì›Œë“œ ê°ì§€: '{user_text}' - ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break

                    manager.add_message("user", user_text)

                    # 3. ë‹µë³€ ìƒì„±
                    # VAD/STT ì¼ì‹œ ì¤‘ë‹¨
                    audio_processor.pause_processing()

                    # LEDë¡œ ìƒê° ì¤‘ í‘œì‹œ
                    thinking_led_task = asyncio.create_task(run_thinking_led_spin(50, 50, 233, speed=4.0, focus=10.0))

                    # LLM ì‘ë‹µ ìƒì„± (Block)
                    response_text, music_action = await run_responses_task(openai_client, manager)
                    

                    # 4. TTS ë° ì•¡ì…˜ ìˆ˜í–‰ (ì·¨ì†Œ ê°€ëŠ¥)
                    if response_text:
                        manager.add_message("assistant", response_text)
                        
                        tts_start_event = asyncio.Event()

                        current_tts_task = asyncio.create_task(
                            run_tts_action_task(websocket, openai_client, response_text, music_action, tts_start_event)
                        )

                        # TTSê°€ ì‹¤ì œë¡œ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
                        try:
                            await asyncio.wait_for(tts_start_event.wait(), timeout=5.0)
                        except asyncio.TimeoutError:
                            logger.warning("âš ï¸ TTS ì‹œì‘ ì´ë²¤íŠ¸ íƒ€ì„ì•„ì›ƒ. ê·¸ëƒ¥ ì§„í–‰í•©ë‹ˆë‹¤.")
                     
                        # "ë‹µë³€ ìƒì„± ì¤‘ ~ TTS ì¤€ë¹„ ì¤‘" ì‚¬ì´ì— ë“¤ì–´ì˜¨ ëª¨ë“  ì…ë ¥ ì‚­ì œ
                        ignored_count = 0
                        while not stt_result_queue.empty():
                            try:
                                stt_result_queue.get_nowait()
                                ignored_count += 1
                            except asyncio.QueueEmpty:
                                break
                        if ignored_count > 0:
                            logger.info(f"ğŸ§¹ TTS ì‹œì‘ ì „ ë“¤ì–´ì˜¨ {ignored_count}ê°œì˜ ì…ë ¥ì„ ë¬´ì‹œí–ˆìŠµë‹ˆë‹¤.")
                        
                        # ë¡œë”© LED ë„ê¸°
                        if not thinking_led_task.done():
                            thinking_led_task.cancel()
                            await asyncio.gather(thinking_led_task, return_exceptions=True)
                        led_set_ring(50, 50, 233)  # ë‹µë³€ ì¤€ë¹„ ì™„ë£Œ í‘œì‹œ

                        # VAD/STT ì¬ê°œ
                        audio_processor.resume_processing()
                    else:
                        # ë‹µë³€ì´ ì—†ëŠ” ê²½ìš° (ì˜¤ë¥˜ ë“±) ë°”ë¡œ ë“£ê¸° ì¬ê°œ
                        audio_processor.resume_processing()
                    
                except asyncio.TimeoutError:
                    logger.info(f"â° {ACTIVE_SESSION_TIMEOUT}ì´ˆ ë™ì•ˆ ì…ë ¥ì´ ì—†ì–´ Active ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(SLEEP_FILE)}))
                    break
    except Exception as e:
        logger.error(f"Unified Active Pipelineì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    finally:
        if current_tts_task and not current_tts_task.done():
            current_tts_task.cancel()
        logger.info("ğŸ¤– Unified Active Pipeline ì¢…ë£Œ.")

# ==================================================================================
# Sleep ëª¨ë“œ
# ==================================================================================

async def wakeword_detection_loop(websocket):
    """START_KEYWORDë¥¼ ê°ì§€í•  ë•Œê¹Œì§€ VAD-STT ë£¨í”„ë¥¼ ì‹¤í–‰ (Sleep ëª¨ë“œ)"""
    logger.info(f"ğŸ’¤ Sleep ëª¨ë“œ ì‹œì‘. '{START_KEYWORD}' í˜¸ì¶œ ëŒ€ê¸° ì¤‘...")
    keyword_queue = asyncio.Queue()
    main_loop = asyncio.get_running_loop()

    try:
        with AudioProcessor(keyword_queue, main_loop, websocket, config=AUDIO_CONFIG) as audio_processor:
            while True:
                stt_result = await keyword_queue.get()
                logger.info(f"[Sleep Mode] STT ê²°ê³¼: {stt_result}")
                if START_KEYWORD in stt_result:
                    await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(AWAKE_FILE)}))
                    return
    except Exception as e:
        logger.error(f"Wakeword detection loopì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    finally:
        logger.info("ğŸ’¤ Sleep ëª¨ë“œ ì¢…ë£Œ.")