import time
import math
import json
import queue
import asyncio
import logging
import threading
from collections import deque

import sounddevice as sd
import numpy as np
import torch # VADì— í•„ìš”
from google.cloud import speech # STTì— í•„ìš”
from google.api_core import exceptions


# --- ë¡œê¹… ì„¤ì • (ë‹¨ë… ì‹¤í–‰ ì‹œ í•„ìš”) ---
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [%(threadName)s] %(message)s',
    datefmt='%H:%M:%S'
)

# ==================================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==================================================================================

def find_input_device(device_name_substring: str = 'pipewire') -> int | None:
    """
    ì£¼ì–´ì§„ ë¬¸ìì—´ì´ í¬í•¨ëœ ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if device_name_substring.lower() in device['name'].lower() and device['max_input_channels'] > 0:
            logging.info(f"ğŸ” ë°œê²¬ëœ ì…ë ¥ ì¥ì¹˜: [{idx}] {device['name']}")
            return idx
    logging.warning(f"âš ï¸ '{device_name_substring}'ê°€ í¬í•¨ëœ ì…ë ¥ ì¥ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return None

# ==================================================================================
# Component 1: ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ë‹´ë‹¹ (Producer)
# ==================================================================================

class MicrophoneStream:
    """ë§ˆì´í¬ë¡œë¶€í„° ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì½ì–´ íì— ë„£ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, audio_queue: queue.Queue, sample_rate: int, chunk_size: int, channels: int, dtype: str, device_idx: int | None = None):
        self.audio_queue = audio_queue
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.channels = channels
        self.dtype = dtype
        self.device_idx = device_idx
        
        self.stream: sd.InputStream | None = None
    
    def start(self):
        if self.stream is not None and self.stream.active:
            logging.warning("MicrophoneStreamì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        logging.info("ğŸ™ï¸ MicrophoneStream ì‹œì‘.")
        self.stream = sd.InputStream(samplerate=self.sample_rate,
                                     blocksize=self.chunk_size,
                                     channels=self.channels,
                                     dtype=self.dtype,
                                     device=self.device_idx,
                                     callback=self._callback)
        self.stream.start()

    def stop(self):
        if self.stream is not None:
            logging.info("ğŸ™ï¸ MicrophoneStream ì¤‘ì§€.")
            self.stream.stop()
            self.stream.close()
            self.stream = None

    def _callback(self, indata: np.ndarray, frames: int, time_info, status) -> None:
        if status:
            logging.warning(f"[ì˜¤ë””ì˜¤ ìƒíƒœ] {status}")
        self.audio_queue.put(indata.copy())

# ==================================================================================
# Component 2: VAD (Voice Activity Detection) ë‹´ë‹¹
# ==================================================================================

class VADProcessor:
    """Silero VAD ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„± í™œë™ì„ ê°ì§€í•˜ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, sample_rate: int, chunk_size: int, threshold: float = 0.5, consecutive_chunks: int = 3, reset_interval: float = 10.0):
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.threshold = threshold
        self.consecutive_chunks_required = consecutive_chunks
        self.reset_interval = reset_interval

        # VAD ëª¨ë¸ ë¡œë“œ
        try:
            model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=True)
            self.vad_model = model
            logging.info("âœ… Silero VAD ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logging.error(f"âŒ Silero VAD ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.vad_model = None
        
        self.vad_buffer = torch.tensor([])
        self.consecutive_speech_chunks = 0
        self.vad_detection_start_time = time.time()
    
    def process_chunk(self, audio_chunk_int16: np.ndarray) -> bool:
        """
        ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³  ìŒì„± ê°ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ìŒì„± ì‹œì‘ ì¡°ê±´(ì—°ì† ì²­í¬ ìˆ˜)ì´ ì¶©ì¡±ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€.
        """
        if self.vad_model is None:
            return False
        
        # float32ë¡œ ë³€í™˜ (Silero VAD ìš”êµ¬ì‚¬í•­)
        audio_chunk_float32 = audio_chunk_int16.astype(np.float32) / 32768.0
        audio_tensor = torch.from_numpy(audio_chunk_float32.flatten())
        
        self.vad_buffer = torch.cat([self.vad_buffer, audio_tensor])
        
        speech_detected = False

        while len(self.vad_buffer) >= self.chunk_size:
            vad_chunk = self.vad_buffer[:self.chunk_size]
            self.vad_buffer = self.vad_buffer[self.chunk_size:]
            
            speech_prob = self.vad_model(vad_chunk, self.sample_rate).item()
            
            if speech_prob > self.threshold:
                self.consecutive_speech_chunks += 1
                self.vad_detection_start_time = time.time() # ê°ì§€ ì‹œì  ê°±ì‹ 
            else:
                self.consecutive_speech_chunks = 0

            if self.consecutive_speech_chunks >= self.consecutive_chunks_required:
                speech_detected = True
                break # ìŒì„± ê°ì§€ ì¡°ê±´ ì¶©ì¡± ì‹œ ë£¨í”„ ì¢…ë£Œ

        return speech_detected
    
    def reset_if_inactive(self):
        """
        ì¼ì • ì‹œê°„ ë™ì•ˆ ìŒì„± ê°ì§€ê°€ ì—†ìœ¼ë©´ VAD ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        if time.time() - self.vad_detection_start_time > self.reset_interval:
            logging.info(f"{self.reset_interval}ì´ˆ ë™ì•ˆ ìŒì„± ê°ì§€ê°€ ì—†ì–´ VAD ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            self.reset()
            self.vad_detection_start_time = time.time()
    
    def reset(self):
        """
        VAD ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        if self.vad_model:
            self.vad_model.reset_states()
        self.vad_buffer = torch.tensor([])
        self.consecutive_speech_chunks = 0
        logging.info("VAD ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ.")

# ==================================================================================
# Component 3: Google STT ìŠ¤íŠ¸ë¦¬ë¨¸ ë‹´ë‹¹
# ==================================================================================

class GoogleSTTStreamer:
    """Google Speech-to-Text APIë¡œ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤."""
    
    def __init__(self, stt_result_queue: asyncio.Queue, main_loop: asyncio.AbstractEventLoop, websocket, sample_rate: int, adaptation_config=None):
        self.stt_result_queue = stt_result_queue
        self.main_loop = main_loop
        self.websocket = websocket
        
        self.stt_client = speech.SpeechClient()
        self.stt_config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=sample_rate,
            language_code="ko-KR",
            enable_automatic_punctuation=True,
            adaptation=adaptation_config
        )
        self.stt_streaming_config = speech.StreamingRecognitionConfig(
            config=self.stt_config,
            interim_results=True,
            single_utterance=True,
        )
        logging.info("âœ… Google STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _stt_audio_generator(self, pre_buffer: deque, audio_queue: queue.Queue, stt_stop_flag: threading.Event, inactivity_stop_flag: threading.Event):
        """
        STT APIì— ì˜¤ë””ì˜¤ë¥¼ ê³µê¸‰í•˜ëŠ” ì œë„ˆë ˆì´í„°.
        """
        # 1. ì‚¬ì „ ë²„í¼(pre-buffer) ì „ì†¡
        if pre_buffer:
            combined_audio = np.concatenate(list(pre_buffer))
            duration_sec = len(combined_audio) / self.stt_config.sample_rate_hertz
            yield speech.StreamingRecognizeRequest(audio_content=combined_audio.tobytes())
            logging.info(f"STT ì‚¬ì „ ë²„í¼ ({duration_sec:.2f}ì´ˆ) ì „ì†¡ ì™„ë£Œ")
            pre_buffer.clear()

        # 2. ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì „ì†¡
        while not stt_stop_flag.is_set() and not inactivity_stop_flag.is_set():
            try:
                # MicrophoneStreamì´ ì±„ì›Œì£¼ëŠ” íì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
                chunk = audio_queue.get(timeout=0.1)
                yield speech.StreamingRecognizeRequest(audio_content=chunk.tobytes())
            except queue.Empty:
                continue
            except Exception as e:
                logging.debug(f"STT ì˜¤ë””ì˜¤ ìƒì„±ê¸° ì˜¤ë¥˜: {e}")
                break

    def run_stt_session(self, pre_buffer: deque, audio_queue: queue.Queue, vad_active_flag: threading.Event):
        """
        ë‹¨ì¼ STT ì„¸ì…˜ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜. ì´ í•¨ìˆ˜ëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë¨ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œë¨).
        """
        
        FIRST_RESPONSE_TIMEOUT = 3.0
        END_OF_SPEECH_TIMEOUT = 3.0
        
        stt_stop_flag = threading.Event()
        end_of_speech_flag = threading.Event()
        first_response_event = threading.Event()
        last_response_time = time.time()

        def false_start_checker():
            """ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ ì²´í¬. VAD ì˜¤ê°ì§€ ë“±ìœ¼ë¡œ STTê°€ ì‹œì‘ëì§€ë§Œ ì‹¤ì œ ìŒì„±ì´ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„."""
            if not first_response_event.wait(timeout=FIRST_RESPONSE_TIMEOUT):
                logging.warning(f"STT ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ - ì„¸ì…˜ ì¢…ë£Œ")
                stt_stop_flag.set()

        def speech_end_checker():
            """ì‚¬ìš©ìì˜ ë°œí™”ê°€ ëë‚¬ëŠ”ì§€(ì¼ì • ì‹œê°„ ë™ì•ˆ ì‘ë‹µì´ ì—†ëŠ”ì§€) ì²´í¬."""
            while not stt_stop_flag.is_set() and not end_of_speech_flag.is_set():
                if time.time() - last_response_time > END_OF_SPEECH_TIMEOUT:
                    logging.info(f"{END_OF_SPEECH_TIMEOUT}ì´ˆ ë™ì•ˆ STT ì‘ë‹µì´ ì—†ì–´ ì˜¤ë””ì˜¤ ì „ì†¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    end_of_speech_flag.set()
                    break
                time.sleep(0.1)

        false_start_check_thread = threading.Thread(target=false_start_checker, daemon=True, name="STTFalseStartChecker")
        false_start_check_thread.start()

        speech_end_check_thread = None

        try:
            audio_gen = self._stt_audio_generator(pre_buffer, audio_queue, stt_stop_flag, end_of_speech_flag)
            responses = self.stt_client.streaming_recognize(self.stt_streaming_config, audio_gen)
            
            for response in responses:
                if stt_stop_flag.is_set(): return
                
                last_response_time = time.time()

                if not first_response_event.is_set():
                    first_response_event.set()
                    speech_end_check_thread = threading.Thread(target=speech_end_checker, daemon=True, name="STTSpeechEndChecker")
                    speech_end_check_thread.start()

                    # C++ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì¸í„°ëŸ½ì…˜ ì‹ í˜¸ ì „ì†¡ (asyncio ë£¨í”„ì— ìŠ¤ì¼€ì¤„ë§)
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "user_interruption"})),
                        self.main_loop
                    )

                if not response.results or not response.results[0].alternatives:
                    continue

                result = response.results[0]
                transcript = result.alternatives[0].transcript

                if result.is_final:
                    final_text = transcript.strip()
                    logging.info(f"âœ… STT ìµœì¢… ê²°ê³¼: '{final_text}'")
                    if final_text:
                        self.main_loop.call_soon_threadsafe(self.stt_result_queue.put_nowait, final_text)
                    
                    stt_completion_time = int(time.time() * 1000)
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "stt_done", "stt_done_time": stt_completion_time})),
                        self.main_loop
                    )
                    return
                else:
                    logging.debug(f"âœ… STT ì¤‘ê°„ ê²°ê³¼: '{transcript}'")
                    
        except exceptions.DeadlineExceeded as e:
            logging.error(f"STT ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ(DeadlineExceeded): {e}")
        except Exception as e:
            logging.error(f"STT ì„¸ì…˜ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        finally:
            stt_stop_flag.set()
            end_of_speech_flag.set()
            vad_active_flag.set() # VAD ê°ì§€ ì¬ê°œ ì‹ í˜¸
            logging.info("STT ì„¸ì…˜ ì¢…ë£Œ.")
    
# ==================================================================================
# Orchestrator: ì˜¤ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
# ==================================================================================

class AudioProcessor:
    """ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼, VAD, STTë¥¼ ì´ê´„í•˜ì—¬ ì˜¤ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤."""
    
    def __init__(self, stt_result_queue: asyncio.Queue, main_loop: asyncio.AbstractEventLoop, websocket, config: dict):
               
        # ì„¤ì • ê°’
        self.sample_rate = config['SAMPLE_RATE']
        self.channels = config['CHANNELS']
        self.audio_dtype = config['AUDIO_DTYPE']
        self.vad_chunk_size = config['VAD_CHUNK_SIZE']
        self.pre_buffer_duration = config['PRE_BUFFER_DURATION']
        
        # í†µì‹  ì±„ë„
        self.stt_result_queue = stt_result_queue
        self.main_loop = main_loop
        self.websocket = websocket
        
        # ì˜¤ë””ì˜¤ ë²„í¼
        self.audio_queue = queue.Queue() # MicrophoneStreamì´ ì±„ìš°ëŠ” í
        pre_buffer_max_chunks = math.ceil(self.sample_rate * self.pre_buffer_duration / self.vad_chunk_size)
        self.stt_pre_buffer = deque(maxlen=pre_buffer_max_chunks)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        device_idx = find_input_device()
        self.mic_stream = MicrophoneStream(
            audio_queue=self.audio_queue,
            sample_rate=self.sample_rate,
            chunk_size=self.vad_chunk_size,
            channels=self.channels,
            dtype=self.audio_dtype,
            device_idx=device_idx
        )
        self.vad_processor = VADProcessor(
            sample_rate=self.sample_rate,
            chunk_size=self.vad_chunk_size,
            threshold=config['VAD_THRESHOLD'],
            consecutive_chunks=config['VAD_CONSECUTIVE_CHUNKS'],
            reset_interval=config['VAD_RESET_INTERVAL']
        )
        self.stt_streamer = GoogleSTTStreamer(
            stt_result_queue=stt_result_queue,
            main_loop=main_loop,
            websocket=websocket,
            sample_rate=self.sample_rate
        )

        # ìƒíƒœ ê´€ë¦¬
        self._is_running = threading.Event()
        self.vad_active_flag = threading.Event()
        self.vad_active_flag.set() # ì´ˆê¸°ì—ëŠ” VAD ê°ì§€ í™œì„±í™” ìƒíƒœ
        self._thread: threading.Thread | None = None

    def _processing_loop(self):
        """
        ì˜¤ë””ì˜¤ íì—ì„œ ë°ì´í„°ë¥¼ ì†Œë¹„í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë£¨í”„.
        """
        logging.info("ğŸ§ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì‹œì‘.")
        
        # MicrophoneStream ì‹œì‘ (sounddevice ë‚´ë¶€ ìŠ¤ë ˆë“œ ì‹œì‘)
        self.mic_stream.start()

        while self._is_running.is_set():
            # STT ì‹¤í–‰ ì¤‘ì¼ ê²½ìš° ëŒ€ê¸°
            self.vad_active_flag.wait()
            if not self._is_running.is_set(): break
            
            self.vad_processor.reset_if_inactive()

            try:
                audio_chunk_int16 = self.audio_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            # ì‚¬ì „ ë²„í¼ ì €ì¥
            self.stt_pre_buffer.append(audio_chunk_int16)

            # VAD ì²˜ë¦¬ ë° ìŒì„± ê°ì§€ í™•ì¸
            if self.vad_processor.process_chunk(audio_chunk_int16):
                
                # ìŒì„± ê°ì§€ ì‹œ VAD ë£¨í”„ë¥¼ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜
                self.vad_active_flag.clear() 
                logging.info(f"ğŸ—£ï¸ ìŒì„± ì‹œì‘ ê°ì§€! STT ì‹œì‘.")
                
                # STT ì„¸ì…˜ì€ ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                threading.Thread(
                    target=self.stt_streamer.run_stt_session, 
                    args=(self.stt_pre_buffer, self.audio_queue, self.vad_active_flag),
                    name="STTSessionThread"
                ).start()
                
                # STT ì‹œì‘ê³¼ í•¨ê»˜ VAD ìƒíƒœ ì´ˆê¸°í™”
                self.vad_processor.reset()
        
        logging.info("ğŸ§ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ.")

    def __enter__(self):
        """AudioProcessorì˜ ìƒëª…ì£¼ê¸° ì‹œì‘."""
        logging.info("AudioProcessor ì»¨í…ìŠ¤íŠ¸ ì‹œì‘...")
        self._is_running.set()
        
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ìŠ¤ë ˆë“œ ì‹œì‘
        self._thread = threading.Thread(target=self._processing_loop, name="AudioProcessingThread")
        self._thread.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """AudioProcessorì˜ ìƒëª…ì£¼ê¸° ì¢…ë£Œ."""
        logging.info("AudioProcessor ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ...")
        
        # 1. ì²˜ë¦¬ ë£¨í”„ ìŠ¤ë ˆë“œì— ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
        self._is_running.clear()
        self.vad_active_flag.set() # ëŒ€ê¸° ì¤‘ì¸ ìŠ¤ë ˆë“œë¥¼ ê¹¨ì›€
        
        # 2. ì²˜ë¦¬ ë£¨í”„ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        
        # 3. MicrophoneStream ì¤‘ì§€ (sounddevice ë‚´ë¶€ ìŠ¤ë ˆë“œ ì¢…ë£Œ)
        self.mic_stream.stop()
        
        logging.info("AudioProcessorê°€ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")