import time
import math
import json
import queue
import asyncio
import logging
import threading
from collections import deque

import sounddevice as sd
import numpy as np
import torch # VADì— í•„ìš”
from google.cloud import speech # STTì— í•„ìš”
from google.api_core import exceptions

# SmartTurn ëª¨ë¸ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import onnxruntime as ort
from transformers import WhisperFeatureExtractor

from config import (
    SMART_TURN_MODEL_PATH, TURN_END_SILENCE_CHUNKS, MAX_TURN_CHUNKS, SMART_TURN_GRACE_PERIOD_S
)


# --- ë¡œê¹… ì„¤ì • (ë‹¨ë… ì‹¤í–‰ ì‹œ í•„ìš”) ---
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [%(threadName)s] %(message)s',
    datefmt='%H:%M:%S'
)

# ==================================================================================
# SmartTurn
# ==================================================================================
class SmartTurnProcessor:
    """Smart Turn v3 ONNX ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°œí™” ì¢…ë£Œë¥¼ ì˜ˆì¸¡í•˜ëŠ” í´ë˜ìŠ¤."""
    def __init__(self, onnx_path):
        try:
            so = ort.SessionOptions()
            so.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
            so.inter_op_num_threads = 1
            so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            self.session = ort.InferenceSession(onnx_path, sess_options=so, providers=['CPUExecutionProvider'])
            self.feature_extractor = WhisperFeatureExtractor(chunk_length=8)
            logging.info(f"âœ… Smart Turn ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {onnx_path}")
        except Exception as e:
            logging.error(f"âŒ Smart Turn ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            self.session = None

    def _truncate_or_pad_audio(self, audio_array, n_seconds=8, sample_rate=16000):
        max_samples = n_seconds * sample_rate
        if len(audio_array) > max_samples:
            return audio_array[-max_samples:]
        elif len(audio_array) < max_samples:
            padding = max_samples - len(audio_array)
            return np.pad(audio_array, (padding, 0), mode='constant', constant_values=0)
        return audio_array

    def predict(self, audio_array_f32: np.ndarray) -> dict:
        """
        ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë°œí™” ì¢…ë£Œ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        Returns: {"prediction": 0 or 1, "probability": float}
        """
        if not self.session:
            # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ, í•­ìƒ 'ì§„í–‰ ì¤‘'ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ëŒ€í™”ê°€ ëŠê¸°ì§€ ì•Šë„ë¡ í•¨
            return {"prediction": 0, "probability": 0.0}

        audio_array = self._truncate_or_pad_audio(audio_array_f32, n_seconds=8)
        inputs = self.feature_extractor(
            audio_array,
            sampling_rate=16000,
            return_tensors="np",
            padding="max_length",
            max_length=8 * 16000,
            truncation=True,
            do_normalize=True,
        )
        input_features = np.expand_dims(inputs.input_features.squeeze(0), axis=0).astype(np.float32)
        outputs = self.session.run(None, {"input_features": input_features})
        probability = outputs[0][0].item()
        prediction = 1 if probability > 0.5 else 0
        return {"prediction": prediction, "probability": probability}


# ==================================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==================================================================================

def find_input_device(device_name_substring: str = 'pipewire') -> int | None:
    """
    ì£¼ì–´ì§„ ë¬¸ìì—´ì´ í¬í•¨ëœ ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if device_name_substring.lower() in device['name'].lower() and device['max_input_channels'] > 0:
            logging.info(f"ğŸ” ë°œê²¬ëœ ì…ë ¥ ì¥ì¹˜: [{idx}] {device['name']}")
            return idx
    logging.warning(f"âš ï¸ '{device_name_substring}'ê°€ í¬í•¨ëœ ì…ë ¥ ì¥ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return None

# ==================================================================================
# Component 1: ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ë‹´ë‹¹ (Producer)
# ==================================================================================

class MicrophoneStream:
    """ë§ˆì´í¬ë¡œë¶€í„° ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì½ì–´ íì— ë„£ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, mic_audio_queue: queue.Queue, sample_rate: int, chunk_size: int, channels: int, dtype: str, device_idx: int | None = None):
        self.mic_audio_queue = mic_audio_queue
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.channels = channels
        self.dtype = dtype
        self.device_idx = device_idx
        self.stream: sd.InputStream | None = None
    
    def start(self):
        if self.stream is not None and self.stream.active:
            logging.warning("MicrophoneStreamì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        logging.info("ğŸ™ï¸ MicrophoneStream ì‹œì‘.")
        self.stream = sd.InputStream(samplerate=self.sample_rate,
                                     blocksize=self.chunk_size,
                                     channels=self.channels,
                                     dtype=self.dtype,
                                     device=self.device_idx,
                                     callback=self._callback)
        self.stream.start()

    def stop(self):
        if self.stream is not None:
            logging.info("ğŸ™ï¸ MicrophoneStream ì¤‘ì§€.")
            self.stream.stop()
            self.stream.close()
            self.stream = None

    def _callback(self, indata: np.ndarray, frames: int, time_info, status) -> None:
        if status:
            logging.warning(f"[ì˜¤ë””ì˜¤ ìƒíƒœ] {status}")
        self.mic_audio_queue.put(indata.copy())


# ==================================================================================
# Component 2: VAD (Voice Activity Detection) ë‹´ë‹¹
# ==================================================================================

class VADProcessor:
    """Silero VAD ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„± í™œë™ì„ ê°ì§€í•˜ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, sample_rate: int, chunk_size: int, threshold: float = 0.5, consecutive_chunks: int = 3, reset_interval: float = 20.0):
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.threshold = threshold
        self.consecutive_chunks_required = consecutive_chunks
        self.reset_interval = reset_interval

        # VAD ëª¨ë¸ ë¡œë“œ
        try:
            model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=True)
            self.vad_model = model
            logging.info("âœ… Silero VAD ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logging.error(f"âŒ Silero VAD ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.vad_model = None
        
        self.vad_buffer = torch.tensor([])
        self.consecutive_speech_chunks = 0
        self.vad_detection_start_time = time.time()
    
    def process_chunk(self, audio_chunk_int16: np.ndarray) -> bool:
        """
        ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³  ìŒì„± ê°ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ìŒì„± ì‹œì‘ ì¡°ê±´(ì—°ì† ì²­í¬ ìˆ˜)ì´ ì¶©ì¡±ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€.
        """
        if self.vad_model is None:
            return False
        
        # float32ë¡œ ë³€í™˜ (Silero VAD ìš”êµ¬ì‚¬í•­)
        audio_chunk_float32 = audio_chunk_int16.astype(np.float32) / 32768.0
        audio_tensor = torch.from_numpy(audio_chunk_float32.flatten())
        
        self.vad_buffer = torch.cat([self.vad_buffer, audio_tensor])
        
        speech_detected = False

        while len(self.vad_buffer) >= self.chunk_size:
            vad_chunk = self.vad_buffer[:self.chunk_size]
            self.vad_buffer = self.vad_buffer[self.chunk_size:]
            
            speech_prob = self.vad_model(vad_chunk, self.sample_rate).item()
            
            if speech_prob > self.threshold:
                self.consecutive_speech_chunks += 1
                self.vad_detection_start_time = time.time() # ê°ì§€ ì‹œì  ê°±ì‹ 
            else:
                self.consecutive_speech_chunks = 0

            if self.consecutive_speech_chunks >= self.consecutive_chunks_required:
                speech_detected = True
                break # ìŒì„± ê°ì§€ ì¡°ê±´ ì¶©ì¡± ì‹œ ë£¨í”„ ì¢…ë£Œ

        return speech_detected
    
    def reset_if_inactive(self):
        """
        ì¼ì • ì‹œê°„ ë™ì•ˆ ìŒì„± ê°ì§€ê°€ ì—†ìœ¼ë©´ VAD ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        if time.time() - self.vad_detection_start_time > self.reset_interval:
            logging.info(f"{self.reset_interval}ì´ˆ ë™ì•ˆ ìŒì„± ê°ì§€ê°€ ì—†ì–´ VAD ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            self.reset()
            self.vad_detection_start_time = time.time()
    
    def reset(self):
        """
        VAD ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        if self.vad_model:
            self.vad_model.reset_states()
        self.vad_buffer = torch.tensor([])
        self.consecutive_speech_chunks = 0
        logging.info("VAD ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ.")

# ==================================================================================
# Component 3: Google STT ìŠ¤íŠ¸ë¦¬ë¨¸ ë‹´ë‹¹
# ==================================================================================

class GoogleSTTStreamer:
    """Google Speech-to-Text APIë¡œ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤."""
    
    def __init__(self, stt_result_queue: asyncio.Queue, main_loop: asyncio.AbstractEventLoop, websocket, sample_rate: int, stt_audio_queue: queue.Queue, stt_stop_event: threading.Event):
        self.stt_result_queue = stt_result_queue
        self.main_loop = main_loop
        self.websocket = websocket
        self.stt_audio_queue = stt_audio_queue
        self.stt_stop_event = stt_stop_event
        
        self.stt_client = speech.SpeechClient()
        self.stt_config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=sample_rate,
            language_code="ko-KR",
            enable_automatic_punctuation=True,
        )
        self.stt_streaming_config = speech.StreamingRecognitionConfig(
            config=self.stt_config,
            interim_results=True,
            single_utterance=False,
        )
        logging.info("âœ… Google STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _stt_audio_generator(self):
        """STT APIì— ì˜¤ë””ì˜¤ë¥¼ ê³µê¸‰í•˜ëŠ” ì œë„ˆë ˆì´í„°."""
        while not self.stt_stop_event.is_set():
            try:
                chunk = self.stt_audio_queue.get(timeout=0.1)
                yield speech.StreamingRecognizeRequest(audio_content=chunk.tobytes())
            except queue.Empty:
                continue
        logging.info("STT ì˜¤ë””ì˜¤ ê³µê¸‰ ì¤‘ë‹¨ë¨.")

    def run_stt_session(self):
        """ë‹¨ì¼ STT ì„¸ì…˜ì„ ì‹¤í–‰í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ íì— ë„£ìŒ."""
        logging.info("ğŸš€ STT ì„¸ì…˜ ìŠ¤ë ˆë“œ ì‹œì‘.")
        first_response_received = False

        accumulated_transcripts = []
        current_interim_transcript = ""

        try:
            audio_gen = self._stt_audio_generator()
            responses = self.stt_client.streaming_recognize(self.stt_streaming_config, audio_gen)
            
            for response in responses:
                if not first_response_received:
                    first_response_received = True
                    # C++ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì¸í„°ëŸ½ì…˜ ì‹ í˜¸ ì „ì†¡
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "user_interruption"})),
                        self.main_loop
                    )

                if not response.results or not response.results[0].alternatives:
                    continue

                result = response.results[0]
                transcript = result.alternatives[0].transcript.strip()

                if result.is_final:
                    accumulated_transcripts.append(transcript)
                    current_interim_transcript = ""
                    logging.info(f"âœ… STT ìµœì¢… ê²°ê³¼ ì¡°ê°: '{transcript}'")
                else:
                    current_interim_transcript = transcript
                    logging.info(f"ğŸŸ© STT ì¤‘ê°„ ê²°ê³¼: '{transcript}'")
                
                if self.stt_stop_event.is_set():
                    break

                # if result.is_final and self.stt_stop_event.is_set():
                #     final_text = transcript.strip()
                #     logging.info(f"âœ… STT ìµœì¢… ê²°ê³¼: '{final_text}'")
                #     if final_text:
                #         # ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ê²°ê³¼ ì „ì†¡
                #         self.main_loop.call_soon_threadsafe(self.stt_result_queue.put_nowait, final_text)
                    
                #     # C++ í´ë¼ì´ì–¸íŠ¸ì— STT ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
                #     stt_completion_time = int(time.time() * 1000)
                #     asyncio.run_coroutine_threadsafe(
                #         self.websocket.send(json.dumps({"type": "stt_done", "stt_done_time": stt_completion_time})),
                #         self.main_loop
                #     )
                #     break # ìµœì¢… ê²°ê³¼ë¥¼ ë°›ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
                # else:
                #     logging.info(f"âœ… STT ì¤‘ê°„ ê²°ê³¼: '{transcript}'")
                    
        except exceptions.DeadlineExceeded as e:
            logging.warning(f"STT ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ(DeadlineExceeded): {e}")
        except Exception as e:
            logging.error(f"STT ì„¸ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        finally:
            # ìµœì¢… ê²°ê³¼ ë°˜í™˜
            final_text_parts = accumulated_transcripts.copy()
            if current_interim_transcript:
                final_text_parts.append(current_interim_transcript)
            final_text = " ".join(final_text_parts).strip()

            if final_text:
                logging.info(f"âœ… STT ìµœì¢… ê²°ê³¼: '{final_text}'")

                # ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ê²°ê³¼ ì „ì†¡
                self.main_loop.call_soon_threadsafe(self.stt_result_queue.put_nowait, final_text)

                # C++ í´ë¼ì´ì–¸íŠ¸ì— STT ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
                stt_completion_time = int(time.time() * 1000)
                asyncio.run_coroutine_threadsafe(
                    self.websocket.send(json.dumps({"type": "stt_done", "stt_done_time": stt_completion_time})),
                    self.main_loop
                )
            else:
                logging.info("â STT ì¸ì‹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            logging.info("ğŸš€ STT ì„¸ì…˜ ìŠ¤ë ˆë“œ ì¢…ë£Œ.")

# ==================================================================================
# Orchestrator: ì˜¤ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
# ==================================================================================

class AudioProcessor:
    """ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼, VAD, SmartTurn, STTë¥¼ ì´ê´„í•˜ì—¬ ì˜¤ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬."""
    
    def __init__(self, stt_result_queue: asyncio.Queue, main_loop: asyncio.AbstractEventLoop, websocket, config: dict):
               
        # ì„¤ì • ê°’
        self.sample_rate = config['SAMPLE_RATE']
        self.channels = config['CHANNELS']
        self.audio_dtype = config['AUDIO_DTYPE']
        self.vad_chunk_size = config['VAD_CHUNK_SIZE']
        self.pre_buffer_duration = config['PRE_BUFFER_DURATION']
        
        # í†µì‹  ì±„ë„
        self.stt_result_queue = stt_result_queue
        self.main_loop = main_loop
        self.websocket = websocket
        
        # ì˜¤ë””ì˜¤ í
        self.mic_audio_queue = queue.Queue()
        self.stt_audio_queue = queue.Queue()
        pre_buffer_max_chunks = math.ceil(self.sample_rate * self.pre_buffer_duration / self.vad_chunk_size)
        self.stt_pre_buffer = deque(maxlen=pre_buffer_max_chunks)
        self.current_turn_audio = []

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        device_idx = find_input_device()
        self.mic_stream = MicrophoneStream(
            mic_audio_queue=self.mic_audio_queue,
            sample_rate=self.sample_rate,
            chunk_size=self.vad_chunk_size,
            channels=self.channels,
            dtype=self.audio_dtype,
            device_idx=device_idx
        )
        self.vad_processor = VADProcessor(
            sample_rate=self.sample_rate,
            chunk_size=self.vad_chunk_size,
            threshold=config['VAD_THRESHOLD'],
            consecutive_chunks=config['VAD_CONSECUTIVE_CHUNKS'],
            reset_interval=config['VAD_RESET_INTERVAL']
        )
        self.smart_turn_processor = SmartTurnProcessor(SMART_TURN_MODEL_PATH)
        self.stt_stop_event = threading.Event()
        self.stt_streamer = GoogleSTTStreamer(
            stt_result_queue=stt_result_queue,
            main_loop=main_loop,
            websocket=websocket,
            sample_rate=self.sample_rate,
            stt_audio_queue=self.stt_audio_queue,
            stt_stop_event=self.stt_stop_event
        )

        # ìƒíƒœ ê´€ë¦¬
        self._is_running = threading.Event()
        self.user_is_speaking = False
        self.silent_chunks_count = 0
        self.turn_chunks_count = 0
        self.in_grace_period = False
        self.grace_period_start_time = 0.0

        self._thread: threading.Thread | None = None

    def _processing_loop(self):
        """
        ì˜¤ë””ì˜¤ íì—ì„œ ë°ì´í„°ë¥¼ ì†Œë¹„í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ë£¨í”„.
        """
        logging.info("ğŸ§ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì‹œì‘.")
        
        # MicrophoneStream ì‹œì‘ (sounddevice ë‚´ë¶€ ìŠ¤ë ˆë“œ ì‹œì‘)
        self.mic_stream.start()

        while self._is_running.is_set():
            try:
                chunk = self.mic_audio_queue.get(timeout=0.1)
            except queue.Empty:
                if self.user_is_speaking:
                    # ì‚¬ìš©ì ë°œí™” ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ, ê°•ì œë¡œ í„´ ì¢…ë£Œ (ì˜ˆì™¸ ì²˜ë¦¬)
                    logging.warning("ì‚¬ìš©ì ë°œí™” ì¤‘ ì˜¤ë””ì˜¤ ì…ë ¥ íƒ€ì„ì•„ì›ƒ. ê°•ì œë¡œ í„´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    self._end_turn()
                continue
            
            if not self.user_is_speaking:
                self._handle_silence_state(chunk)
            else:
                self._handle_speaking_state(chunk)

        logging.info("ğŸ§ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ.")

    def _handle_silence_state(self, chunk: np.ndarray):
        """ì‚¬ìš©ìê°€ ë§í•˜ê³  ìˆì§€ ì•Šì„ ë•Œì˜ ë¡œì§ (ë°œí™” ì‹œì‘ ê°ì§€)"""
        self.stt_pre_buffer.append(chunk)
        self.vad_processor.reset_if_inactive()
        
        if self.vad_processor.process_chunk(chunk):
            logging.info("ğŸ—£ï¸ ì‚¬ìš©ì ë°œí™” ì‹œì‘ ê°ì§€!")
            self.user_is_speaking = True
            self.silent_chunks_count = 0
            self.turn_chunks_count = 0
            self.in_grace_period = False
            self.current_turn_audio.clear()
            self.stt_stop_event.clear()
            
            # STT ì„¸ì…˜ ì‹œì‘
            threading.Thread(
                target=self.stt_streamer.run_stt_session,
                name="STTSessionThread"
            ).start()
            
            # ì‚¬ì „ ë²„í¼ë¥¼ STT íë¡œ ì „ì†¡
            for pre_chunk in self.stt_pre_buffer:
                self.stt_audio_queue.put(pre_chunk)
                self.current_turn_audio.append(pre_chunk)
            
            self.stt_audio_queue.put(chunk)
            self.current_turn_audio.append(chunk)
            self.vad_processor.reset()

    def _handle_speaking_state(self, chunk: np.ndarray):
        """ì‚¬ìš©ìê°€ ë§í•˜ê³  ìˆì„ ë•Œì˜ ë¡œì§ (ë°œí™” ì¢…ë£Œ ê°ì§€)"""
        # STT ë° ë‚´ë¶€ ë²„í¼ë¡œ ì˜¤ë””ì˜¤ ì „ë‹¬
        self.stt_audio_queue.put(chunk)
        self.current_turn_audio.append(chunk)
        self.turn_chunks_count += 1

        # VADë¡œ ë¬´ìŒ ê°ì§€
        is_speech_in_chunk = self.vad_processor.process_chunk(chunk)
        if is_speech_in_chunk:
            self.silent_chunks_count = 0
            if self.in_grace_period:
                logging.info("â³ ìœ ì˜ˆ ê¸°ê°„ ì¤‘ ì¶”ê°€ ë°œí™” ê°ì§€. ìœ ì˜ˆ ê¸°ê°„ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                self.in_grace_period = False
        else:
            self.silent_chunks_count += 1
        
        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        turn_ended = False

        if self.in_grace_period and (time.time() - self.grace_period_start_time) > SMART_TURN_GRACE_PERIOD_S:
            logging.info("â³ ìœ ì˜ˆ ê¸°ê°„ ì¢…ë£Œ. í„´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            turn_ended = True

        elif not self.in_grace_period and self.silent_chunks_count > TURN_END_SILENCE_CHUNKS:
            concatenated_audio_int16 = np.concatenate([c.flatten() for c in self.current_turn_audio])
            full_audio_float32 = concatenated_audio_int16.astype(np.float32) / 32768.0
            
            start_time = time.time()
            result = self.smart_turn_processor.predict(full_audio_float32)
            duration_ms = (time.time() - start_time) * 1000
            
            logging.info(f"ğŸ¤– SmartTurn ì˜ˆì¸¡: {'ì¢…ë£Œ' if result['prediction'] == 1 else 'ì§„í–‰ì¤‘'} (í™•ë¥ : {result['probability']:.2f}, ì†Œìš”ì‹œê°„: {duration_ms:.1f}ms)")
            
            if result['prediction'] == 1:
                turn_ended = True
            else:
                logging.info(f"â³ SmartTurnì´ 'ì§„í–‰ì¤‘'ìœ¼ë¡œ íŒë‹¨. {SMART_TURN_GRACE_PERIOD_S}ì´ˆì˜ ìœ ì˜ˆ ì‹œê°„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                self.in_grace_period = True
                self.grace_period_start_time = time.time()
        
        # elif self.turn_chunks_count > MAX_TURN_CHUNKS:
        #     logging.warning(f"ìµœëŒ€ ë°œí™” ê¸¸ì´({MAX_TURN_CHUNKS * 0.032:.1f}ì´ˆ) ì´ˆê³¼. í„´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        #     turn_ended = True
        
        if turn_ended:
            self._end_turn()

    def _end_turn(self):
        """í˜„ì¬ ë°œí™” í„´ì„ ì¢…ë£Œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if not self.user_is_speaking: return
        
        logging.info("ğŸ¤« ì¸ì‹ ì¢…ë£Œ. STT ì˜¤ë””ì˜¤ ê³µê¸‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        self.stt_stop_event.set()
        self.user_is_speaking = False
        self.in_grace_period = False
        
        # ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” íë¥¼ ë¹„ì›Œ ë‹¤ìŒ í„´ì— ì˜í–¥ì´ ì—†ë„ë¡ í•¨
        with self.stt_audio_queue.mutex:
            self.stt_audio_queue.queue.clear()

    def __enter__(self):
        """AudioProcessorì˜ ìƒëª…ì£¼ê¸° ì‹œì‘."""
        logging.info("AudioProcessor ì»¨í…ìŠ¤íŠ¸ ì‹œì‘...")
        self._is_running.set()
        
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ìŠ¤ë ˆë“œ ì‹œì‘
        self._thread = threading.Thread(target=self._processing_loop, name="AudioProcessingThread")
        self._thread.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """AudioProcessorì˜ ìƒëª…ì£¼ê¸° ì¢…ë£Œ."""
        logging.info("AudioProcessor ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ...")
        
        # 1. ì²˜ë¦¬ ë£¨í”„ ìŠ¤ë ˆë“œì— ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
        self._is_running.clear()
        self.stt_stop_event.set() # ëŒ€ê¸° ì¤‘ì¸ ìŠ¤ë ˆë“œë¥¼ ê¹¨ì›€
        
        # 2. ì²˜ë¦¬ ë£¨í”„ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        
        # 3. MicrophoneStream ì¤‘ì§€ (sounddevice ë‚´ë¶€ ìŠ¤ë ˆë“œ ì¢…ë£Œ)
        self.mic_stream.stop()
        
        logging.info("AudioProcessorê°€ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
