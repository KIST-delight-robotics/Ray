import os
import re
import time
import wave
import json
import queue
import base64
import asyncio
import logging
import websockets
import sounddevice as sd
from pathlib import Path
from openai import AsyncOpenAI
from google.cloud import speech, texttospeech
from google.api_core import exceptions

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s')

# --- ì„¤ì • ---
# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# Google STT, TTS ì¸ì¦ ì •ë³´ ì„¤ì •
# GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ì— Google Cloud API í‚¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •í•´ì•¼ í•¨.

# --- ë””ë ‰í† ë¦¬ ì„¤ì • ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
ASSETS_DIR = PROJECT_ROOT / "assets"
CONFIG_DIR = PROJECT_ROOT / "config"
OUTPUT_DIR = PROJECT_ROOT / "output"

# ê²½ë¡œ ì •ì˜
ASSETS_AUDIO_DIR = ASSETS_DIR / "audio"
MUSIC_DIR = ASSETS_AUDIO_DIR / "music"
OUTPUT_AUDIO_DIR = OUTPUT_DIR / "audio"
LOG_DIR = OUTPUT_DIR / "logs"

# ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ì„ ê²½ìš°)
OUTPUT_AUDIO_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

COMMAND_CONFIG_FILE = str(CONFIG_DIR / "ray_conversation.json")
OUTPUT_GPT_FILE = str(OUTPUT_AUDIO_DIR / "output_gpt.wav")
OUTPUT_TTS_FILE = str(OUTPUT_AUDIO_DIR / "output_tts.wav")
AWAKE_FILE = str(ASSETS_AUDIO_DIR / "awake.wav")
FINISH_FILE = str(ASSETS_AUDIO_DIR / "finish.wav")

# Load command configuration
with open(COMMAND_CONFIG_FILE, encoding="utf-8") as f:
    cfg = json.load(f)["Kor"]

# ì˜¤ë””ì˜¤ ì„¤ì •
CHANNELS = 1
SAMPLE_RATE = 24000
VOICE = "ash"

# --- ì „ì—­ ë³€ìˆ˜ ---
openai_lock = asyncio.Lock()
# ì‹œê°„ ì¸¡ì •ìš© ì „ì—­ ë³€ìˆ˜
stt_first_attempt_flag = True  # STT ì²« ì‹œë„ ì—¬ë¶€ í”Œë˜ê·¸
STT_READY_TIME = 0  # STT ì¤€ë¹„ ì‹œê°„ (í”„ë¡œê·¸ë¨ ì‹œì‘ í›„ ìŒì„± ì…ë ¥ ëŒ€ê¸°ê¹Œì§€ì˜ ì‹œê°„ ì¸¡ì •ìš©)
STT_DONE_TIME = 0  # STT ì™„ë£Œ ì‹œê°„ (ì‚¬ìš©ì ì…ë ¥ ì™„ë£Œ í›„ ìŒì„± ì¶œë ¥ê¹Œì§€ì˜ ì‹œê°„ ì¸¡ì •ìš©)
GPT_RESPONSE_TIME = 0  # GPT ì‘ë‹µ ì‹œê°„ (GPT ì‘ë‹µ ìƒì„±ì— ê±¸ë¦° ì‹œê°„)
GPT_RESPONSE_TEXT = ""

# --- STT ê¸°ëŠ¥ (ê¸°ì¡´ push_to_talk_app.pyì—ì„œ ê°€ì ¸ì˜´) ---
async def run_stt(timeout_sec: float = 5.0):
    """
    timeout_sec ì´ˆ ì•ˆì— ìµœì¢… STT ê²°ê³¼(final_text)ë¥¼ ëª» ì–»ìœ¼ë©´
    ë¹ˆ ë¬¸ìì—´ì„ ë¦¬í„´í•˜ê³  ì¦‰ì‹œ ì¢…ë£Œí•©ë‹ˆë‹¤.
    """
    global STT_READY_TIME, STT_DONE_TIME, stt_first_attempt_flag
    q_audio = queue.Queue()
    recording_done = asyncio.Event()

    def callback(indata, frames, time_info, status):
        if status:
            logging.warning(f"[ì˜¤ë””ì˜¤ ìƒíƒœ] {status}")
        q_audio.put(bytes(indata))

    input_device_index = None
    for idx, dev in enumerate(sd.query_devices()):
        if dev['max_input_channels'] > 0:
            input_device_index = idx
            break

    if input_device_index is None:
        logging.error("[ì˜¤ë¥˜] ì…ë ¥ ê°€ëŠ¥í•œ ë§ˆì´í¬ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    device_info = sd.query_devices(input_device_index, 'input')
    stt_sample_rate = int(device_info['default_samplerate'])

    stt_client = speech.SpeechClient()
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=stt_sample_rate,
        language_code="ko-KR",
    )
    streaming_config = speech.StreamingRecognitionConfig(
        config=config,
        interim_results=True,
        single_utterance=True,
    )

    def audio_generator():
        while not recording_done.is_set():
            chunk = q_audio.get()
            if chunk is None:
                break
            yield speech.StreamingRecognizeRequest(audio_content=chunk)

    final_text = ""

    try:
        with sd.InputStream(samplerate=stt_sample_rate, channels=CHANNELS, dtype="int16", callback=callback):
            logging.info("ğŸ™ï¸ STT ì‹œì‘: ë§í•˜ì„¸ìš”...")
            if stt_first_attempt_flag:
                STT_READY_TIME = time.time() * 1000
            responses = stt_client.streaming_recognize(streaming_config, audio_generator(), timeout=timeout_sec)

            for response in responses:
                if not response.results or not response.results[0].alternatives:
                    continue
                result = response.results[0]
                transcript = result.alternatives[0].transcript

                if result.is_final:
                    final_text += transcript
                    logging.info(f"[âœ… ìµœì¢… ì¸ì‹ ê²°ê³¼] {final_text}")
                    STT_DONE_TIME = time.time() * 1000
                    recording_done.set()
                    return final_text
                else:
                    logging.info(f"[ğŸ“ ì¤‘ê°„ ì¸ì‹] {transcript}")

    except exceptions.DeadlineExceeded:
        logging.warning(f"[STT] íƒ€ì„ì•„ì›ƒ({timeout_sec}s) ë°œìƒ.")
    except Exception as e:
        logging.error(f"STT ì²˜ë¦¬ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        logging.info("ğŸ™ï¸ STT ì¢…ë£Œ.")
        q_audio.put(None)
    
    return final_text

# --- TTS ê¸°ëŠ¥ ---
async def run_tts(text, output_file=OUTPUT_TTS_FILE):
    """
    Google TTSë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    :param text: ë³€í™˜í•  í…ìŠ¤íŠ¸
    :param output_file: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    :return: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    logging.info(f"TTS ìƒì„± ìš”ì²­: '{text}' -> {output_file}")
    tts_client = texttospeech.TextToSpeechClient()
    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(language_code="ko-KR", ssml_gender=texttospeech.SsmlVoiceGender.MALE)
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.LINEAR16, sample_rate_hertz=SAMPLE_RATE)
    
    response = await asyncio.to_thread(tts_client.synthesize_speech, input=synthesis_input, voice=voice, audio_config=audio_config)
    
    with open(output_file, "wb") as out:
        out.write(response.audio_content)
    logging.info(f"TTS íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    return output_file

# --- ë…¸ë˜ íŒŒì¼ ê²€ìƒ‰ ê¸°ëŠ¥ (C++ ë¡œì§ê³¼ ë™ì¼) ---
def normalize_string(input_str):
    return re.sub(r'\s+', '', input_str).lower()

def find_music_file(user_text):
    normalized_input = normalize_string(user_text)
    music_files = list(Path(MUSIC_DIR).glob("*.wav"))
    
    best_match = {'song_path': None, 'type': 'none', 'match_length': 0}

    for f_path in music_files:
        try:
            title, artist = f_path.stem.split('_', 1)
            norm_title = normalize_string(title)
            norm_artist = normalize_string(artist)
            
            # ì œëª© ìš°ì„  ê²€ìƒ‰
            if norm_title in normalized_input:
                if best_match['type'] != 'title' or len(norm_title) > best_match['match_length']:
                    best_match = {'song_path': str(f_path), 'title': title, 'artist': artist, 'type': 'title', 'match_length': len(norm_title)}
            # ì œëª©ì´ ì¼ì¹˜í•˜ì§€ ì•Šì„ ë•Œë§Œ ê°€ìˆ˜ ê²€ìƒ‰
            elif norm_artist in normalized_input and best_match['type'] != 'title':
                 if len(norm_artist) > best_match['match_length']:
                    best_match = {'song_path': str(f_path), 'title': title, 'artist': artist, 'type': 'artist', 'match_length': len(norm_artist)}
        except ValueError:
            logging.warning(f"íŒŒì¼ëª… í˜•ì‹ ì˜¤ë¥˜: {f_path.name}")
            continue
                
    return best_match if best_match['song_path'] else None

# --- OpenAI ì„¸ì…˜ ê´€ë¦¬ ---
async def start_new_openai_session():
    """
    ìƒˆë¡œìš´ OpenAI ì‹¤ì‹œê°„ ì„¸ì…˜ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì„¤ì •í•˜ê³ ,
    connection_managerì™€ connection ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logging.info("ğŸ¤– ìƒˆë¡œìš´ OpenAI ì„¸ì…˜ ì—°ê²° ì‹œì‘...")
    connection_manager = None
    try:
        openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        connection_manager = openai_client.beta.realtime.connect(model="gpt-4o-realtime-preview")
        openai_connection = await connection_manager.__aenter__()
        await openai_connection.session.update(session={
            "instructions": "ë„ˆëŠ” ì• ë‹ˆë§¤íŠ¸ë¡œë‹‰ìŠ¤ ë¡œë´‡ì´ì•¼. ë„ˆì˜ ì´ë¦„ì€ ë ˆì´ì•¼. ë‚´ê°€ ë¬¼ì–´ë³´ëŠ” ê²ƒë“¤ì— ëŒ€í•´ ì˜ ëŒ€ë‹µí•´ì¤˜",
            "voice": VOICE
        })
        logging.info("âœ… ìƒˆë¡œìš´ OpenAI ì„¸ì…˜ì´ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return connection_manager, openai_connection
    except Exception as e:
        logging.error(f"âŒ OpenAI ì„¸ì…˜ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if connection_manager:
            await connection_manager.__aexit__(None, None, None)
        return None, None

async def end_current_openai_session(connection_manager):
    """
    í˜„ì¬ OpenAI ì„¸ì…˜ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.
    """
    if not connection_manager:
        return
    logging.info("ğŸ”Œ OpenAI ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    try:
        await connection_manager.__aexit__(None, None, None)
        logging.info("âœ… OpenAI ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"âŒ OpenAI ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- GPT ì‘ë‹µ ìƒì„± ---
async def generate_gpt_response_audio(user_text: str, openai_connection, log_file: str) -> str:
    """
    ì£¼ì–´ì§„ OpenAI ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì í…ìŠ¤íŠ¸ì— ëŒ€í•œ AI ìŒì„± ì‘ë‹µì„ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    global GPT_RESPONSE_TIME, GPT_RESPONSE_TEXT
    if not openai_connection:
        logging.error("âŒ GPT ìš”ì²­ ì‹¤íŒ¨: OpenAI ì„¸ì…˜ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    async with openai_lock:
        first_received = True
        start_time = time.time() * 1000
        logging.info(f"ğŸ’¬ GPT ëŒ€í™” ì‹œì‘: {user_text}")
        log_conversation("user", user_text, log_file)
        await openai_connection.conversation.item.create(
            item={"type": "message", "role": "user", "content": [{"type": "input_text", "text": user_text}]}
        )
        await openai_connection.response.create()
        with wave.open(OUTPUT_GPT_FILE, "wb") as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)
            wf.setframerate(SAMPLE_RATE)
            accumulated_transcripts = {}
            async for event in openai_connection:
                if event.type == "response.audio.delta":
                    if first_received:
                        logging.info(f"GPT ì‘ë‹µ ì‹œì‘ ì‹œê°„: {time.time() * 1000 - start_time}ms")
                        first_received = False
                    wf.writeframes(base64.b64decode(event.delta))
                elif event.type == "response.audio_transcript.delta":
                    accumulated_transcripts[event.item_id] = accumulated_transcripts.get(event.item_id, "") + event.delta
                elif event.type == "response.audio.done":
                    final_text = accumulated_transcripts.get(event.item_id, "")
                    logging.info(f"[ì‘ë‹µ] {final_text}")
                    GPT_RESPONSE_TIME = time.time() * 1000 - start_time
                    GPT_RESPONSE_TEXT = final_text
                    log_conversation("assistant", final_text, log_file)
                    break
    return OUTPUT_GPT_FILE

# --- ëŒ€í™” ë¡œê¹… ---
def log_conversation(role, text, log_file):
    """ì§€ì •ëœ ë¡œê·¸ íŒŒì¼ì— ëŒ€í™”ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
    if not log_file:
        logging.warning("âš ï¸ ë¡œê·¸ íŒŒì¼ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ëŒ€í™” ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    try:
        with open(log_file, "a", encoding="utf-8") as f:
            log_entry = {"role": role, "content": text, "timestamp": time.time()}
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
    except Exception as e:
        logging.error(f"âŒ ë¡œê·¸ íŒŒì¼({log_file}) ì‘ì„± ì¤‘ ì˜¤ë¥˜: {e}")

# --- ë©”ì¸ í•¸ë“¤ëŸ¬ ---
async def chat_handler(websocket):
    global stt_first_attempt_flag, STT_READY_TIME, STT_DONE_TIME, GPT_RESPONSE_TIME, GPT_RESPONSE_TEXT
    
    ray_mode = "sleep"
    session_task = None
    log_file_path = None

    logging.info(f"âœ… C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨. ì´ˆê¸° ëª¨ë“œ: {ray_mode}")
    
    try:
        async for message in websocket:
            data = json.loads(message)
            if data.get("request") != "next_action":
                continue

            response_payload = None
            user_text = ""
            GPT_RESPONSE_TEXT = ""
            STT_DONE_TIME = 0

            # --- SLEEP ëª¨ë“œ ì²˜ë¦¬ ---
            if ray_mode == "sleep":
                logging.info("ğŸ’¤ Sleep ëª¨ë“œ ì‹œì‘. 'ë ˆì´' í˜¸ì¶œ ëŒ€ê¸° ì¤‘...")
                user_text = await run_stt(timeout_sec=5)
                if "ë ˆì´" in user_text:
                    logging.info("'ë ˆì´' í˜¸ì¶œ ê°ì§€! Active ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    ray_mode = "active"
                    
                    response_payload = {"action": "play_audio", "file_to_play": AWAKE_FILE}

                    if session_task:
                        logging.warning("âš ï¸ ì´ì „ ì„¸ì…˜ ì‘ì—…ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì·¨ì†Œí•˜ê³  ìƒˆ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                        session_task.cancel()

                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    log_file_path = str(LOG_DIR / f"conversation_{timestamp}.json")
                    logging.info(f"ìƒˆ ëŒ€í™” ì„¸ì…˜ ì‹œì‘. ë¡œê·¸ íŒŒì¼: {log_file_path}")
                    session_task = asyncio.create_task(start_new_openai_session())
                else:
                    response_payload = {"action": "sleep"}
            
            # --- ACTIVE ëª¨ë“œ ì²˜ë¦¬ ---
            elif ray_mode == "active":
                logging.info("âš¡ Active ëª¨ë“œ ì‹œì‘. ì‚¬ìš©ì ì§ˆë¬¸ ëŒ€ê¸° ì¤‘...")
                user_text = ""
                for attempt in range(3):
                    stt_first_attempt_flag = (attempt == 0)
                    text = await run_stt(timeout_sec=5.0)
                    if text:
                        user_text = text
                        break
                    logging.info(f"ë¬µë¬µë¶€ë‹µ... ({attempt+1}/3)")

                is_quit_command = any(kw in user_text for kw in cfg["conditions"].get("QUIT_PROGRAM", []))
                if not user_text or is_quit_command:
                    logging.info("ì‘ë‹µ ì—†ê±°ë‚˜ ì¢…ë£Œ ëª…ë ¹ì–´ ê°ì§€. Sleep ëª¨ë“œë¡œ ì „í™˜.")
                    ray_mode = "sleep"
                    response_payload = {"action": "play_audio", "file_to_play": FINISH_FILE}
                    
                    if session_task:
                        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„¸ì…˜ ì¢…ë£Œ ì‹¤í–‰
                        async def end_session_safely(task):
                            try:
                                if task.done() and not task.cancelled():
                                    manager, _ = task.result()
                                    if manager:
                                        await end_current_openai_session(manager)
                                else:
                                    task.cancel()
                            except Exception as e:
                                logging.error(f"ì„¸ì…˜ ì¢…ë£Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        asyncio.create_task(end_session_safely(session_task))
                    
                    session_task = None
                    log_file_path = None
                
                else: # ì‹¤ì œ ëŒ€í™” ì²˜ë¦¬
                    if not session_task:
                        logging.error("ë¹„ì •ìƒì ì¸ ìƒíƒœ: Active ëª¨ë“œì´ì§€ë§Œ ì„¸ì…˜ ìƒì„± ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. Sleep ëª¨ë“œë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤.")
                        ray_mode = "sleep"
                        response_payload = {"action": "play_audio", "file_to_play": FINISH_FILE}
                    else:
                        try:
                            logging.info("OpenAI ì„¸ì…˜ ì¤€ë¹„ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                            connection_manager, openai_connection = await asyncio.wait_for(session_task, timeout=10.0)
                            
                            if not openai_connection:
                                raise ValueError("OpenAI ì„¸ì…˜ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

                            # í‚¤ì›Œë“œ ì²˜ë¦¬ (ë…¸ë˜)
                            if any(kw in user_text for kw in cfg["conditions"].get("SING_A_SONG", [])):
                                logging.info("ë…¸ë˜ ëª…ë ¹ì–´ ê°ì§€.")
                                norm_input = normalize_string(user_text)
                                if norm_input in ["ë…¸ë˜ë¶ˆëŸ¬ì¤˜", "ë…¸ë˜ë“¤ë ¤ì¤˜", "ë…¸ë˜í‹€ì–´ì¤˜"]:
                                    response_text = "ë„¤, ë¬´ìŠ¨ ë…¸ë˜ ë¶ˆëŸ¬ì¤„ê¹Œìš”?"
                                    file_to_play = await run_tts(response_text, OUTPUT_TTS_FILE)
                                    response_payload = {"action": "play_audio", "file_to_play": str(file_to_play)}
                                else:
                                    found_song_info = find_music_file(user_text)
                                    if found_song_info:
                                        title, artist = found_song_info['title'], found_song_info['artist']
                                        response_text = f"{title} ë§ì”€ì´ì‹ ê°€ìš”? ì§€ê¸ˆ {title} by {artist}ë¥¼ ì¬ìƒí• ê²Œìš”."
                                        file_to_play = await run_tts(response_text, OUTPUT_TTS_FILE)
                                        response_payload = {"action": "play_music", "file_to_play": file_to_play, "title": title, "artist": artist}
                                    else:
                                        response_text = "ë§ì”€í•˜ì‹  ê³¡ì€ ëª©ë¡ì— ì—†ì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”!"
                                        file_to_play = await run_tts(response_text, OUTPUT_TTS_FILE)
                                        response_payload = {"action": "play_audio", "file_to_play": str(file_to_play)}
                            # ì¼ë°˜ ëŒ€í™” (GPT)
                            else:
                                file_to_play = await generate_gpt_response_audio(user_text, openai_connection, log_file_path)
                                response_payload = {
                                    "action": "play_audio",
                                    "file_to_play": str(file_to_play) if file_to_play else None,
                                    "stt_ready_time": STT_READY_TIME, "stt_done_time": STT_DONE_TIME,
                                    "gpt_response_time": GPT_RESPONSE_TIME,
                                    "user_text": user_text, "gpt_response_text": GPT_RESPONSE_TEXT
                                }

                        except (asyncio.TimeoutError, ValueError) as e:
                            logging.error(f"âŒ ì„¸ì…˜ ì¤€ë¹„ ì‹¤íŒ¨ ({type(e).__name__}). Sleep ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                            ray_mode = "sleep"
                            response_payload = {"action": "play_audio", "file_to_play": FINISH_FILE}
                            if session_task:
                                session_task.cancel()
                            session_task = None
                        except Exception as e:
                            logging.error(f"âŒ Active ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}. Sleep ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.", exc_info=True)
                            ray_mode = "sleep"
                            response_payload = {"action": "play_audio", "file_to_play": FINISH_FILE}
                            if session_task and session_task.done() and not session_task.cancelled():
                                manager, _ = session_task.result()
                                if manager:
                                    asyncio.create_task(end_current_openai_session(manager))
                            session_task = None

            if response_payload:
                await websocket.send(json.dumps(response_payload))
                logging.info(f"C++ í´ë¼ì´ì–¸íŠ¸ì— ì‘ë‹µ ì „ì†¡: {response_payload['action']}")

    except websockets.exceptions.ConnectionClosed as e:
        logging.warning(f"â„¹ï¸ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
        if session_task:
            logging.info("í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¡œ ì¸í•œ ì„¸ì…˜ ì •ë¦¬ ì‹œì‘.")
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
            async def final_cleanup(task):
                if task.done() and not task.cancelled():
                    try:
                        manager, _ = task.result()
                        if manager: await end_current_openai_session(manager)
                    except Exception as ex:
                        logging.error(f"ìµœì¢… ì„¸ì…˜ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {ex}")
                else:
                    task.cancel()
            asyncio.create_task(final_cleanup(session_task))
    except Exception as e:
        logging.error(f"âŒ í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    finally:
        logging.info(f"ğŸ”Œ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•¸ë“¤ëŸ¬ ì¢…ë£Œ: {websocket.remote_address}")

# --- Google STT API ì›Œë°ì—… ---
async def warm_up_stt_api():
    """
    ìµœì´ˆ 1íšŒ ì‹¤í–‰í•©ë‹ˆë‹¤.
    Google STT APIì— ë”ë¯¸ ìš”ì²­ì„ ë³´ë‚´ ì´ˆê¸° ì—°ê²° ì§€ì—°ì„ í•´ì†Œí•©ë‹ˆë‹¤.
    """
    logging.info("â˜ï¸ Google STT API ì›Œë°ì—… ì‹œì‘...")
    start_time = time.time()
    try:
        def dummy_audio_generator():
            yield speech.StreamingRecognizeRequest(audio_content=b'\x00\x00')

        stt_client = speech.SpeechClient()
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="ko-KR",
        )
        streaming_config = speech.StreamingRecognitionConfig(
            config=config, single_utterance=True
        )

        def run_dummy_request():
            responses = stt_client.streaming_recognize(streaming_config, dummy_audio_generator())
            for _ in responses: pass
                
        await asyncio.to_thread(run_dummy_request)
        elapsed_time = time.time() - start_time
        logging.info("â˜ï¸ Google STT API ì›Œë°ì—… ì™„ë£Œ. ì†Œìš” ì‹œê°„: {:.2f}ì´ˆ".format(elapsed_time))
    except Exception as e:
        logging.error(f"âŒ STT API ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

async def main():
    logging.info("ğŸš€ ì„œë²„ ì´ˆê¸°í™” ì‹œì‘: API ì›Œë°ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    try:
        await warm_up_stt_api()
        server = await websockets.serve(chat_handler, "127.0.0.1", 5000)
        logging.info("ğŸš€ í†µí•© WebSocket ì„œë²„ê°€ 127.0.0.1:5000 ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        await server.wait_closed()
    except Exception as e:
        logging.error(f"âŒ ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass