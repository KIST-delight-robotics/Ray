import os
import sys
import time
import json
import queue
import asyncio
import logging
import threading
import math
from collections import deque

import websockets
import sounddevice as sd
import torch
import torchaudio
import numpy as np
from pathlib import Path
from openai import AsyncOpenAI
from google.cloud import speech
from google.api_core import exceptions

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%H:%M:%S')

# --- ê¸°ë³¸ ì„¤ì • --- 
# OpenAI & Google Cloud ì¸ì¦ ì •ë³´ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# --- ê²½ë¡œ ì„¤ì • ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if PROJECT_ROOT not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

import config.prompts as prompts

ASSETS_DIR = PROJECT_ROOT / "assets"
OUTPUT_DIR = PROJECT_ROOT / "output"
OUTPUT_AUDIO_DIR = OUTPUT_DIR / "audio"
OUTPUT_AUDIO_DIR.mkdir(parents=True, exist_ok=True)

# ì¬ìƒìš© ì˜¤ë””ì˜¤ íŒŒì¼
AWAKE_FILE = ASSETS_DIR / "audio" / "awake.wav"
SLEEP_FILE = ASSETS_DIR / "audio" / "sleep.wav"

# --- ì˜¤ë””ì˜¤ ì„¤ì • ---
# Google STT ê¶Œì¥ì‚¬í•­ ë° VAD ëª¨ë¸ê³¼ì˜ í†µì¼ì„ ìœ„í•´ 16000Hzë¡œ ìƒ˜í”Œë ˆì´íŠ¸ë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
SAMPLE_RATE = 16000
CHANNELS = 1
AUDIO_DTYPE = "int16"

# --- OpenAI ì„¤ì • ---
PROMPT = prompts.MONDAY_PROMPT
VOICE = "coral"

# ==================================================================================================
# ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° (VAD & STT í†µí•©)
# ==================================================================================================

class AudioProcessor:
    """ë§ˆì´í¬ ì…ë ¥ë¶€í„° VAD, STTê¹Œì§€ ëª¨ë“  ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì „ë‹´í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, stt_result_queue: asyncio.Queue, main_loop: asyncio.AbstractEventLoop, websocket):
        # --- ìƒíƒœ ë³€ìˆ˜ ---
        self.stt_result_queue = stt_result_queue
        self.main_loop = main_loop
        self.websocket = websocket
        self.is_running = threading.Event()
        self.vad_active_flag = threading.Event() # VAD ê°ì§€ í™œì„±í™” í”Œë˜ê·¸
        self.vad_active_flag.set() # ì´ˆê¸° ìƒíƒœëŠ” 'í™œì„±í™”(set)'ë¡œ ì„¤ì •

        # --- ì˜¤ë””ì˜¤ ë²„í¼ ---
        self.audio_queue = queue.Queue() # ë§ˆì´í¬ ì½œë°±ì—ì„œ ë°›ì€ ì›ë³¸ ì˜¤ë””ì˜¤ê°€ ìŒ“ì´ëŠ” ê³³
        # VAD ê°ì§€ ì „ ì˜¤ë””ì˜¤ë¥¼ ì €ì¥í•˜ëŠ” ë¡¤ë§ ë²„í¼ (numpy ë°°ì—´ ì²­í¬ ì €ì¥)
        PRE_BUFFER_DURATION = 0.5  # ì‚¬ì „ ë²„í¼ë§ ì‹œê°„ (ì´ˆ)
        self.VAD_CHUNK_SIZE = 512 # Silero VADëŠ” 16kHzì—ì„œ 512 ìƒ˜í”Œ í¬ê¸°ë¥¼ ì‚¬ìš©
        pre_buffer_max_chunks = math.ceil(SAMPLE_RATE * PRE_BUFFER_DURATION / self.VAD_CHUNK_SIZE)
        self.stt_pre_buffer = deque(maxlen=pre_buffer_max_chunks)
        self.vad_buffer = torch.tensor([]) # VAD ì²˜ë¦¬ë¥¼ ìœ„í•œ ë²„í¼

        # --- VAD ì„¤ì • ---
        model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=True)
        self.vad_model = model
        self.VAD_THRESHOLD = 0.5  # ìŒì„±ìœ¼ë¡œ íŒë‹¨í•  í™•ë¥  ì„ê³„ê°’
        self.VAD_CONSECUTIVE_CHUNKS = 3 # ì—°ì†ìœ¼ë¡œ ê°ì§€í•´ì•¼í•  ì²­í¬ ìˆ˜
        self.consecutive_speech_chunks = 0
        logging.info("âœ… Silero VAD ì´ˆê¸°í™” ì™„ë£Œ")

        # --- STT ì„¤ì • ---
        self.stt_client = speech.SpeechClient()
        self.stt_config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=SAMPLE_RATE,
            language_code="ko-KR",
        )
        self.stt_streaming_config = speech.StreamingRecognitionConfig(
            config=self.stt_config,
            interim_results=True,
            single_utterance=True,
        )
        logging.info("âœ… Google STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _audio_callback(self, indata, frames, time_info, status):
        """ì‚¬ìš´ë“œë””ë°”ì´ìŠ¤ ì½œë°±. ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ íì— ë„£ê¸°ë§Œ í•¨."""
        if status:
            # ALSA ì—ëŸ¬ë¥¼ ë¡œê·¸ë¡œë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
            if status.input_overflow:
                logging.debug("Input overflow ë°œìƒ (ì¼ì‹œì )")
            elif status.input_underflow:
                logging.debug("Input underflow ë°œìƒ (ì¼ì‹œì )")
            else:
                logging.warning(f"[ì˜¤ë””ì˜¤ ìƒíƒœ] {status}")
        
        try:
            self.audio_queue.put(indata.copy())
        except Exception as e:
            logging.debug(f"ì˜¤ë””ì˜¤ í ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    def _stt_audio_generator(self, stt_should_stop=None):
        """STT APIì— ì˜¤ë””ì˜¤ë¥¼ ê³µê¸‰í•˜ëŠ” ì œë„ˆë ˆì´í„°. ì‚¬ì „ ë²„í¼ -> ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìˆœìœ¼ë¡œ ê³µê¸‰."""
        # 1. VADê°€ ê°ì§€ë˜ê¸° ì „ê¹Œì§€ ìŒ“ì•„ë‘” ì‚¬ì „ ë²„í¼(pre-buffer)ë¶€í„° ë³´ëƒ„
        if self.stt_pre_buffer:
            combined_audio = np.concatenate(list(self.stt_pre_buffer))
            duration_sec = len(combined_audio) / SAMPLE_RATE
            yield speech.StreamingRecognizeRequest(audio_content=combined_audio.tobytes())
            logging.info(f"ì‚¬ì „ ë²„í¼ ({duration_sec:.2f}ì´ˆ) ì „ì†¡ ì™„ë£Œ")
            self.stt_pre_buffer.clear() # ì‚¬ì „ ë²„í¼ëŠ” ì „ì†¡ í›„ ì´ˆê¸°í™”

        # 2. ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì˜¤ë””ì˜¤ ì „ì†¡
        while not self.vad_active_flag.is_set():
            # íƒ€ì„ì•„ì›ƒ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
            if stt_should_stop and stt_should_stop.is_set():
                logging.info("íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•´ ì˜¤ë””ì˜¤ ìƒì„±ê¸° ì¤‘ë‹¨")
                break
                
            try:
                chunk = self.audio_queue.get(timeout=0.1)
                yield speech.StreamingRecognizeRequest(audio_content=chunk.tobytes())
            except queue.Empty:
                if self.vad_active_flag.is_set():
                    break
                continue

    def _run_stt(self):
        """ë‹¨ì¼ STT ì„¸ì…˜ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜. ì´ í•¨ìˆ˜ëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë¨."""
        
        first_response_timeout = 3.0  # ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        start_time = time.time()
        has_received_first_response = threading.Event()
        stt_should_stop = threading.Event()
        
        def timeout_checker():
            """ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²´í¬í•˜ëŠ” í•¨ìˆ˜"""
            if not has_received_first_response.wait(timeout=first_response_timeout):
                logging.warning(f"STT ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ ({first_response_timeout}ì´ˆ) - ì„¸ì…˜ ì¢…ë£Œ")
                stt_should_stop.set()
        
        # íƒ€ì„ì•„ì›ƒ ì²´ì»¤ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        timeout_thread = threading.Thread(target=timeout_checker, daemon=True)
        timeout_thread.start()
        
        try:
            responses = self.stt_client.streaming_recognize(self.stt_streaming_config, self._stt_audio_generator(stt_should_stop))
            
            for response in responses:
                # STT ì¤‘ë‹¨ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                if stt_should_stop.is_set():
                    logging.info("íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ STT ì„¸ì…˜ ì¤‘ë‹¨")
                    return
                
                # ì²« ì‘ë‹µì´ ë„ì°©í–ˆìŒì„ ì•Œë¦¼
                if not has_received_first_response.is_set():
                    has_received_first_response.set()
                    logging.info(f"STT ì²« ì‘ë‹µ ìˆ˜ì‹  (ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
                    
                    # c++ì— ì¸í„°ëŸ½ì…˜ ì‹ í˜¸ ì „ì†¡
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "user_interruption"})),
                        self.main_loop
                    )

                if not response.results or not response.results[0].alternatives:
                    continue
                result = response.results[0]
                transcript = result.alternatives[0].transcript
                if result.is_final:
                    final_text = result.alternatives[0].transcript.strip()
                    logging.info(f"âœ… STT ìµœì¢… ê²°ê³¼: '{final_text}'")
                    # ë©”ì¸ asyncio ë£¨í”„ë¡œ ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ ì „ì†¡
                    if final_text: # ìµœì¢… í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œë§Œ íì— ë„£ìŒ
                        self.main_loop.call_soon_threadsafe(self.stt_result_queue.put_nowait, final_text)
                    return
                else:
                    logging.info(f"âœ… STT ì¤‘ê°„ ê²°ê³¼: '{transcript}'")
        except exceptions.DeadlineExceeded as e:
            logging.error(f"STT ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ(DeadlineExceeded): {e}")
        except Exception as e:
            logging.error(f"STT ì„¸ì…˜ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            # íƒ€ì„ì•„ì›ƒ ì²´ì»¤ ìŠ¤ë ˆë“œ ì •ë¦¬
            has_received_first_response.set()  # íƒ€ì„ì•„ì›ƒ ìŠ¤ë ˆë“œê°€ ëŒ€ê¸° ì¤‘ì´ë¼ë©´ ê¹¨ì›Œì„œ ì¢…ë£Œì‹œí‚´
            self.stt_pre_buffer.clear() # ì‚¬ì „ ë²„í¼ ì´ˆê¸°í™”
            self.vad_model.reset_states() # VAD ëª¨ë¸ ìƒíƒœ ì´ˆê¸°í™”
            self.vad_active_flag.set() # VAD ë£¨í”„ë¥¼ ë‹¤ì‹œ ì‹œì‘í•˜ë„ë¡ ì‹ í˜¸
            logging.info("STT ì„¸ì…˜ ì¢…ë£Œ ë° VAD ê°ì§€ ì‹œì‘")

    def start(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œì˜ ë©”ì¸ ë£¨í”„. ì´ í•¨ìˆ˜ê°€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë¨."""
        self.is_running.set()
        logging.info("ğŸ™ï¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘...")

        device_idx = find_input_device()
        if device_idx is None: return

        try:
            with sd.InputStream(
                samplerate=SAMPLE_RATE,
                channels=CHANNELS,
                dtype=AUDIO_DTYPE,
                callback=self._audio_callback,
                device=device_idx,
                blocksize=self.VAD_CHUNK_SIZE
            ):
                while self.is_running.is_set():
                    # STT ì‹¤í–‰ì¤‘ì¼ ê²½ìš° ëŒ€ê¸°
                    self.vad_active_flag.wait()
                    if not self.is_running.is_set(): break

                    try:
                        # ì²˜ë¦¬ ì „ í ì‚¬ì´ì¦ˆë¥¼ í™•ì¸í•˜ì—¬ ì²˜ë¦¬ê°€ ë°€ë¦¬ëŠ”ì§€ íŒŒì•…
                        queue_size = self.audio_queue.qsize()
                        if queue_size > 1:
                            logging.warning(f"ì˜¤ë””ì˜¤ íê°€ ë°€ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ í¬ê¸°: {queue_size}")
                        
                        audio_chunk_int16 = self.audio_queue.get(timeout=0.1)
                    except queue.Empty:
                        continue

                    start_time = time.perf_counter()

                    # ì‚¬ì „ ë²„í¼ ì €ì¥ (ë¡¤ë§ ë²„í¼)
                    self.stt_pre_buffer.append(audio_chunk_int16)

                    # VAD ì²˜ë¦¬ë¥¼ ìœ„í•´ float32ë¡œ ë³€í™˜í•˜ê³  ë²„í¼ì— ì¶”ê°€
                    audio_chunk_float32 = audio_chunk_int16.astype(np.float32) / 32768.0
                    audio_tensor = torch.from_numpy(audio_chunk_float32.flatten())

                    if len(audio_tensor) == self.VAD_CHUNK_SIZE and len(self.vad_buffer) == 0:
                        speech_prob = self.vad_model(audio_tensor, SAMPLE_RATE).item()
                        if speech_prob > self.VAD_THRESHOLD:
                            self.consecutive_speech_chunks += 1
                        else:
                            self.consecutive_speech_chunks = 0
                    else:
                        logging.debug("ì˜ˆì™¸ ê²½ë¡œ ì‹¤í–‰: ì˜¤ë””ì˜¤ ì²­í¬ í¬ê¸°ê°€ ë¹„ì •ìƒì´ê±°ë‚˜ ë²„í¼ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
                        self.vad_buffer = torch.cat([self.vad_buffer, audio_tensor])
                        
                        # ë²„í¼ì— VADë¥¼ ì²˜ë¦¬í•  ë§Œí¼ì˜ ë°ì´í„°ê°€ ìŒ“ì˜€ëŠ”ì§€ í™•ì¸.
                        while len(self.vad_buffer) >= self.VAD_CHUNK_SIZE:
                            vad_chunk = self.vad_buffer[:self.VAD_CHUNK_SIZE]
                            self.vad_buffer = self.vad_buffer[self.VAD_CHUNK_SIZE:]

                            speech_prob = self.vad_model(vad_chunk, SAMPLE_RATE).item()
                            if speech_prob > self.VAD_THRESHOLD:
                                self.consecutive_speech_chunks += 1
                            else:
                                self.consecutive_speech_chunks = 0
                            break
                    
                    # ì—°ì†ì ìœ¼ë¡œ ìŒì„±ì´ ê°ì§€ë˜ë©´ STT ì„¸ì…˜ì„ ì‹œì‘.
                    if self.consecutive_speech_chunks >= self.VAD_CONSECUTIVE_CHUNKS:
                        self.vad_active_flag.clear() # VAD ë£¨í”„ë¥¼ 'ëŒ€ê¸°' ìƒíƒœë¡œ ì „í™˜.
                        logging.info(f"ğŸ—£ï¸ ìŒì„± ì‹œì‘ ê°ì§€! STT ì‹œì‘.")
                        threading.Thread(target=self._run_stt).start()
                        
                        # STT ì‹œì‘ê³¼ í•¨ê»˜ VAD ê´€ë ¨ ìƒíƒœë¥¼ ê¹¨ë—í•˜ê²Œ ì´ˆê¸°í™”.
                        self.vad_buffer = torch.tensor([])
                        self.consecutive_speech_chunks = 0

                    processing_time_ms = (time.perf_counter() - start_time) * 1000
                    logging.debug(f"ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ì‹œê°„: {processing_time_ms:.2f}ms")

        except Exception as e:
            logging.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)

    def stop(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ."""
        self.is_running.clear()
        self.vad_active_flag.set() # ëŒ€ê¸° ìƒíƒœì˜ ìŠ¤ë ˆë“œê°€ ìˆë‹¤ë©´ ì¦‰ì‹œ ê¹¨ì›Œì„œ ì¢…ë£Œë˜ë„ë¡
        logging.info("ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡")



# ==================================================================================================
# ë¹„ë™ê¸° í†µì‹  ë° ë©”ì¸ ë¡œì§
# ==================================================================================================

async def handle_stt_results(stt_queue: asyncio.Queue, openai_connection, session_state, session_end_flag: asyncio.Event):
    """(íƒœìŠ¤í¬ A) STT ê²°ê³¼ë¥¼ ë°›ì•„ OpenAIì— ì „ì†¡í•˜ëŠ” ì—­í• """
    while True:
        try:
            user_text = await stt_queue.get()
            if not user_text:
                continue

            # AIê°€ ì‘ë‹µ ì¤‘ì´ì—ˆë‹¤ë©´, ì‘ë‹µì„ ì¤‘ë‹¨ì‹œí‚´
            if session_state['is_streaming_response']:
                try:
                    await openai_connection.response.cancel()
                    logging.info("ê¸°ì¡´ AI ì‘ë‹µì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                    session_state['is_streaming_response'] = False
                except Exception as e:
                    logging.warning(f"ì‘ë‹µ ì¤‘ë‹¨ ì¤‘ ì˜¤ë¥˜: {e}")
            
            if any(kw in user_text for kw in ["ì¢…ë£Œ", "ì‰¬ì–´"]):
                # ì¢…ë£Œ í‚¤ì›Œë“œ ê°ì§€ ì‹œ ì„¸ì…˜ ì¢…ë£Œ, Sleep ëª¨ë“œë¡œ ì „í™˜
                logging.info(f"ì¢…ë£Œ í‚¤ì›Œë“œ ê°ì§€: '{user_text}' - ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                session_end_flag.set() 
                break  # STT ê²°ê³¼ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ
            else:
                # OpenAIì— ì‚¬ìš©ì ë©”ì‹œì§€ ì „ì†¡ ë° AI ì‘ë‹µ ìš”ì²­
                await openai_connection.conversation.item.create(
                    item={"type": "message", "role": "user", "content": [{"type": "input_text", "text": user_text}]}
                )
                await openai_connection.response.create()
                logging.info(f"OpenAIì— ì‚¬ìš©ì ë©”ì‹œì§€ '{user_text}' ì „ì†¡ ë° ì‘ë‹µ ìš”ì²­")

        except asyncio.CancelledError:
            logging.info("STT ê²°ê³¼ ì²˜ë¦¬ íƒœìŠ¤í¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            logging.error(f"STT ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

async def handle_openai_responses(openai_connection, websocket, session_state, session_end_flag: asyncio.Event):
    """(íƒœìŠ¤í¬ B) OpenAIì˜ ì‘ë‹µì„ ë°›ì•„ C++ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡í•˜ëŠ” ì—­í• """
    try:
        async for event in openai_connection:
            if event.type == "response.created":
                session_state['is_streaming_response'] = True
                await websocket.send(json.dumps({"type": "gpt_stream_start"}))
            
            elif event.type == "response.audio.delta":
                await websocket.send(json.dumps({"type": "audio_chunk", "data": event.delta}))

            elif event.type == "response.done":
                session_state['is_streaming_response'] = False
                await websocket.send(json.dumps({"type": "gpt_stream_end"}))
                response = event.response.output[0].content[0].transcript
                logging.info(f"OpenAI ì‘ë‹µ ì™„ë£Œ: '{response}'")

    except asyncio.CancelledError:
        logging.info("OpenAI ì‘ë‹µ ì²˜ë¦¬ íƒœìŠ¤í¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"OpenAI ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        if not session_end_flag.set():
            session_end_flag.set()

async def realtime_session(websocket):
    """ì‚¬ìš©ì ë°œí™”ì™€ AI ì‘ë‹µì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ì„¸ì…˜ (Active ëª¨ë“œ)"""
    logging.info("ğŸ¤– Realtime GPT ì„¸ì…˜ ì‹œì‘...")
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    audio_processor = None
    audio_thread = None

    try:
        async with openai_client.beta.realtime.connect(model="gpt-4o-realtime-preview") as openai_connection:
            await openai_connection.session.update(session={
                "instructions": PROMPT,
                "voice": VOICE
            })

            stt_result_queue = asyncio.Queue()
            main_loop = asyncio.get_running_loop()
            session_state = {'is_streaming_response': False}
            session_end_flag = asyncio.Event()  # ì„¸ì…˜ ì¢…ë£Œ ì‹ í˜¸ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸

            # 1. ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° ìƒì„± ë° ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            audio_processor = AudioProcessor(stt_result_queue, main_loop, websocket)
            audio_thread = threading.Thread(target=audio_processor.start, daemon=True)
            audio_thread.start()

            # 2. STT ê²°ê³¼ ì²˜ë¦¬ì™€ OpenAI ì‘ë‹µ ì²˜ë¦¬ë¥¼ ë‘ ê°œì˜ íƒœìŠ¤í¬ë¡œ ë§Œë“¤ì–´ ë™ì‹œ ì‹¤í–‰
            task_a = asyncio.create_task(handle_stt_results(stt_result_queue, openai_connection, session_state, session_end_flag))
            task_b = asyncio.create_task(handle_openai_responses(openai_connection, websocket, session_state, session_end_flag))

            await session_end_flag.wait()

    except websockets.exceptions.ConnectionClosed:
        logging.warning("í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"Realtime GPT ì„¸ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    finally:
        # audio_processorì™€ audio_thread ì •ë¦¬
        if audio_processor:
            audio_processor.stop()
        if audio_thread and audio_thread.is_alive():
            audio_thread.join(timeout=1.0)
        # íƒœìŠ¤í¬ ì·¨ì†Œ
        if 'task_a' in locals() and not task_a.done():
            task_a.cancel()
        if 'task_b' in locals() and not task_b.done():
            task_b.cancel()
        logging.info("ğŸ¤– Realtime GPT ì„¸ì…˜ ì¢…ë£Œ.")

# --- Sleep ëª¨ë“œ ë¡œì§ ---
async def wakeword_detection_loop(websocket, keyword: str = "ë ˆì´"):
    """'ë ˆì´'ë¼ëŠ” í‚¤ì›Œë“œë¥¼ ê°ì§€í•  ë•Œê¹Œì§€ VAD-STT ë£¨í”„ë¥¼ ì‹¤í–‰ (Sleep ëª¨ë“œ)"""
    logging.info(f"ğŸ’¤ Sleep ëª¨ë“œ ì‹œì‘. '{keyword}' í˜¸ì¶œ ëŒ€ê¸° ì¤‘...")
    audio_processor = None
    audio_thread = None
    
    try:
        keyword_queue = asyncio.Queue()
        main_loop = asyncio.get_running_loop()

        audio_processor = AudioProcessor(keyword_queue, main_loop, websocket)
        audio_thread = threading.Thread(target=audio_processor.start, daemon=True)
        audio_thread.start()

        while True:
            stt_result = await keyword_queue.get()
            logging.info(f"[Sleep Mode] STT ê²°ê³¼: {stt_result}")
            if keyword in stt_result:
                logging.info(f"'{keyword}' í˜¸ì¶œ ê°ì§€! Active ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return # í˜¸ì¶œì´ ê°ì§€ë˜ë©´ í•¨ìˆ˜ ì¢…ë£Œ -> Active ëª¨ë“œë¡œ ì „í™˜
    
    except asyncio.CancelledError:
        logging.info("í˜¸ì¶œ ê°ì§€ ë£¨í”„ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"í˜¸ì¶œ ê°ì§€ ë£¨í”„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        if audio_processor:
            audio_processor.stop()
        if audio_thread and audio_thread.is_alive():
            audio_thread.join(timeout=1.0)
        logging.info("ğŸ’¤ Sleep ëª¨ë“œ ì¢…ë£Œ.")


# --- ê¸°ì¡´ í—¬í¼ í•¨ìˆ˜ë“¤ ---
def find_input_device():
    try:
        devices = sd.query_devices()
        for idx, device in enumerate(devices):
            if device['max_input_channels'] > 0 and 'pipewire' in str(device['name']).lower():
                logging.info(f"ğŸ” ë°œê²¬ëœ ì…ë ¥ ì¥ì¹˜: [{idx}] {device['name']}")
                return idx
        logging.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        logging.error(f"ì¥ì¹˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --- ë©”ì¸ í•¸ë“¤ëŸ¬ ë° ì„œë²„ ì‹œì‘ ---
async def chat_handler(websocket):
    logging.info(f"âœ… C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨: {websocket.remote_address}")
    
    try:
        while True:
            # 1. Sleep ëª¨ë“œ: í‚¤ì›Œë“œ ê°ì§€ ëŒ€ê¸°
            await wakeword_detection_loop(websocket)

            # Sleep ëª¨ë“œ ì¢…ë£Œ í›„ ê¸°ìƒ ìŒì„± ì¬ìƒ
            await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(AWAKE_FILE)}))

            # 2. Active ëª¨ë“œ: ì‹¤ì‹œê°„ ëŒ€í™” ì„¸ì…˜ ì§„í–‰
            await realtime_session(websocket)
            
            # Active ëª¨ë“œê°€ ëë‚˜ë©´ ë‹¤ì‹œ Sleep ëª¨ë“œë¡œ ëŒì•„ê°
            await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(SLEEP_FILE)}))
            logging.info("Active ì„¸ì…˜ ì¢…ë£Œ. ë‹¤ì‹œ Sleep ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

    except websockets.exceptions.ConnectionClosed:
        logging.warning(f"ğŸ”Œ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¨: {websocket.remote_address}")
    except Exception as e:
        logging.error(f"Chat í•¸ë“¤ëŸ¬ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
    finally:
        logging.info(f"ğŸ”Œ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•¸ë“¤ëŸ¬ ì¢…ë£Œ: {websocket.remote_address}")

async def main():
    logging.info("ğŸš€ ì„œë²„ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    server = await websockets.serve(chat_handler, "127.0.0.1", 5000)
    logging.info("ğŸš€ í†µí•© WebSocket ì„œë²„ê°€ 127.0.0.1:5000 ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    await server.wait_closed()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")