import os
import sys
import time
import json
import queue
import asyncio
import logging
import threading
import math
import base64
from collections import deque

import websockets
import sounddevice as sd
import torch
import numpy as np
from pathlib import Path
from openai import AsyncOpenAI
from google.cloud import speech
from google.api_core import exceptions

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%H:%M:%S')

# --- ê¸°ë³¸ ì„¤ì • ---
# OpenAI í‚¤ & Google Cloud ì¸ì¦íŒŒì¼ ê²½ë¡œ í™˜ê²½ë³€ìˆ˜ ë“±ë¡ í•„ìš”
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# --- ê²½ë¡œ ì„¤ì • ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if PROJECT_ROOT not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from config.prompts import SYSTEM_PROMPT, REALTIME_PROMPT

ASSETS_DIR = PROJECT_ROOT / "assets"
OUTPUT_DIR = PROJECT_ROOT / "output"
OUTPUT_AUDIO_DIR = OUTPUT_DIR / "audio"
OUTPUT_AUDIO_DIR.mkdir(parents=True, exist_ok=True)

# ì¬ìƒìš© ì˜¤ë””ì˜¤ íŒŒì¼
AWAKE_FILE = ASSETS_DIR / "audio" / "awake.wav"
SLEEP_FILE = ASSETS_DIR / "audio" / "sleep.wav"

# --- ì˜¤ë””ì˜¤ ì„¤ì • ---
SAMPLE_RATE = 16000
CHANNELS = 1
AUDIO_DTYPE = "int16"

# --- OpenAI ì„¤ì • ---
VOICE = "coral"
TTS_MODEL = "gpt-4o-mini-tts"

# --- í‚¤ì›Œë“œ ì„¤ì • ---
START_KEYWORD = "ë ˆì´"
END_KEYWORDS = ["ì¢…ë£Œ", "ì‰¬ì–´"]


def find_input_device():
    """ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ ê²€ìƒ‰"""
    try:
        devices = sd.query_devices()
        for idx, device in enumerate(devices):
            if device['max_input_channels'] > 0 and 'pipewire' in str(device['name']).lower():
                logging.info(f"ğŸ” ë°œê²¬ëœ ì…ë ¥ ì¥ì¹˜: [{idx}] {device['name']}")
                return idx
        logging.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        logging.error(f"ì¥ì¹˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# ==================================================================================================
# ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° (VAD & STT í†µí•©)
# ==================================================================================================

class AudioProcessor:
    """ë§ˆì´í¬ ì…ë ¥ë¶€í„° VAD, STTê¹Œì§€ ëª¨ë“  ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì „ë‹´í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, stt_result_queue: asyncio.Queue, main_loop: asyncio.AbstractEventLoop, websocket, adaptation_config=None):
        # --- ìƒíƒœ ë³€ìˆ˜ ---
        self.stt_result_queue = stt_result_queue
        self.main_loop = main_loop
        self.websocket = websocket
        self.is_running = threading.Event()
        self.vad_active_flag = threading.Event()
        self.vad_active_flag.set()

        # --- ì˜¤ë””ì˜¤ ë²„í¼ ---
        # ì›ë³¸ ì˜¤ë””ì˜¤ ë²„í¼
        self.audio_queue = queue.Queue()
        # STT ì‚¬ì „ ë²„í¼
        PRE_BUFFER_DURATION = 0.3 # ì‚¬ì „ ë²„í¼ ê¸¸ì´ (ì´ˆ)
        self.VAD_CHUNK_SIZE = 512
        pre_buffer_max_chunks = math.ceil(SAMPLE_RATE * PRE_BUFFER_DURATION / self.VAD_CHUNK_SIZE)
        self.stt_pre_buffer = deque(maxlen=pre_buffer_max_chunks)
        # VAD ì²˜ë¦¬ë¥¼ ìœ„í•œ ë²„í¼
        self.vad_buffer = torch.tensor([])

        # --- VAD ì„¤ì • ---
        model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=True)
        self.vad_model = model
        self.VAD_THRESHOLD = 0.5  # ìŒì„±ìœ¼ë¡œ íŒë‹¨í•  í™•ë¥  ì„ê³„ê°’
        self.VAD_CONSECUTIVE_CHUNKS = 3 # ì—°ì†ìœ¼ë¡œ ê°ì§€í•´ì•¼í•  ì²­í¬ ìˆ˜
        self.consecutive_speech_chunks = 0
        logging.info("âœ… Silero VAD ì´ˆê¸°í™” ì™„ë£Œ")

        # --- STT ì„¤ì • ---
        self.stt_client = speech.SpeechClient()
        self.stt_config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=SAMPLE_RATE,
            language_code="ko-KR",
            adaptation=adaptation_config
        )
        self.stt_streaming_config = speech.StreamingRecognitionConfig(
            config=self.stt_config,
            interim_results=True,
            single_utterance=True,
        )
        logging.info("âœ… Google STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _audio_callback(self, indata, frames, time_info, status):
        """ì‚¬ìš´ë“œë””ë°”ì´ìŠ¤ ì½œë°±. ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ íì— ì €ì¥."""
        if status:
            logging.warning(f"[ì˜¤ë””ì˜¤ ìƒíƒœ] {status}")

        try:
            self.audio_queue.put(indata.copy())
        except Exception as e:
            logging.debug(f"ì˜¤ë””ì˜¤ í ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    def _stt_audio_generator(self, stt_stop_flag=None, inactivity_stop_flag=None):
        """STT APIì— ì˜¤ë””ì˜¤ë¥¼ ê³µê¸‰í•˜ëŠ” ì œë„ˆë ˆì´í„°. ì‚¬ì „ ë²„í¼ -> ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìˆœìœ¼ë¡œ ê³µê¸‰."""
        # 1. VADê°€ ê°ì§€ë˜ê¸° ì „ê¹Œì§€ ìŒ“ì•„ë‘” ì‚¬ì „ ë²„í¼(pre-buffer) ì „ì†¡
        if self.stt_pre_buffer:
            combined_audio = np.concatenate(list(self.stt_pre_buffer))
            duration_sec = len(combined_audio) / SAMPLE_RATE
            yield speech.StreamingRecognizeRequest(audio_content=combined_audio.tobytes())
            logging.info(f"ì‚¬ì „ ë²„í¼ ({duration_sec:.2f}ì´ˆ) ì „ì†¡ ì™„ë£Œ")
            self.stt_pre_buffer.clear()

        # 2. ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì˜¤ë””ì˜¤ ì „ì†¡
        while not self.vad_active_flag.is_set():
            # íƒ€ì„ì•„ì›ƒ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
            if (stt_stop_flag and stt_stop_flag.is_set()) or (inactivity_stop_flag and inactivity_stop_flag.is_set()):
                logging.info("ì˜¤ë””ì˜¤ ìƒì„±ê¸° ì¤‘ë‹¨")
                break
                
            try:
                chunk = self.audio_queue.get(timeout=0.1)
                yield speech.StreamingRecognizeRequest(audio_content=chunk.tobytes())
            except queue.Empty:
                if self.vad_active_flag.is_set():
                    break
                continue

    def _run_stt(self):
        """ë‹¨ì¼ STT ì„¸ì…˜ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜. ì´ í•¨ìˆ˜ëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë¨."""
        
        FIRST_RESPONSE_TIMEOUT = 3.0  # ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        first_response_event = threading.Event()
        stt_stop_flag = threading.Event()
        
        INACTIVITY_TIMEOUT = 3.0
        inactivity_stop_flag = threading.Event()
        last_response_time = time.time()
        inactivity_thread = None

        def timeout_checker():
            """ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²´í¬í•˜ëŠ” í•¨ìˆ˜"""
            if not first_response_event.wait(timeout=FIRST_RESPONSE_TIMEOUT):
                logging.warning(f"STT ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ - ì„¸ì…˜ ì¢…ë£Œ")
                stt_stop_flag.set()

        def inactivity_timeout_checker():
            """STT ì‘ë‹µì´ ì—†ì„ ê²½ìš°(ë¹„í™œì„±) ì˜¤ë””ì˜¤ ì „ì†¡ì„ ì¤‘ë‹¨."""
            while not stt_stop_flag.is_set() and not inactivity_stop_flag.is_set():
                if time.time() - last_response_time > INACTIVITY_TIMEOUT:
                    logging.info(f"{INACTIVITY_TIMEOUT}ì´ˆ ë™ì•ˆ STT ì‘ë‹µì´ ì—†ì–´ ì˜¤ë””ì˜¤ ì „ì†¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    inactivity_stop_flag.set()
                    break
                time.sleep(0.1)
        
        # íƒ€ì„ì•„ì›ƒ ì²´ì»¤ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        timeout_thread = threading.Thread(target=timeout_checker, daemon=True)
        timeout_thread.start()
        
        try:
            responses = self.stt_client.streaming_recognize(self.stt_streaming_config, self._stt_audio_generator(stt_stop_flag, inactivity_stop_flag))
            
            for response in responses:
                # STT ì¤‘ë‹¨ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                if stt_stop_flag.is_set():
                    logging.info("íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ STT ì„¸ì…˜ ì¤‘ë‹¨")
                    return
                
                last_response_time = time.time()

                # ì²« ì‘ë‹µì´ ë„ì°©í–ˆìŒì„ ì•Œë¦¼
                if not first_response_event.is_set():
                    first_response_event.set()
                    
                    inactivity_thread = threading.Thread(target=inactivity_timeout_checker, daemon=True)
                    inactivity_thread.start()
                    
                    # c++ì— ì¸í„°ëŸ½ì…˜ ì‹ í˜¸ ì „ì†¡
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "user_interruption"})),
                        self.main_loop
                    )

                if not response.results or not response.results[0].alternatives:
                    continue

                result = response.results[0]
                transcript = result.alternatives[0].transcript

                if result.is_final:
                    final_text = result.alternatives[0].transcript.strip()
                    logging.info(f"âœ… STT ìµœì¢… ê²°ê³¼: '{final_text}'")
                    # STT ì™„ë£Œì‹œ ë©”ì¸ asyncio ë£¨í”„ë¡œ ê²°ê³¼ ì „ì†¡
                    if final_text:
                        self.main_loop.call_soon_threadsafe(self.stt_result_queue.put_nowait, final_text)
                    stt_completion_time = int(time.time() * 1000)
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "stt_done", "stt_done_time": stt_completion_time})),
                        self.main_loop
                    )
                    return
                else:
                    logging.info(f"âœ… STT ì¤‘ê°„ ê²°ê³¼: '{transcript}'")
                    
        except exceptions.DeadlineExceeded as e:
            logging.error(f"STT ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ(DeadlineExceeded): {e}")
        except Exception as e:
            logging.error(f"STT ì„¸ì…˜ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            first_response_event.set()
            inactivity_stop_flag.set()
            self.stt_pre_buffer.clear()
            self.vad_model.reset_states()
            self.vad_active_flag.set()
            logging.info("STT ì„¸ì…˜ ì¢…ë£Œ ë° VAD ê°ì§€ ì‹œì‘")

    def _process_audio_for_vad(self, audio_chunk_int16):
        # float32ë¡œ ë³€í™˜ (Silero VAD ìš”êµ¬ì‚¬í•­)
        audio_chunk_float32 = audio_chunk_int16.astype(np.float32) / 32768.0
        audio_tensor = torch.from_numpy(audio_chunk_float32.flatten())
        
        # ë²„í¼ì— í˜„ì¬ ì²­í¬ ì¶”ê°€
        self.vad_buffer = torch.cat([self.vad_buffer, audio_tensor])
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ VAD ì²˜ë¦¬
        while len(self.vad_buffer) >= self.VAD_CHUNK_SIZE:
            # ì²­í¬ í¬ê¸°ë§Œí¼ ì¶”ì¶œí•˜ì—¬ ì²˜ë¦¬
            vad_chunk = self.vad_buffer[:self.VAD_CHUNK_SIZE]
            self.vad_buffer = self.vad_buffer[self.VAD_CHUNK_SIZE:]
            
            # VAD ëª¨ë¸ë¡œ ìŒì„± í™•ë¥  ê³„ì‚°
            speech_prob = self.vad_model(vad_chunk, SAMPLE_RATE).item()
            
            # ì„ê³„ê°’ ì´ìƒì´ë©´ ì—°ì† ì¹´ìš´í„° ì¦ê°€, ì•„ë‹ˆë©´ ë¦¬ì…‹
            if speech_prob > self.VAD_THRESHOLD:
                self.consecutive_speech_chunks += 1
                logging.debug(f"VAD ìŒì„± ê°ì§€: {speech_prob:.2f}, ì—°ì† ì²­í¬: {self.consecutive_speech_chunks}")
            else:
                self.consecutive_speech_chunks = 0

    def start(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œì˜ ë©”ì¸ ë£¨í”„. ì´ í•¨ìˆ˜ê°€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë¨."""
        self.is_running.set()
        logging.info("ğŸ™ï¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘...")

        device_idx = find_input_device()
        if device_idx is None: return

        try:
            with sd.InputStream(
                samplerate=SAMPLE_RATE,
                channels=CHANNELS,
                dtype=AUDIO_DTYPE,
                callback=self._audio_callback,
                device=device_idx,
                blocksize=self.VAD_CHUNK_SIZE
            ):
                while self.is_running.is_set():
                    # STT ì‹¤í–‰ì¤‘ì¼ ê²½ìš° ëŒ€ê¸°
                    self.vad_active_flag.wait()
                    if not self.is_running.is_set(): break

                    try:
                        # ì²˜ë¦¬ ì „ í ì‚¬ì´ì¦ˆë¥¼ í™•ì¸í•˜ì—¬ ì²˜ë¦¬ê°€ ë°€ë¦¬ëŠ”ì§€ íŒŒì•…
                        queue_size = self.audio_queue.qsize()
                        if queue_size > 1:
                            logging.warning(f"ì˜¤ë””ì˜¤ íê°€ ë°€ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ í¬ê¸°: {queue_size}")
                        
                        audio_chunk_int16 = self.audio_queue.get(timeout=0.1)
                    except queue.Empty:
                        continue

                    # ì‚¬ì „ ë²„í¼ ì €ì¥
                    self.stt_pre_buffer.append(audio_chunk_int16)

                    # VAD ì²˜ë¦¬
                    self._process_audio_for_vad(audio_chunk_int16)

                    # ì—°ì†ì ìœ¼ë¡œ ìŒì„±ì´ ê°ì§€ë˜ë©´ STT ì„¸ì…˜ ì‹œì‘.
                    if self.consecutive_speech_chunks >= self.VAD_CONSECUTIVE_CHUNKS:
                        self.vad_active_flag.clear() # VAD ë£¨í”„ë¥¼ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜.
                        logging.info(f"ğŸ—£ï¸ ìŒì„± ì‹œì‘ ê°ì§€! STT ì‹œì‘.")
                        threading.Thread(target=self._run_stt).start()
                        
                        # STT ì‹œì‘ê³¼ í•¨ê»˜ VAD ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”.
                        self.vad_buffer = torch.tensor([])
                        self.consecutive_speech_chunks = 0
        except Exception as e:
            logging.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)

    def stop(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ."""
        self.is_running.clear()
        self.vad_active_flag.set() # ëŒ€ê¸° ìƒíƒœì˜ ìŠ¤ë ˆë“œê°€ ìˆë‹¤ë©´ ì¦‰ì‹œ ê¹¨ì›Œì„œ ì¢…ë£Œë˜ë„ë¡
        logging.info("ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡")


# ==================================================================================================
# TTS í•¸ë“¤ëŸ¬
# ==================================================================================================

async def handle_tts_stream(response_stream, client, websocket, conversation_log, responses_start_time=None):
    """Responses APIì˜ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ TTS ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜ í›„ ì „ì†¡"""
    await websocket.send(json.dumps({"type": "responses_stream_start"}))
    
    full_response_text = ""
    sentence_buffer = ""
    try:
        async for event in response_stream:
            if event.type == "response.output_text.delta":
                text_chunk = event.delta
                sentence_buffer += text_chunk
                full_response_text += text_chunk
                
                if any(p in sentence_buffer for p in ".?!"):
                    async with client.audio.speech.with_streaming_response.create(
                        model=TTS_MODEL,
                        voice=VOICE,
                        input=sentence_buffer,
                        response_format="pcm"
                    ) as tts_response:
                        async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                            await websocket.send(json.dumps({
                                "type": "responses_audio_chunk", 
                                "data": base64.b64encode(audio_chunk).decode('utf-8')
                            }))
                    sentence_buffer = ""
            
            if event.type == "response.completed":
                if responses_start_time is not None:
                    message = f"(ì†Œìš”ì‹œê°„: {time.time() - responses_start_time:.2f}ì´ˆ)"
                else:
                    message = ""
                logging.info(f"OpenAI ì‘ë‹µ ì™„ë£Œ: '{full_response_text}' {message}")

        if sentence_buffer.strip():
            async with client.audio.speech.with_streaming_response.create(
                model=TTS_MODEL,
                voice=VOICE,
                input=sentence_buffer,
                response_format="pcm"
            ) as tts_response:
                async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                    await websocket.send(json.dumps({
                        "type": "responses_audio_chunk", 
                        "data": base64.b64encode(audio_chunk).decode('utf-8')
                    }))

    except asyncio.CancelledError:
        logging.info("TTS ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise # ì¸í„°ëŸ½ì…˜ì„ ìƒìœ„ ë£¨í”„ì— ì „íŒŒ
    finally:
        await websocket.send(json.dumps({"type": "responses_stream_end"}))
        if full_response_text:
            conversation_log.append({"role": "assistant", "content": full_response_text})
            logging.info(f"OpenAI ì‘ë‹µ ì™„ë£Œ: '{full_response_text}'")

async def handle_tts_oneshot(response_text, client, websocket):
    """ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ í•œ ë²ˆì— TTS ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        await websocket.send(json.dumps({"type": "responses_stream_start"}))
        async with client.audio.speech.with_streaming_response.create(
            model=TTS_MODEL,
            voice=VOICE,
            input=response_text,
            response_format="pcm"
        ) as tts_response:
            async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                await websocket.send(json.dumps({
                    "type": "responses_audio_chunk", 
                    "data": base64.b64encode(audio_chunk).decode('utf-8')
                }))
        
    except asyncio.CancelledError:
        logging.info("TTS ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise
    finally:
        await websocket.send(json.dumps({"type": "responses_stream_end"}))


# ==================================================================================================
# Unified API Pipeline (Realtime + Responses)
# ==================================================================================================

async def run_realtime_task(websocket, openai_connection, conversation_log, realtime_finished_event: asyncio.Event, item_ids_to_manage: list, user_text):
    """(Task 1) Realtime APIë¥¼ í˜¸ì¶œí•˜ê³  ì˜¤ë””ì˜¤ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    logging.info("âš¡ï¸ Realtime Task ì‹œì‘...")
    
    try:
        # 1. ì´ì „ í„´ì—ì„œ ìƒì„±ëœ ëª¨ë“  ëŒ€í™” ì•„ì´í…œë“¤ì„ ì‚­ì œí•˜ì—¬ ì„¸ì…˜ì„ ì´ˆê¸°í™”
        if item_ids_to_manage:
            logging.info(f"ì´ì „ Realtime ëŒ€í™” ì•„ì´í…œ {len(item_ids_to_manage)}ê°œ ì‚­ì œ ì¤‘...")
            delete_tasks = [openai_connection.conversation.item.delete(item_id=item_id) for item_id in item_ids_to_manage]
            await asyncio.gather(*delete_tasks, return_exceptions=True) # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
            item_ids_to_manage.clear()
            logging.info("ì´ì „ ì•„ì´í…œ ì‚­ì œ ì™„ë£Œ.")

        # 2. í˜„ì¬ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ì•„ì´í…œë“¤ì„ ìƒì„±
        await openai_connection.session.update(session={"instructions": REALTIME_PROMPT, "voice": VOICE})

        await openai_connection.conversation.item.create(
            item={"type": "message", "role": "user", "content": [{"type": "input_text", "text": user_text}]}
        )

        # history_items = [entry for entry in conversation_log if entry['role'] != 'system']
        # for entry in history_items:
        #     item_to_create = {
        #         "type": "message",
        #         "role": entry['role'],
        #         "content": [{"type": "input_text" if entry['role'] == 'user' else "text", "text": entry['content']}]
        #     }
        #     await openai_connection.conversation.item.create(item=item_to_create)

        # 3. ì‘ë‹µ ìƒì„± ì‹œì‘
        realtime_start_time = time.time()
        await openai_connection.response.create()

        # 4. ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
        async for event in openai_connection:
            if event.type == "conversation.item.created":
                item_ids_to_manage.append(event.item.id)
                # await openai_connection.conversation.item.retrieve(item_id=event.previous_item_id)

            elif event.type == "response.audio.delta":
                await websocket.send(json.dumps({"type": "realtime_audio_chunk", "data": event.delta}))

            elif event.type == "response.created":
                await websocket.send(json.dumps({"type": "realtime_stream_start"}))

            elif event.type == "response.done":
                await websocket.send(json.dumps({"type": "realtime_stream_end"}))
                logging.info(f"âš¡ï¸ Realtime API ë‹µë³€ ìƒì„± ì™„ë£Œ: '{event.response.output[0].content[0].transcript}' (ì†Œìš”ì‹œê°„: {time.time() - realtime_start_time:.2f}ì´ˆ)")
                break

            elif event.type == "conversation.item.retrieved":
                logging.info(f"ì´ì „ ëŒ€í™” í•­ëª©ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤: {event.item}")

            elif event.type == "error":
                logging.error(f"Realtime API ì˜¤ë¥˜ ì´ë²¤íŠ¸: {event}")
    
    except asyncio.CancelledError:
        logging.info("âš¡ï¸ Realtime Taskê°€ ì™¸ë¶€ì—ì„œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"âš¡ï¸ Realtime Task ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        # íƒœìŠ¤í¬ê°€ ì •ìƒì ìœ¼ë¡œ ëë‚˜ë“ , ì·¨ì†Œë˜ë“  í•­ìƒ ì´ë²¤íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ Responses Taskì˜ ëŒ€ê¸°ë¥¼ í•´ì œ
        realtime_finished_event.set()
        logging.info("âš¡ï¸ Realtime Task ì¢…ë£Œ.")


async def run_responses_task(websocket, openai_client, conversation_log, realtime_finished_event: asyncio.Event):
    """(Task 2) Responses APIë¥¼ í˜¸ì¶œí•˜ê³ , Realtime ì‘ë‹µì´ ëë‚œ í›„ TTSë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    
    logging.info("ğŸ§  Responses Task ì‹œì‘...")
    response_text = ""

    try:
        responses_start_time = time.time()
        # 1. Responses APIë¡œë¶€í„° í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„±
        response = await openai_client.responses.create(
            model="gpt-4.1",
            input=conversation_log,
            # reasoning={ "effort": "low" },
            # text={ "verbosity": "low" },
            # stream=True
        )
        response_text = response.output_text
        logging.info(f"ğŸ§  Responses API ë‹µë³€ ìƒì„± ì™„ë£Œ: '{response_text}' (ì†Œìš”ì‹œê°„: {time.time() - responses_start_time:.2f}ì´ˆ)")

        # 2. TTS ìŠ¤íŠ¸ë¦¬ë°
        logging.info("...Realtime ì‘ë‹µ ì™„ë£Œ. Responses APIì˜ TTSë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        await handle_tts_oneshot(response_text, openai_client, websocket)

        # 3. ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        conversation_log.append({"role": "assistant", "content": response_text})

    except asyncio.CancelledError:
        logging.info("ğŸ§  Responses Taskê°€ ì™¸ë¶€ì—ì„œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"ğŸ§  Responses Task ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        logging.info("ğŸ§  Responses Task ì¢…ë£Œ.")


async def unified_active_pipeline(websocket, conversation_log):
    """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ Realtime APIì™€ Responses APIë¥¼ ë™ì‹œì— í˜¸ì¶œí•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸"""
    logging.info("ğŸ¤– Unified Active Pipeline ì‹œì‘...")
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    audio_processor, audio_thread = None, None
    active_response_tasks = []

    stt_result_queue = asyncio.Queue()
    main_loop = asyncio.get_running_loop()

    # Active ëª¨ë“œì— ì§„ì…í•  ë•Œ Realtime API ì—°ê²°ì„ í•œ ë²ˆë§Œ ìƒì„±
    async with openai_client.beta.realtime.connect(model="gpt-4o-mini-realtime-preview") as openai_connection:
        realtime_item_ids_to_manage = []
        try:
            # 1. ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° ì‹œì‘
            audio_processor = AudioProcessor(stt_result_queue, main_loop, websocket)
            audio_thread = threading.Thread(target=audio_processor.start, daemon=True)
            audio_thread.start()

            # 2. ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦¬ëŠ” ë©”ì¸ ë£¨í”„
            while True:
                user_text = await stt_result_queue.get()

                # 3. ìƒˆ ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´, ì´ì „ì˜ ëª¨ë“  AI ì‘ë‹µ íƒœìŠ¤í¬ë¥¼ ì¦‰ì‹œ ì¤‘ë‹¨
                if active_response_tasks:
                    logging.info(f"ì‚¬ìš©ì ì¸í„°ëŸ½ì…˜ ê°ì§€: '{user_text}'. ì´ì „ ì‘ë‹µ íƒœìŠ¤í¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    for task in active_response_tasks:
                        task.cancel()
                    # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì™„ì „íˆ ì·¨ì†Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
                    await asyncio.gather(*active_response_tasks, return_exceptions=True)
                    active_response_tasks = []

                # 4. ì¢…ë£Œ í‚¤ì›Œë“œ í™•ì¸
                if any(kw in user_text for kw in END_KEYWORDS):
                    await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(SLEEP_FILE)}))
                    logging.info(f"ì¢…ë£Œ í‚¤ì›Œë“œ ê°ì§€: '{user_text}' - ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break # Active Pipeline ì¢…ë£Œ

                # 5. ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
                conversation_log.append({"role": "user", "content": user_text})

                # 6. ë‘ API íƒœìŠ¤í¬ ê°„ì˜ ë™ê¸°í™”ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ìƒì„±
                realtime_finished_event = asyncio.Event()

                # 7. Realtime ë° Responses API íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ê³  ë™ì‹œì— ì‹¤í–‰
                realtime_task = asyncio.create_task(
                    run_realtime_task(websocket, openai_connection, conversation_log, realtime_finished_event, realtime_item_ids_to_manage, user_text)
                )
                # await websocket.send(json.dumps({"type": "responses_only"}))
                responses_task = asyncio.create_task(
                    run_responses_task(websocket, openai_client, conversation_log, realtime_finished_event)
                )
                active_response_tasks = [responses_task, realtime_task]

        except Exception as e:
            logging.error(f"Unified Active Pipelineì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        finally:
            # íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹œ ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if active_response_tasks:
                for task in active_response_tasks:
                    task.cancel()
                await asyncio.gather(*active_response_tasks, return_exceptions=True)

            # Active ëª¨ë“œ ì¢…ë£Œ ì‹œ ì„œë²„ì— ë‚¨ì•„ìˆëŠ” ì•„ì´í…œë“¤ì„ ëª¨ë‘ ì •ë¦¬
            if realtime_item_ids_to_manage:
                logging.info(f"Active ì„¸ì…˜ ì¢…ë£Œ. ë‚¨ì€ Realtime ì•„ì´í…œ {len(realtime_item_ids_to_manage)}ê°œ ì •ë¦¬ ì¤‘...")
                delete_tasks = [openai_connection.conversation.item.delete(item_id=item_id) for item_id in realtime_item_ids_to_manage]
                await asyncio.gather(*delete_tasks, return_exceptions=True)
                logging.info("ë‚¨ì€ ì•„ì´í…œ ì •ë¦¬ ì™„ë£Œ.")

            if audio_processor: audio_processor.stop()
            if audio_thread and audio_thread.is_alive(): audio_thread.join(timeout=1.0)
            logging.info("ğŸ¤– Unified Active Pipeline ì¢…ë£Œ.")


# ==================================================================================================
# Sleep ëª¨ë“œ
# ==================================================================================================

async def wakeword_detection_loop(websocket):
    """START_KEYWORDë¥¼ ê°ì§€í•  ë•Œê¹Œì§€ VAD-STT ë£¨í”„ë¥¼ ì‹¤í–‰ (Sleep ëª¨ë“œ)"""
    logging.info(f"ğŸ’¤ Sleep ëª¨ë“œ ì‹œì‘. '{START_KEYWORD}' í˜¸ì¶œ ëŒ€ê¸° ì¤‘...")
    audio_processor, audio_thread = None, None
    try:
        keyword_queue = asyncio.Queue()
        main_loop = asyncio.get_running_loop()

        # adaptation_client = speech.AdaptationClient()
        # parent = f"projects/{GOOGLE_CLOUD_PROJECT_ID}/locations/global"

        # phrase_set_response = adaptation_client.create_phrase_set(
        #     {
        #         "parent": parent,
        #         "phrase_set_id": "wakeup_keywords",
        #         "phrase_set": {
        #             "phrases": [{"value": START_KEYWORD}],
        #             "boost": 20.0  # boost ê°’ìœ¼ë¡œ ì¸ì‹ë¥  ê°€ì¤‘ì¹˜ ë¶€ì—¬
        #         }
        #     }
        # )
        # phrase_set_name = phrase_set_response.name

        # adaptation_config = speech.SpeechAdaptation(phrase_set_references=[phrase_set_name])

        audio_processor = AudioProcessor(keyword_queue, main_loop, websocket)
        audio_thread = threading.Thread(target=audio_processor.start, daemon=True)
        audio_thread.start()

        while True:
            stt_result = await keyword_queue.get()
            logging.info(f"[Sleep Mode] STT ê²°ê³¼: {stt_result}")
            if START_KEYWORD in stt_result:
                return
    finally:
        if audio_processor: audio_processor.stop()
        if audio_thread and audio_thread.is_alive(): audio_thread.join(timeout=1.0)
        logging.info("ğŸ’¤ Sleep ëª¨ë“œ ì¢…ë£Œ.")


# ==================================================================================================
# ë©”ì¸ ë£¨í”„
# ==================================================================================================

async def chat_handler(websocket):
    logging.info(f"âœ… C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨: {websocket.remote_address}")
    
    conversation_log = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    conversation_log.append({"role": "system", "content": SYSTEM_PROMPT})
    conversation_log.append({"role": "system", "content": "[start new chat]"})

    try:
        while True:
            # 1. Sleep ëª¨ë“œ: í‚¤ì›Œë“œ ê°ì§€ ëŒ€ê¸°
            await wakeword_detection_loop(websocket)

            # 2. Sleep ëª¨ë“œ ì¢…ë£Œ í›„ AWAKE ìŒì„± ì¬ìƒ
            await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(AWAKE_FILE)}))
            
            # 3. Active ëª¨ë“œ
            await unified_active_pipeline(websocket, conversation_log)
            
            logging.info("Active ì„¸ì…˜ ì¢…ë£Œ. ë‹¤ì‹œ Sleep ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

    except websockets.exceptions.ConnectionClosed:
        logging.warning(f"ğŸ”Œ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¨: {websocket.remote_address}")
    except Exception as e:
        logging.error(f"Chat í•¸ë“¤ëŸ¬ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
    finally:
        logging.info(f"ğŸ”Œ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•¸ë“¤ëŸ¬ ì¢…ë£Œ: {websocket.remote_address}")

async def main():
    logging.info("ğŸš€ ì„œë²„ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    server = await websockets.serve(chat_handler, "127.0.0.1", 5000)
    logging.info("ğŸš€ í†µí•© WebSocket ì„œë²„ê°€ 127.0.0.1:5000 ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    await server.wait_closed()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")