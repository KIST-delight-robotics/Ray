import os
import sys
import time
import json
import uuid
import queue
import asyncio
import logging
import threading
import math
import base64
from datetime import datetime
from collections import deque

import websockets
import sounddevice as sd
import torch
import numpy as np
from pathlib import Path
from openai import AsyncOpenAI
from google.cloud import speech
from google.api_core import exceptions

PROJECT_ROOT = Path(__file__).resolve().parent.parent
if PROJECT_ROOT not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from config.prompts import SYSTEM_PROMPT, REALTIME_PROMPT, SUMMARY_PROMPT_TEMPLATE

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%H:%M:%S')

# --- ê¸°ë³¸ ì„¤ì • ---
# OpenAI í‚¤ & Google Cloud ì¸ì¦íŒŒì¼ ê²½ë¡œ í™˜ê²½ë³€ìˆ˜ ë“±ë¡ í•„ìš”
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# --- ê²½ë¡œ ì„¤ì • ---
ASSETS_DIR = PROJECT_ROOT / "assets"
OUTPUT_DIR = PROJECT_ROOT / "output"
OUTPUT_AUDIO_DIR = OUTPUT_DIR / "audio"
OUTPUT_LOG_DIR = OUTPUT_DIR / "logs"
OUTPUT_AUDIO_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_LOG_DIR.mkdir(parents=True, exist_ok=True)

# ì¬ìƒìš© ì˜¤ë””ì˜¤ íŒŒì¼
AWAKE_FILE = ASSETS_DIR / "audio" / "awake.wav"
SLEEP_FILE = ASSETS_DIR / "audio" / "sleep.wav"

# --- ì˜¤ë””ì˜¤ ì„¤ì • ---
SAMPLE_RATE = 16000
CHANNELS = 1
AUDIO_DTYPE = "int16"

# --- OpenAI ì„¤ì • ---
VOICE = "coral"
TTS_MODEL = "gpt-4o-mini-tts"

# --- í‚¤ì›Œë“œ ì„¤ì • ---
START_KEYWORD = "ë ˆì´"
END_KEYWORDS = ["ì¢…ë£Œ", "ì‰¬ì–´"]

ACTIVE_SESSION_TIMEOUT = 120.0 # ì´ˆ ë‹¨ìœ„


def find_input_device():
    """ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ ê²€ìƒ‰"""
    try:
        devices = sd.query_devices()
        for idx, device in enumerate(devices):
            if device['max_input_channels'] > 0 and 'pipewire' in str(device['name']).lower():
                logging.info(f"ğŸ” ë°œê²¬ëœ ì…ë ¥ ì¥ì¹˜: [{idx}] {device['name']}")
                return idx
        logging.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        logging.error(f"ì¥ì¹˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# ==================================================================================================
# ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° (VAD & STT í†µí•©)
# ==================================================================================================

class AudioProcessor:
    """ë§ˆì´í¬ ì…ë ¥ë¶€í„° VAD, STTê¹Œì§€ ëª¨ë“  ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì „ë‹´í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, stt_result_queue: asyncio.Queue, main_loop: asyncio.AbstractEventLoop, websocket, adaptation_config=None):
        # --- ìƒíƒœ ë³€ìˆ˜ ---
        self.stt_result_queue = stt_result_queue
        self.main_loop = main_loop
        self.websocket = websocket
        self.is_running = threading.Event()
        self.vad_active_flag = threading.Event()
        self.vad_active_flag.set()

        # --- ì˜¤ë””ì˜¤ ë²„í¼ ---
        # ì›ë³¸ ì˜¤ë””ì˜¤ ë²„í¼
        self.audio_queue = queue.Queue()
        # STT ì‚¬ì „ ë²„í¼
        PRE_BUFFER_DURATION = 0.3 # ì‚¬ì „ ë²„í¼ ê¸¸ì´ (ì´ˆ)
        self.VAD_CHUNK_SIZE = 512
        pre_buffer_max_chunks = math.ceil(SAMPLE_RATE * PRE_BUFFER_DURATION / self.VAD_CHUNK_SIZE)
        self.stt_pre_buffer = deque(maxlen=pre_buffer_max_chunks)
        # VAD ì²˜ë¦¬ë¥¼ ìœ„í•œ ë²„í¼
        self.vad_buffer = torch.tensor([])

        # --- VAD ì„¤ì • ---
        model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=True)
        self.vad_model = model
        self.VAD_THRESHOLD = 0.5  # ìŒì„±ìœ¼ë¡œ íŒë‹¨í•  í™•ë¥  ì„ê³„ê°’
        self.VAD_CONSECUTIVE_CHUNKS = 3 # ì—°ì†ìœ¼ë¡œ ê°ì§€í•´ì•¼í•  ì²­í¬ ìˆ˜
        self.consecutive_speech_chunks = 0
        logging.info("âœ… Silero VAD ì´ˆê¸°í™” ì™„ë£Œ")

        # --- STT ì„¤ì • ---
        self.stt_client = speech.SpeechClient()
        self.stt_config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=SAMPLE_RATE,
            language_code="ko-KR",
            enable_automatic_punctuation=True,
            adaptation=adaptation_config
        )
        self.stt_streaming_config = speech.StreamingRecognitionConfig(
            config=self.stt_config,
            interim_results=True,
            single_utterance=True,
        )
        logging.info("âœ… Google STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _audio_callback(self, indata, frames, time_info, status):
        """ì‚¬ìš´ë“œë””ë°”ì´ìŠ¤ ì½œë°±. ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ íì— ì €ì¥."""
        if status:
            logging.warning(f"[ì˜¤ë””ì˜¤ ìƒíƒœ] {status}")

        try:
            self.audio_queue.put(indata.copy())
        except Exception as e:
            logging.debug(f"ì˜¤ë””ì˜¤ í ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    def _stt_audio_generator(self, stt_stop_flag=None, inactivity_stop_flag=None):
        """STT APIì— ì˜¤ë””ì˜¤ë¥¼ ê³µê¸‰í•˜ëŠ” ì œë„ˆë ˆì´í„°. ì‚¬ì „ ë²„í¼ -> ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìˆœìœ¼ë¡œ ê³µê¸‰."""
        # 1. VADê°€ ê°ì§€ë˜ê¸° ì „ê¹Œì§€ ìŒ“ì•„ë‘” ì‚¬ì „ ë²„í¼(pre-buffer) ì „ì†¡
        if self.stt_pre_buffer:
            combined_audio = np.concatenate(list(self.stt_pre_buffer))
            duration_sec = len(combined_audio) / SAMPLE_RATE
            yield speech.StreamingRecognizeRequest(audio_content=combined_audio.tobytes())
            logging.info(f"ì‚¬ì „ ë²„í¼ ({duration_sec:.2f}ì´ˆ) ì „ì†¡ ì™„ë£Œ")
            self.stt_pre_buffer.clear()

        # 2. ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì˜¤ë””ì˜¤ ì „ì†¡
        while not self.vad_active_flag.is_set():
            # íƒ€ì„ì•„ì›ƒ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
            if (stt_stop_flag and stt_stop_flag.is_set()) or (inactivity_stop_flag and inactivity_stop_flag.is_set()):
                logging.info("STT ì˜¤ë””ì˜¤ ìƒì„±ê¸° ì¤‘ë‹¨")
                break
                
            try:
                chunk = self.audio_queue.get(timeout=0.1)
                yield speech.StreamingRecognizeRequest(audio_content=chunk.tobytes())
            except queue.Empty:
                if self.vad_active_flag.is_set():
                    break
                continue

    def _run_stt(self):
        """ë‹¨ì¼ STT ì„¸ì…˜ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜. ì´ í•¨ìˆ˜ëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë¨."""
        
        FIRST_RESPONSE_TIMEOUT = 3.0  # ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        first_response_event = threading.Event()
        stt_stop_flag = threading.Event()
        
        INACTIVITY_TIMEOUT = 3.0
        inactivity_stop_flag = threading.Event()
        last_response_time = time.time()
        inactivity_thread = None

        def timeout_checker():
            """ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²´í¬í•˜ëŠ” í•¨ìˆ˜"""
            if not first_response_event.wait(timeout=FIRST_RESPONSE_TIMEOUT):
                logging.warning(f"STT ì²« ì‘ë‹µ íƒ€ì„ì•„ì›ƒ - ì„¸ì…˜ ì¢…ë£Œ")
                stt_stop_flag.set()

        def inactivity_timeout_checker():
            """STT ì‘ë‹µì´ ì—†ì„ ê²½ìš°(ë¹„í™œì„±) ì˜¤ë””ì˜¤ ì „ì†¡ì„ ì¤‘ë‹¨."""
            while not stt_stop_flag.is_set() and not inactivity_stop_flag.is_set():
                if time.time() - last_response_time > INACTIVITY_TIMEOUT:
                    logging.info(f"{INACTIVITY_TIMEOUT}ì´ˆ ë™ì•ˆ STT ì‘ë‹µì´ ì—†ì–´ ì˜¤ë””ì˜¤ ì „ì†¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    inactivity_stop_flag.set()
                    break
                time.sleep(0.1)
        
        # íƒ€ì„ì•„ì›ƒ ì²´ì»¤ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        timeout_thread = threading.Thread(target=timeout_checker, daemon=True)
        timeout_thread.start()
        
        try:
            responses = self.stt_client.streaming_recognize(self.stt_streaming_config, self._stt_audio_generator(stt_stop_flag, inactivity_stop_flag))
            
            for response in responses:
                # STT ì¤‘ë‹¨ ì‹ í˜¸ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                if stt_stop_flag.is_set():
                    logging.info("íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ STT ì„¸ì…˜ ì¤‘ë‹¨")
                    return
                
                last_response_time = time.time()

                # ì²« ì‘ë‹µì´ ë„ì°©í–ˆìŒì„ ì•Œë¦¼
                if not first_response_event.is_set():
                    first_response_event.set()
                    
                    inactivity_thread = threading.Thread(target=inactivity_timeout_checker, daemon=True)
                    inactivity_thread.start()
                    
                    # c++ì— ì¸í„°ëŸ½ì…˜ ì‹ í˜¸ ì „ì†¡
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "user_interruption"})),
                        self.main_loop
                    )

                if not response.results or not response.results[0].alternatives:
                    continue

                result = response.results[0]
                transcript = result.alternatives[0].transcript

                if result.is_final:
                    final_text = result.alternatives[0].transcript.strip()
                    logging.info(f"âœ… STT ìµœì¢… ê²°ê³¼: '{final_text}'")
                    # STT ì™„ë£Œì‹œ ë©”ì¸ asyncio ë£¨í”„ë¡œ ê²°ê³¼ ì „ì†¡
                    if final_text:
                        self.main_loop.call_soon_threadsafe(self.stt_result_queue.put_nowait, final_text)
                    stt_completion_time = int(time.time() * 1000)
                    asyncio.run_coroutine_threadsafe(
                        self.websocket.send(json.dumps({"type": "stt_done", "stt_done_time": stt_completion_time})),
                        self.main_loop
                    )
                    return
                else:
                    logging.info(f"âœ… STT ì¤‘ê°„ ê²°ê³¼: '{transcript}'")
                    
        except exceptions.DeadlineExceeded as e:
            logging.error(f"STT ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ(DeadlineExceeded): {e}")
        except Exception as e:
            logging.error(f"STT ì„¸ì…˜ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            first_response_event.set()
            inactivity_stop_flag.set()
            self.stt_pre_buffer.clear()
            self.vad_model.reset_states()
            self.vad_active_flag.set()
            logging.info("STT ì„¸ì…˜ ì¢…ë£Œ ë° VAD ê°ì§€ ì‹œì‘")

    def _process_audio_for_vad(self, audio_chunk_int16):
        # float32ë¡œ ë³€í™˜ (Silero VAD ìš”êµ¬ì‚¬í•­)
        audio_chunk_float32 = audio_chunk_int16.astype(np.float32) / 32768.0
        audio_tensor = torch.from_numpy(audio_chunk_float32.flatten())
        
        # ë²„í¼ì— í˜„ì¬ ì²­í¬ ì¶”ê°€
        self.vad_buffer = torch.cat([self.vad_buffer, audio_tensor])
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ VAD ì²˜ë¦¬
        while len(self.vad_buffer) >= self.VAD_CHUNK_SIZE:
            # ì²­í¬ í¬ê¸°ë§Œí¼ ì¶”ì¶œí•˜ì—¬ ì²˜ë¦¬
            vad_chunk = self.vad_buffer[:self.VAD_CHUNK_SIZE]
            self.vad_buffer = self.vad_buffer[self.VAD_CHUNK_SIZE:]
            
            # VAD ëª¨ë¸ë¡œ ìŒì„± í™•ë¥  ê³„ì‚°
            speech_prob = self.vad_model(vad_chunk, SAMPLE_RATE).item()
            
            # ì„ê³„ê°’ ì´ìƒì´ë©´ ì—°ì† ì¹´ìš´í„° ì¦ê°€, ì•„ë‹ˆë©´ ë¦¬ì…‹
            if speech_prob > self.VAD_THRESHOLD:
                self.consecutive_speech_chunks += 1
                logging.debug(f"VAD ìŒì„± ê°ì§€: {speech_prob:.2f}, ì—°ì† ì²­í¬: {self.consecutive_speech_chunks}")
            else:
                self.consecutive_speech_chunks = 0

    def start(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œì˜ ë©”ì¸ ë£¨í”„. ì´ í•¨ìˆ˜ê°€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë¨."""
        self.is_running.set()
        logging.info("ğŸ™ï¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘...")

        device_idx = find_input_device()
        if device_idx is None: return

        try:
            with sd.InputStream(
                samplerate=SAMPLE_RATE,
                channels=CHANNELS,
                dtype=AUDIO_DTYPE,
                callback=self._audio_callback,
                device=device_idx,
                blocksize=self.VAD_CHUNK_SIZE
            ):
                while self.is_running.is_set():
                    # STT ì‹¤í–‰ì¤‘ì¼ ê²½ìš° ëŒ€ê¸°
                    self.vad_active_flag.wait()
                    if not self.is_running.is_set(): break

                    try:
                        # ì²˜ë¦¬ ì „ í ì‚¬ì´ì¦ˆë¥¼ í™•ì¸í•˜ì—¬ ì²˜ë¦¬ê°€ ë°€ë¦¬ëŠ”ì§€ íŒŒì•…
                        queue_size = self.audio_queue.qsize()
                        if queue_size > 1:
                            logging.warning(f"ì˜¤ë””ì˜¤ íê°€ ë°€ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ í¬ê¸°: {queue_size}")
                        
                        audio_chunk_int16 = self.audio_queue.get(timeout=0.1)
                    except queue.Empty:
                        continue

                    # ì‚¬ì „ ë²„í¼ ì €ì¥
                    self.stt_pre_buffer.append(audio_chunk_int16)

                    # VAD ì²˜ë¦¬
                    self._process_audio_for_vad(audio_chunk_int16)

                    # ì—°ì†ì ìœ¼ë¡œ ìŒì„±ì´ ê°ì§€ë˜ë©´ STT ì„¸ì…˜ ì‹œì‘.
                    if self.consecutive_speech_chunks >= self.VAD_CONSECUTIVE_CHUNKS:
                        self.vad_active_flag.clear() # VAD ë£¨í”„ë¥¼ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜.
                        logging.info(f"ğŸ—£ï¸ ìŒì„± ì‹œì‘ ê°ì§€! STT ì‹œì‘.")
                        threading.Thread(target=self._run_stt).start()
                        
                        # STT ì‹œì‘ê³¼ í•¨ê»˜ VAD ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”.
                        self.vad_buffer = torch.tensor([])
                        self.consecutive_speech_chunks = 0
        except Exception as e:
            logging.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë£¨í”„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)

    def stop(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ."""
        self.is_running.clear()
        self.vad_active_flag.set() # ëŒ€ê¸° ìƒíƒœì˜ ìŠ¤ë ˆë“œê°€ ìˆë‹¤ë©´ ì¦‰ì‹œ ê¹¨ì›Œì„œ ì¢…ë£Œë˜ë„ë¡
        logging.info("ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡")


# ==================================================================================================
# TTS í•¸ë“¤ëŸ¬
# ==================================================================================================

async def handle_tts_stream(response_stream, client, websocket, conversation_log, responses_start_time=None):
    """Responses APIì˜ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ TTS ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜ í›„ ì „ì†¡"""
    await websocket.send(json.dumps({"type": "responses_stream_start"}))
    
    full_response_text = ""
    sentence_buffer = ""
    try:
        async for event in response_stream:
            if event.type == "response.output_text.delta":
                text_chunk = event.delta
                sentence_buffer += text_chunk
                full_response_text += text_chunk
                
                if any(p in sentence_buffer for p in ".?!"):
                    async with client.audio.speech.with_streaming_response.create(
                        model=TTS_MODEL,
                        voice=VOICE,
                        input=sentence_buffer,
                        response_format="pcm"
                    ) as tts_response:
                        async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                            await websocket.send(json.dumps({
                                "type": "responses_audio_chunk", 
                                "data": base64.b64encode(audio_chunk).decode('utf-8')
                            }))
                    sentence_buffer = ""
            
            if event.type == "response.completed":
                if responses_start_time is not None:
                    message = f"(ì†Œìš”ì‹œê°„: {time.time() - responses_start_time:.2f}ì´ˆ)"
                else:
                    message = ""
                logging.info(f"OpenAI ì‘ë‹µ ì™„ë£Œ: '{full_response_text}' {message}")

        if sentence_buffer.strip():
            async with client.audio.speech.with_streaming_response.create(
                model=TTS_MODEL,
                voice=VOICE,
                input=sentence_buffer,
                response_format="pcm"
            ) as tts_response:
                async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                    await websocket.send(json.dumps({
                        "type": "responses_audio_chunk", 
                        "data": base64.b64encode(audio_chunk).decode('utf-8')
                    }))

    except asyncio.CancelledError:
        logging.info("TTS ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise # ì¸í„°ëŸ½ì…˜ì„ ìƒìœ„ ë£¨í”„ì— ì „íŒŒ
    finally:
        await websocket.send(json.dumps({"type": "responses_stream_end"}))
        if full_response_text:
            conversation_log.append({"role": "assistant", "content": full_response_text})
            logging.info(f"OpenAI ì‘ë‹µ ì™„ë£Œ: '{full_response_text}'")

async def handle_tts_oneshot(response_text, client, websocket):
    """ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ í•œ ë²ˆì— TTS ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        await websocket.send(json.dumps({"type": "responses_stream_start"}))
        async with client.audio.speech.with_streaming_response.create(
            model=TTS_MODEL,
            voice=VOICE,
            input=response_text,
            response_format="pcm"
        ) as tts_response:
            async for audio_chunk in tts_response.iter_bytes(chunk_size=4096):
                await websocket.send(json.dumps({
                    "type": "responses_audio_chunk", 
                    "data": base64.b64encode(audio_chunk).decode('utf-8')
                }))
        
    except asyncio.CancelledError:
        logging.info("TTS ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        raise
    finally:
        await websocket.send(json.dumps({"type": "responses_stream_end"}))


# ==================================================================================================
# ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
# ==================================================================================================

class ConversationManager:
    def __init__(self, client: AsyncOpenAI):
        self.client = client
        self.session_id = None
        self.session_start_time = None
        self.current_conversation_log = []

    def start_new_session(self):
        self.session_id = str(uuid.uuid4())
        self.session_start_time = datetime.now()
        self.current_conversation_log = self._create_initial_context(SYSTEM_PROMPT, num_recent=2)

    def add_message(self, role: str, content: str):
        message = {"role": role, "content": content}
        self.current_conversation_log.append(message)

    def get_current_log(self) -> list:
        return self.current_conversation_log
    
    async def end_session(self):
        if len(self.current_conversation_log) < 2:
            logging.info("ì €ì¥í•  ëŒ€í™” ê¸°ë¡ì´ ì—†ì–´ ì„¸ì…˜ ì¢…ë£Œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # 1. ìš”ì•½ ìƒì„±
        summary = await self._summarize_session()

        # 2. ë°ì´í„° êµ¬ì¡°í™”
        session_data = {
            "session_id": self.session_id,
            "start_time": self.session_start_time.isoformat(),
            "end_time": datetime.now().isoformat(),
            "summary": summary,
            "full_log": self.current_conversation_log
        }

        # 3. íŒŒì¼ë¡œ ì €ì¥ (íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë„£ì–´ ì •ë ¬í•˜ê¸° ì‰½ê²Œ)
        timestamp = self.session_start_time.strftime("%Y%m%d_%H%M%S")
        filepath = OUTPUT_LOG_DIR / f"{timestamp}.json"

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            logging.info(f"ğŸ“‹ ì„¸ì…˜ ê¸°ë¡ ì €ì¥ ì™„ë£Œ: {filepath}")
        except Exception as e:
            logging.error(f"ğŸ“‹ ì„¸ì…˜ ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # í˜„ì¬ ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™”
        self.session_id = None
        self.session_start_time = None
        self.current_conversation_log = []
    
    async def _summarize_session(self) -> str:
        """OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ í˜„ì¬ ì„¸ì…˜ì˜ ëŒ€í™”ë¥¼ ìš”ì•½."""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ìš”ì•½ì— í¬í•¨í•˜ì§€ ì•ŠìŒ.
        log_for_summary = [msg for msg in self.current_conversation_log if msg.get("role") != "system"]
        
        if not log_for_summary:
            return "ìš”ì•½í•  ë‚´ìš© ì—†ìŒ."
        
        # ëŒ€í™” ê¸°ë¡ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        conversation_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in log_for_summary])

        prompt = SUMMARY_PROMPT_TEMPLATE.format(conversation_text=conversation_text)

        try:
            logging.info("ğŸ“‹ ì„¸ì…˜ ìš”ì•½ API í˜¸ì¶œ...")
            response = await self.client.responses.create(
                model="gpt-5-mini",
                input=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            summary = response.output_text
            logging.info(f"ğŸ“‹ ì„¸ì…˜ ìš”ì•½ ì™„ë£Œ:\n{summary}")
            return summary
        except Exception as e:
            logging.error(f"ğŸ“‹ ì„¸ì…˜ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ìš”ì•½ ìƒì„± ì‹¤íŒ¨."
    
    def _create_initial_context(self, system_prompt: str, num_recent: int = 2) -> list:
        """ìƒˆ ì„¸ì…˜ ì‹œì‘ ì‹œ ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±."""
        initial_context = []
        
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        initial_context.append({"role": "system", "content": system_prompt})
        
        # 2. ìµœê·¼ ì„¸ì…˜ ìš”ì•½ë³¸ ë¡œë“œ
        recent_summaries = self._load_recent_summaries_from_files(num_recent)
        if recent_summaries:
            summary_text = "\n\n---\n\n".join(recent_summaries)
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ê³¼ê±° ìš”ì•½ ì •ë³´ë¥¼ ì œê³µ
            initial_context.append({
                "role": "system",
                "content": f"## ì°¸ê³ : ê³¼ê±° ëŒ€í™” ìš”ì•½\n{summary_text}"
            })
            logging.info(f" ìµœê·¼ ëŒ€í™” ìš”ì•½ {len(recent_summaries)}ê°œë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

        initial_context.append({"role": "system", "content": "[ìƒˆ ëŒ€í™” ì‹œì‘]"})
        return initial_context

    def _load_recent_summaries_from_files(self, num_to_load: int) -> list:
        """ë¡œê·¸ í´ë”ì—ì„œ ê°€ì¥ ìµœê·¼ì˜ ìš”ì•½ íŒŒì¼ì„ ì°¾ì•„ ë‚´ìš©ì„ ë°˜í™˜."""
        try:
            # íŒŒì¼ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
            history_files = sorted(OUTPUT_LOG_DIR.glob("*.json"), key=os.path.getmtime, reverse=True)
            
            summaries = []
            for filepath in history_files[:num_to_load]:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ìš”ì•½ë³¸ ì•ì— ì‹œê°„ ì •ë³´ë¥¼ ë¶™ì—¬ì£¼ë©´ LLMì´ ì‹œê°„ ìˆœì„œë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë¨
                    summary_with_time = f"[{data.get('start_time')[:10]}]\n{data.get('summary', '')}"
                    summaries.append(summary_with_time)
            
            return summaries
        except Exception as e:
            logging.error(f"ìµœê·¼ ìš”ì•½ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

# ==================================================================================================
# Unified API Pipeline (Realtime + Responses)
# ==================================================================================================

async def run_realtime_task(websocket, realtime_connection, conversation_log, realtime_finished_event: asyncio.Event, item_ids_to_manage: list, user_text):
    """(Task 1) Realtime APIë¥¼ í˜¸ì¶œí•˜ê³  ì˜¤ë””ì˜¤ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    logging.info("âš¡ï¸ Realtime Task ì‹œì‘...")
    
    try:
        # 1. ì´ì „ í„´ì—ì„œ ìƒì„±ëœ ëª¨ë“  ëŒ€í™” ì•„ì´í…œë“¤ì„ ì‚­ì œí•˜ì—¬ ì„¸ì…˜ì„ ì´ˆê¸°í™”
        if item_ids_to_manage:
            logging.info(f"ì´ì „ Realtime ëŒ€í™” ì•„ì´í…œ {len(item_ids_to_manage)}ê°œ ì‚­ì œ ì¤‘...")
            delete_tasks = [realtime_connection.conversation.item.delete(item_id=item_id) for item_id in item_ids_to_manage]
            await asyncio.gather(*delete_tasks, return_exceptions=True) # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
            item_ids_to_manage.clear()
            logging.info("ì´ì „ ì•„ì´í…œ ì‚­ì œ ì™„ë£Œ.")

        # 2. í˜„ì¬ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ì•„ì´í…œë“¤ì„ ìƒì„±
        await realtime_connection.session.update(session={"instructions": REALTIME_PROMPT, "voice": VOICE})

        await realtime_connection.conversation.item.create(
            item={"type": "message", "role": "user", "content": [{"type": "input_text", "text": user_text}]}
        )

        # history_items = [entry for entry in conversation_log if entry['role'] != 'system']
        # for entry in history_items:
        #     item_to_create = {
        #         "type": "message",
        #         "role": entry['role'],
        #         "content": [{"type": "input_text" if entry['role'] == 'user' else "text", "text": entry['content']}]
        #     }
        #     await realtime_connection.conversation.item.create(item=item_to_create)

        # 3. ì‘ë‹µ ìƒì„± ì‹œì‘
        realtime_start_time = time.time()
        await realtime_connection.response.create()

        # 4. ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
        async for event in realtime_connection:
            if event.type == "conversation.item.created":
                item_ids_to_manage.append(event.item.id)
                # await realtime_connection.conversation.item.retrieve(item_id=event.previous_item_id)

            elif event.type == "response.audio.delta":
                await websocket.send(json.dumps({"type": "realtime_audio_chunk", "data": event.delta}))

            elif event.type == "response.created":
                await websocket.send(json.dumps({"type": "realtime_stream_start"}))

            elif event.type == "response.done":
                await websocket.send(json.dumps({"type": "realtime_stream_end"}))
                logging.info(f"âš¡ï¸ Realtime API ë‹µë³€ ìƒì„± ì™„ë£Œ: '{event.response.output[0].content[0].transcript}' (ì†Œìš”ì‹œê°„: {time.time() - realtime_start_time:.2f}ì´ˆ)")
                break

            elif event.type == "conversation.item.retrieved":
                logging.info(f"ì´ì „ ëŒ€í™” í•­ëª©ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤: {event.item}")

            elif event.type == "error":
                logging.error(f"Realtime API ì˜¤ë¥˜ ì´ë²¤íŠ¸: {event}")
    
    except asyncio.CancelledError:
        logging.info("âš¡ï¸ Realtime Taskê°€ ì™¸ë¶€ì—ì„œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"âš¡ï¸ Realtime Task ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        # íƒœìŠ¤í¬ê°€ ì •ìƒì ìœ¼ë¡œ ëë‚˜ë“ , ì·¨ì†Œë˜ë“  í•­ìƒ ì´ë²¤íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ Responses Taskì˜ ëŒ€ê¸°ë¥¼ í•´ì œ
        realtime_finished_event.set()
        logging.info("âš¡ï¸ Realtime Task ì¢…ë£Œ.")


async def run_responses_task(websocket, openai_client, manager: ConversationManager, realtime_finished_event: asyncio.Event):
    """(Task 2) Responses APIë¥¼ í˜¸ì¶œí•˜ê³ , Realtime ì‘ë‹µì´ ëë‚œ í›„ TTSë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    
    logging.info("ğŸ§  Responses Task ì‹œì‘...")
    responses_start_time = time.time()
    response_text = ""
    current_log = manager.get_current_log()

    try:
        # 1. Responses APIë¡œë¶€í„° í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„±
        response = await openai_client.responses.create(
            model="gpt-4.1",
            input=current_log,
            # reasoning={ "effort": "low" },
            # text={ "verbosity": "low" },
            # stream=True
        )
        response_text = response.output_text
        logging.info(f"ğŸ§  Responses API ë‹µë³€ ìƒì„± ì™„ë£Œ: '{response_text}' (ì†Œìš”ì‹œê°„: {time.time() - responses_start_time:.2f}ì´ˆ)")

        # 2. TTS ìŠ¤íŠ¸ë¦¬ë°
        logging.info("...Realtime ì‘ë‹µ ì™„ë£Œ. Responses APIì˜ TTSë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        await handle_tts_oneshot(response_text, openai_client, websocket)

        # 3. ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        manager.add_message("assistant", response_text)

    except asyncio.CancelledError:
        logging.info("ğŸ§  Responses Taskê°€ ì™¸ë¶€ì—ì„œ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"ğŸ§  Responses Task ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        logging.info("ğŸ§  Responses Task ì¢…ë£Œ.")


async def unified_active_pipeline(websocket, openai_client, manager: ConversationManager):
    """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ Realtime APIì™€ Responses APIë¥¼ ë™ì‹œì— í˜¸ì¶œí•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸"""
    logging.info("ğŸ¤– Unified Active Pipeline ì‹œì‘...")
    audio_processor, audio_thread = None, None
    active_response_tasks = []

    stt_result_queue = asyncio.Queue()
    main_loop = asyncio.get_running_loop()

    # Active ëª¨ë“œì— ì§„ì…í•  ë•Œ Realtime API ì—°ê²°ì„ í•œ ë²ˆë§Œ ìƒì„±
    async with openai_client.beta.realtime.connect(model="gpt-4o-mini-realtime-preview") as realtime_connection:
        realtime_item_ids_to_manage = []
        try:
            # 1. ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° ì‹œì‘
            audio_processor = AudioProcessor(stt_result_queue, main_loop, websocket)
            audio_thread = threading.Thread(target=audio_processor.start, daemon=True)
            audio_thread.start()

            # 2. ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦¬ëŠ” ë©”ì¸ ë£¨í”„
            while True:
                try:
                    user_text = await asyncio.wait_for(stt_result_queue.get(), timeout=ACTIVE_SESSION_TIMEOUT)

                    # 3. ìƒˆ ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´, ì´ì „ì˜ ëª¨ë“  AI ì‘ë‹µ íƒœìŠ¤í¬ë¥¼ ì¦‰ì‹œ ì¤‘ë‹¨
                    if active_response_tasks:
                        logging.info(f"ì‚¬ìš©ì ì¸í„°ëŸ½ì…˜ ê°ì§€: '{user_text}'. ì´ì „ ì‘ë‹µ íƒœìŠ¤í¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        for task in active_response_tasks: task.cancel()
                        await asyncio.gather(*active_response_tasks, return_exceptions=True)
                        active_response_tasks = []

                    # 4. ì¢…ë£Œ í‚¤ì›Œë“œ í™•ì¸
                    if any(kw in user_text for kw in END_KEYWORDS):
                        await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(SLEEP_FILE)}))
                        logging.info(f"ì¢…ë£Œ í‚¤ì›Œë“œ ê°ì§€: '{user_text}' - ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break

                    # 5. ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
                    manager.add_message("user", user_text)

                    # 6. ë‘ API íƒœìŠ¤í¬ ê°„ì˜ ë™ê¸°í™”ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ìƒì„±
                    realtime_finished_event = asyncio.Event()

                    # 7. Realtime ë° Responses API íƒœìŠ¤í¬ë¥¼ ìƒì„±í•˜ê³  ë™ì‹œì— ì‹¤í–‰
                    # await websocket.send(json.dumps({"type": "responses_only"}))
                    realtime_task = asyncio.create_task(
                        run_realtime_task(websocket, realtime_connection, manager, realtime_finished_event, realtime_item_ids_to_manage, user_text)
                    )
                    responses_task = asyncio.create_task(
                        run_responses_task(websocket, openai_client, manager, realtime_finished_event)
                    )
                    active_response_tasks = [responses_task, realtime_task]
                    
                except asyncio.TimeoutError:
                    logging.info(f"â° {ACTIVE_SESSION_TIMEOUT}ì´ˆ ë™ì•ˆ ì…ë ¥ì´ ì—†ì–´ Active ì„¸ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(SLEEP_FILE)}))
                    break

        except Exception as e:
            logging.error(f"Unified Active Pipelineì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        finally:
            # íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹œ ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if active_response_tasks:
                for task in active_response_tasks: task.cancel()
                await asyncio.gather(*active_response_tasks, return_exceptions=True)
                active_response_tasks = []

            if audio_processor: audio_processor.stop()
            if audio_thread and audio_thread.is_alive(): audio_thread.join(timeout=1.0)
            logging.info("ğŸ¤– Unified Active Pipeline ì¢…ë£Œ.")


# ==================================================================================================
# Sleep ëª¨ë“œ
# ==================================================================================================

async def wakeword_detection_loop(websocket):
    """START_KEYWORDë¥¼ ê°ì§€í•  ë•Œê¹Œì§€ VAD-STT ë£¨í”„ë¥¼ ì‹¤í–‰ (Sleep ëª¨ë“œ)"""
    logging.info(f"ğŸ’¤ Sleep ëª¨ë“œ ì‹œì‘. '{START_KEYWORD}' í˜¸ì¶œ ëŒ€ê¸° ì¤‘...")
    audio_processor, audio_thread = None, None
    try:
        keyword_queue = asyncio.Queue()
        main_loop = asyncio.get_running_loop()

        # adaptation_client = speech.AdaptationClient()
        # parent = f"projects/{GOOGLE_CLOUD_PROJECT_ID}/locations/global"

        # phrase_set_response = adaptation_client.create_phrase_set(
        #     {
        #         "parent": parent,
        #         "phrase_set_id": "wakeup_keywords",
        #         "phrase_set": {
        #             "phrases": [{"value": START_KEYWORD}],
        #             "boost": 20.0  # boost ê°’ìœ¼ë¡œ ì¸ì‹ë¥  ê°€ì¤‘ì¹˜ ë¶€ì—¬
        #         }
        #     }
        # )
        # phrase_set_name = phrase_set_response.name

        # adaptation_config = speech.SpeechAdaptation(phrase_set_references=[phrase_set_name])

        audio_processor = AudioProcessor(keyword_queue, main_loop, websocket)
        audio_thread = threading.Thread(target=audio_processor.start, daemon=True)
        audio_thread.start()

        while True:
            stt_result = await keyword_queue.get()
            logging.info(f"[Sleep Mode] STT ê²°ê³¼: {stt_result}")
            if START_KEYWORD in stt_result:
                return
    finally:
        if audio_processor: audio_processor.stop()
        if audio_thread and audio_thread.is_alive(): audio_thread.join(timeout=1.0)
        logging.info("ğŸ’¤ Sleep ëª¨ë“œ ì¢…ë£Œ.")


# ==================================================================================================
# ë©”ì¸ ë£¨í”„
# ==================================================================================================

async def chat_handler(websocket):
    logging.info(f"âœ… C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨: {websocket.remote_address}")

    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    conversation_manager = ConversationManager(client=openai_client)

    try:
        while True:
            # 1. Sleep ëª¨ë“œ: í‚¤ì›Œë“œ ê°ì§€ ëŒ€ê¸°
            await wakeword_detection_loop(websocket)

            # 2. Sleep ëª¨ë“œ ì¢…ë£Œ í›„ AWAKE ìŒì„± ì¬ìƒ
            await websocket.send(json.dumps({"type": "play_audio", "file_to_play": str(AWAKE_FILE)}))
            
            # 3. ìƒˆ ì„¸ì…˜ ì‹œì‘
            conversation_manager.start_new_session()
            
            # 4. Active ëª¨ë“œ ì‹¤í–‰
            await unified_active_pipeline(websocket, openai_client, conversation_manager)

            # 5. Active ëª¨ë“œ ì¢…ë£Œ í›„ ì„¸ì…˜ ì •ë¦¬
            await conversation_manager.end_session()

            logging.info("Active ì„¸ì…˜ ì¢…ë£Œ. ë‹¤ì‹œ Sleep ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

    except websockets.exceptions.ConnectionClosed:
        logging.warning(f"ğŸ”Œ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¨: {websocket.remote_address}")
    except Exception as e:
        logging.error(f"Chat í•¸ë“¤ëŸ¬ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
    finally:
        logging.info(f"ğŸ”Œ C++ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•¸ë“¤ëŸ¬ ì¢…ë£Œ: {websocket.remote_address}")

async def main():
    logging.info("ğŸš€ ì„œë²„ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    server = await websockets.serve(chat_handler, "127.0.0.1", 5000)
    logging.info("ğŸš€ í†µí•© WebSocket ì„œë²„ê°€ 127.0.0.1:5000 ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    await server.wait_closed()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")