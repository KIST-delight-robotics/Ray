"""
Cine21 ì›¹ í¬ë¡¤ëŸ¬ (Selenium ë²„ì „)

ì˜í™”ì½ê¸°, ì˜í™”ë¹„í‰, ì”¨ë„¤21 ë¦¬ë·° ì„¹ì…˜ì˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê° ì¹´í…Œê³ ë¦¬ 2í˜ì´ì§€ì”©)
    python cine21_crawler.py --test-mode
    
    # ì „ì²´ í¬ë¡¤ë§
    python cine21_crawler.py
    
    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ
    python cine21_crawler.py --category review
    
    # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
    python cine21_crawler.py --reset
"""

import json
import os
import re
import sys
import ssl
import time
import argparse
import urllib3
from datetime import datetime
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from bs4 import BeautifulSoup

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
except ImportError:
    print("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    print("  pip install selenium beautifulsoup4 lxml requests")
    sys.exit(1)

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings()
# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (ê¸°ì—… í™˜ê²½ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆìŒ)
ssl._create_default_https_context = ssl._create_unverified_context


# ============================================================================
# ì„¤ì •
# ============================================================================

BASE_URL = "https://cine21.com"

CATEGORIES = {
    "reading": {
        "name": "ì˜í™”ì½ê¸°",
        "section": "005004001",
        "max_pages": 78,
        "category_type": "critique"
    },
    "critique": {
        "name": "ì˜í™”ë¹„í‰",
        "section": "005004016",
        "max_pages": 76,
        "category_type": "critique"
    },
    "review": {
        "name": "ì”¨ë„¤21 ë¦¬ë·°",
        "section": "002001001",
        "max_pages": 974,
        "category_type": "review"
    }
}

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
MAX_CONCURRENT_REQUESTS = 5  # ë™ì‹œ ìš”ì²­ ìˆ˜
REQUEST_DELAY = 1.0  # ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
PAGE_LOAD_TIMEOUT = 20  # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "crawled"
PROGRESS_FILE = DATA_DIR / "progress.json"

# HTTP í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}


# ============================================================================
# ë°ì´í„° í´ë˜ìŠ¤
# ============================================================================

@dataclass
class Article:
    mag_id: str
    title: str
    author: str
    author_id: str
    date: str
    section: str
    magazine_issue: str
    category: str
    category_name: str
    content: str
    related_articles: list
    related_movies: list  # ê´€ë ¨ ì˜í™” ì •ë³´ [{movie_id, title, year}, ...]
    url: str
    crawled_at: str


# ============================================================================
# ì§„í–‰ ìƒíƒœ ê´€ë¦¬
# ============================================================================

def load_progress() -> dict:
    """ì €ì¥ëœ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
    if PROGRESS_FILE.exists():
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"categories": {}, "crawled_ids": []}


def save_progress(progress: dict):
    """ì§„í–‰ ìƒíƒœ ì €ì¥"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def save_articles(articles: list, category_key: str):
    """ìˆ˜ì§‘í•œ ê¸°ì‚¬ ì €ì¥"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    output_file = DATA_DIR / f"cine21_{category_key}.json"
    
    existing = []
    if output_file.exists():
        with open(output_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            existing = data.get("articles", [])
    
    existing_ids = {a["mag_id"] for a in existing}
    new_articles = [a for a in articles if a["mag_id"] not in existing_ids]
    all_articles = existing + new_articles
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "category": CATEGORIES[category_key]["name"],
            "total_count": len(all_articles),
            "last_updated": datetime.now().isoformat(),
            "articles": all_articles
        }, f, ensure_ascii=False, indent=2)
    
    print(f"  ğŸ’¾ ì €ì¥ë¨: {output_file.name} (ì‹ ê·œ {len(new_articles)}ê°œ, ì´ {len(all_articles)}ê°œ)")
    return len(new_articles)


# ============================================================================
# í¬ë¡¤ë§ í•¨ìˆ˜
# ============================================================================

def create_driver() -> webdriver.Chrome:
    """Selenium WebDriver ìƒì„± - ë¡œì»¬ Chrome/Edge ì‚¬ìš©"""
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument(f"user-agent={HEADERS['User-Agent']}")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--ignore-ssl-errors")
    options.add_argument("--allow-insecure-localhost")
    
    # ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ Chrome ì‚¬ìš©
    try:
        driver = webdriver.Chrome(options=options)
        return driver
    except Exception as e:
        print(f"  â„¹ï¸ Chrome ë“œë¼ì´ë²„ ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
        print("  â„¹ï¸ Edge ë¸Œë¼ìš°ì €ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
        
        # Edge ì‹œë„
        from selenium.webdriver.edge.options import Options as EdgeOptions
        from selenium.webdriver.edge.service import Service as EdgeService
        
        edge_options = EdgeOptions()
        edge_options.add_argument("--headless=new")
        edge_options.add_argument("--no-sandbox")
        edge_options.add_argument("--disable-dev-shm-usage")
        edge_options.add_argument("--disable-gpu")
        edge_options.add_argument(f"user-agent={HEADERS['User-Agent']}")
        edge_options.add_argument("--ignore-certificate-errors")
        
        return webdriver.Edge(options=edge_options)


def get_article_ids_from_page(driver, section: str, page_num: int) -> list:
    """ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ID ì¶”ì¶œ"""
    url = f"{BASE_URL}/news/section/?section={section}&p={page_num}"
    
    try:
        driver.get(url)
        WebDriverWait(driver, PAGE_LOAD_TIMEOUT).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(1.5)
        
        soup = BeautifulSoup(driver.page_source, "lxml")
        
        article_ids = []
        for link in soup.find_all("a", href=True):
            href = link["href"]
            match = re.search(r"mag_id=(\d+)", href)
            if match:
                article_ids.append(match.group(1))
        
        return list(dict.fromkeys(article_ids))
        
    except Exception as e:
        print(f"  âš ï¸ í˜ì´ì§€ {page_num} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def get_article_detail(mag_id: str, category_info: dict, session: requests.Session) -> Optional[dict]:
    """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    url = f"{BASE_URL}/news/view/?mag_id={mag_id}"
    
    try:
        r = session.get(url, headers=HEADERS, verify=False, timeout=15)
        if r.status_code != 200:
            return None
            
        soup = BeautifulSoup(r.text, "lxml")
        
        title = ""
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text().strip()
            if " - " in title:
                title = title.split(" - ")[0].strip()
        
        author = ""
        author_id = ""
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if "/db/writer/info/" in href:
                author = link.get_text().strip()
                match = re.search(r"pre_code=(\w+)", href)
                if match:
                    author_id = match.group(1)
                break
        
        date = ""
        text = soup.get_text()
        date_match = re.search(r"(\d{4}-\d{2}-\d{2})", text)
        if date_match:
            date = date_match.group(1)
        
        section = ""
        magazine_issue = ""
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if "/news/section/" in href and "section=" in href:
                section = link.get_text().strip()
            if "/db/mag/content/" in href:
                magazine_issue = link.get_text().strip()
        
        paragraphs = soup.find_all("p")
        texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30]
        content_text = "\n\n".join(texts)
        
        related_articles = []
        for link in soup.find_all("a", href=True):
            href = link["href"]
            match = re.search(r"mag_id=(\d+)", href)
            if match and match.group(1) != mag_id:
                related_articles.append(match.group(1))
        related_articles = list(dict.fromkeys(related_articles))[:5]
        
        # ê´€ë ¨ ì˜í™” ì¶”ì¶œ (.list_with_upthumb_item ì—ì„œ)
        related_movies = []
        for item in soup.select(".list_with_upthumb_item a"):
            href = item.get("href", "")
            if "/movie/info/?movie_id=" in href or "movie_id=" in href:
                movie_id_match = re.search(r"movie_id=(\d+)", href)
                movie_id = movie_id_match.group(1) if movie_id_match else None
                title_el = item.select_one(".title")
                movie_title = title_el.text.strip() if title_el else None
                year_el = item.select_one(".etc_info p")
                year = year_el.text.strip().replace("(", "").replace(")", "") if year_el else None
                
                if movie_id:
                    related_movies.append({
                        "movie_id": movie_id,
                        "title": movie_title,
                        "year": year
                    })
        
        article = Article(
            mag_id=mag_id,
            title=title,
            author=author,
            author_id=author_id,
            date=date,
            section=section,
            magazine_issue=magazine_issue,
            category=category_info["category_type"],
            category_name=category_info["name"],
            content=content_text,
            related_articles=related_articles,
            related_movies=related_movies,
            url=url,
            crawled_at=datetime.now().isoformat()
        )
        
        return asdict(article)
        
    except Exception as e:
        print(f"  âš ï¸ ê¸°ì‚¬ {mag_id} ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def crawl_category(category_key: str, max_pages: Optional[int] = None):
    """ì¹´í…Œê³ ë¦¬ ì „ì²´ í¬ë¡¤ë§"""
    category = CATEGORIES[category_key]
    section = category["section"]
    total_pages = max_pages or category["max_pages"]
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ {category['name']} í¬ë¡¤ë§ ì‹œì‘ ({total_pages}í˜ì´ì§€)")
    print(f"{'='*60}")
    
    progress = load_progress()
    cat_progress = progress.get("categories", {}).get(category_key, {})
    start_page = cat_progress.get("last_page", 0) + 1
    crawled_ids = set(progress.get("crawled_ids", []))
    
    if start_page > 1:
        print(f"  ğŸ“Œ ì´ì „ ì§„í–‰ ìƒíƒœì—ì„œ ì¬ê°œ: {start_page}í˜ì´ì§€ë¶€í„°")
    
    print(f"\nğŸ“‹ 1ë‹¨ê³„: ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
    driver = create_driver()
    new_ids = []
    
    try:
        for page_num in range(start_page, total_pages + 1):
            ids = get_article_ids_from_page(driver, section, page_num)
            new_found = [id for id in ids if id not in crawled_ids]
            new_ids.extend(new_found)
            
            print(f"  ğŸ“„ í˜ì´ì§€ {page_num}/{total_pages}: {len(ids)}ê°œ ë°œê²¬ (ì‹ ê·œ: {len(new_found)}ê°œ)")
            
            crawled_ids.update(ids)
            if "categories" not in progress:
                progress["categories"] = {}
            progress["categories"][category_key] = {
                "last_page": page_num,
                "updated_at": datetime.now().isoformat()
            }
            progress["crawled_ids"] = list(crawled_ids)
            
            if page_num % 5 == 0:
                save_progress(progress)
            
            time.sleep(REQUEST_DELAY)
        
        save_progress(progress)
        
    finally:
        driver.quit()
    
    print(f"  âœ… ê¸°ì‚¬ ID ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(new_ids)}ê°œ ì‹ ê·œ")
    
    if not new_ids:
        print("  â„¹ï¸ ìƒˆë¡œ í¬ë¡¤ë§í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“ 2ë‹¨ê³„: ê¸°ì‚¬ ìƒì„¸ í¬ë¡¤ë§ ì¤‘...")
    articles = []
    session = requests.Session()
    
    def fetch_article(args):
        idx, mag_id = args
        time.sleep(REQUEST_DELAY / MAX_CONCURRENT_REQUESTS)
        return get_article_detail(mag_id, category, session)
    
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
        futures = {executor.submit(fetch_article, (idx, mag_id)): (idx, mag_id) 
                   for idx, mag_id in enumerate(new_ids)}
        
        completed = 0
        for future in as_completed(futures):
            completed += 1
            result = future.result()
            if result:
                articles.append(result)
            
            if completed % 20 == 0 or completed == len(new_ids):
                print(f"  ğŸ“° ì§„í–‰: {completed}/{len(new_ids)} ({len(articles)}ê°œ ì„±ê³µ)")
    
    if articles:
        save_articles(articles, category_key)
        print(f"  âœ… {category['name']} í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")


def main(args):
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸ¬ Cine21 í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    if args.reset:
        if PROGRESS_FILE.exists():
            os.remove(PROGRESS_FILE)
        print("ğŸ—‘ï¸ ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”ë¨")
    
    categories_to_crawl = []
    if args.category:
        if args.category in CATEGORIES:
            categories_to_crawl = [args.category]
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {args.category}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥: {', '.join(CATEGORIES.keys())}")
            return
    else:
        categories_to_crawl = list(CATEGORIES.keys())
    
    max_pages = 2 if args.test_mode else None
    if args.test_mode:
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê° ì¹´í…Œê³ ë¦¬ 2í˜ì´ì§€ì”©ë§Œ í¬ë¡¤ë§")
    
    for category_key in categories_to_crawl:
        crawl_category(category_key, max_pages)
    
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"â° ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {DATA_DIR}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Cine21 ì˜í™” í‰ë¡ /ë¦¬ë·° í¬ë¡¤ëŸ¬")
    parser.add_argument("--test-mode", action="store_true", help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê° ì¹´í…Œê³ ë¦¬ 2í˜ì´ì§€)")
    parser.add_argument("--category", type=str, help="íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ (reading, critique, review)")
    parser.add_argument("--reset", action="store_true", help="ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”")
    
    args = parser.parse_args()
    main(args)
